{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(82603, 192, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 192)\n",
       "      (token_type_embeddings): Embedding(1000, 192)\n",
       "      (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "root_path = \"/home/mai.kassem/Documents/Med-BERT/\"\n",
    "BertForSequenceClassification.from_pretrained(root_path + \"MedBERT Pretraining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bert.embeddings.position_ids',\n",
       "              tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "                        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "                        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "                        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "                        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "                        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "                        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "                        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "                       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "                       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "                       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "                       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "                       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "                       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "                       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "                       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "                       224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "                       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "                       252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "                       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "                       280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "                       294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "                       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "                       322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "                       336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "                       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "                       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "                       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "                       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "                       406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "                       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "                       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "                       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
       "                       462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "                       476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "                       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
       "                       504, 505, 506, 507, 508, 509, 510, 511]])),\n",
       "             ('bert.embeddings.word_embeddings.weight',\n",
       "              tensor([[-0.0011,  0.0004, -0.0045,  ...,  0.0159, -0.0101,  0.0027],\n",
       "                      [-0.0006,  0.0035,  0.0629,  ..., -0.0453,  0.0473, -0.0018],\n",
       "                      [ 0.0705,  0.0396,  0.0785,  ...,  0.0846,  0.0274, -0.0472],\n",
       "                      ...,\n",
       "                      [-0.0316, -0.0742, -0.0694,  ..., -0.0674,  0.0521,  0.0679],\n",
       "                      [-0.0442, -0.0457, -0.0513,  ..., -0.0837,  0.0522,  0.0241],\n",
       "                      [-0.0279, -0.0400, -0.0354,  ..., -0.0921,  0.0450,  0.0153]])),\n",
       "             ('bert.embeddings.position_embeddings.weight',\n",
       "              tensor([[ 9.3884e-04,  2.0054e-02,  3.6423e-03,  ..., -6.4174e-03,\n",
       "                        1.6820e-02, -1.5715e-02],\n",
       "                      [-8.4210e-04,  7.8851e-03,  2.9606e-05,  ..., -8.2686e-03,\n",
       "                        2.5066e-02, -1.0212e-02],\n",
       "                      [ 3.4766e-03,  3.1109e-03,  1.9220e-03,  ..., -3.6584e-03,\n",
       "                        2.3661e-02, -1.2941e-02],\n",
       "                      ...,\n",
       "                      [-2.4363e-02, -1.8279e-02, -1.6384e-03,  ...,  3.5928e-03,\n",
       "                       -1.8235e-02,  1.6383e-02],\n",
       "                      [-1.4871e-02, -1.4826e-02, -4.1375e-03,  ...,  2.2456e-02,\n",
       "                        7.9456e-03, -7.8619e-03],\n",
       "                      [-1.9159e-02,  7.4150e-03,  3.4810e-03,  ..., -1.7728e-02,\n",
       "                       -1.8190e-02, -3.3084e-02]])),\n",
       "             ('bert.embeddings.token_type_embeddings.weight',\n",
       "              tensor([[-2.4736e-02, -6.7489e-03,  7.2180e-05,  ..., -2.2455e-02,\n",
       "                       -3.0579e-03, -3.1690e-03],\n",
       "                      [ 3.0013e-03, -2.9438e-04,  8.8294e-03,  ...,  8.7520e-03,\n",
       "                       -7.3407e-03,  1.4424e-03],\n",
       "                      [ 4.1184e-03,  3.6665e-03, -1.1664e-02,  ...,  2.1012e-02,\n",
       "                       -3.0616e-03, -8.4772e-03],\n",
       "                      ...,\n",
       "                      [ 2.5341e-02, -2.0056e-02,  6.0744e-03,  ...,  3.4701e-04,\n",
       "                        1.3433e-02, -1.8149e-02],\n",
       "                      [ 2.5132e-02,  1.2827e-02, -1.8674e-02,  ..., -6.0113e-03,\n",
       "                       -2.2775e-02, -1.1294e-02],\n",
       "                      [ 3.4658e-02, -1.4957e-02, -2.2392e-02,  ...,  2.9001e-03,\n",
       "                        2.1782e-02,  1.4470e-02]])),\n",
       "             ('bert.embeddings.LayerNorm.weight',\n",
       "              tensor([1.0002, 1.0167, 0.9905, 1.0062, 1.0070, 1.0214, 1.0089, 1.0177, 1.0036,\n",
       "                      1.0036, 1.0241, 0.9807, 0.9888, 1.0042, 1.0040, 1.0144, 1.0141, 0.9900,\n",
       "                      1.0166, 1.0202, 0.9783, 1.0109, 1.0094, 1.0078, 1.0175, 1.0173, 1.0003,\n",
       "                      0.9992, 1.0294, 0.9914, 0.9992, 1.0112, 0.9842, 1.0089, 1.0037, 1.0095,\n",
       "                      1.0126, 1.0157, 1.0295, 1.0023, 0.9982, 1.0042, 1.0019, 1.0111, 0.9856,\n",
       "                      1.0123, 0.9838, 1.0011, 1.0091, 1.0057, 0.9934, 1.0117, 1.0256, 0.9944,\n",
       "                      1.0194, 1.0150, 1.0110, 1.0191, 0.9940, 0.9983, 1.0138, 1.0134, 1.0098,\n",
       "                      1.0347, 1.0075, 0.9976, 1.0205, 1.0369, 1.0044, 1.0162, 0.9834, 0.9923,\n",
       "                      0.9923, 0.9948, 1.0145, 1.0185, 0.9950, 1.0052, 0.9978, 0.9988, 1.0025,\n",
       "                      0.9799, 1.0164, 1.0137, 1.0026, 1.0197, 0.9874, 1.0101, 1.0040, 1.0074,\n",
       "                      0.9881, 1.0389, 0.9992, 1.0081, 1.0237, 1.0228, 1.0068, 1.0199, 1.0065,\n",
       "                      1.0084, 0.9984, 1.0044, 1.0061, 1.0142, 1.0040, 1.0072, 0.9878, 1.0292,\n",
       "                      1.0231, 1.0099, 0.9934, 1.0197, 1.0026, 1.0353, 1.0818, 1.0145, 1.0150,\n",
       "                      1.0034, 0.9808, 1.0014, 1.0324, 1.0179, 0.9975, 1.0053, 0.9891, 0.9904,\n",
       "                      1.0114, 1.0095, 0.9900, 0.9933, 1.0242, 1.0294, 1.0107, 1.0014, 0.9960,\n",
       "                      1.0302, 1.0328, 1.0101, 1.0137, 1.0222, 1.0076, 1.0131, 1.0058, 1.0018,\n",
       "                      1.0092, 1.0002, 1.0275, 1.0190, 1.0172, 0.9979, 1.0160, 1.0346, 1.0174,\n",
       "                      1.0062, 1.0077, 1.0074, 1.0017, 1.0023, 1.0248, 1.0137, 1.0073, 0.9943,\n",
       "                      1.0307, 0.9999, 1.0108, 1.0260, 1.0011, 1.0082, 1.0064, 0.9985, 1.0244,\n",
       "                      0.9890, 0.9927, 0.9977, 1.0070, 1.0030, 1.0087, 1.0008, 1.0067, 1.0148,\n",
       "                      1.0479, 1.0126, 1.0246, 1.0074, 1.0018, 1.0183, 1.0154, 1.0077, 1.0327,\n",
       "                      1.0010, 1.0155, 1.0117])),\n",
       "             ('bert.embeddings.LayerNorm.bias',\n",
       "              tensor([ 1.0130e-02,  8.4799e-03,  1.9876e-02,  1.8879e-02, -3.2263e-03,\n",
       "                      -3.2475e-02, -3.0300e-03, -7.3575e-03,  4.2456e-03, -1.0752e-04,\n",
       "                       2.7130e-02,  2.3462e-02,  1.4520e-02, -1.1518e-03, -2.9247e-02,\n",
       "                      -9.8584e-03, -1.6170e-02, -3.4927e-02,  1.2359e-02, -1.9967e-02,\n",
       "                       2.7630e-02, -2.0359e-02, -5.1176e-03,  7.9096e-03, -2.1885e-02,\n",
       "                       1.0464e-02,  1.5008e-02, -7.0709e-03,  6.7204e-03,  1.6219e-02,\n",
       "                      -1.5073e-02, -1.7574e-03,  2.8149e-02,  5.2511e-03,  8.3892e-03,\n",
       "                      -3.3542e-03, -1.5894e-02,  1.1754e-02, -9.0678e-03,  1.5069e-02,\n",
       "                      -1.2864e-02, -6.5148e-03, -1.2674e-02, -1.3521e-02,  9.1207e-03,\n",
       "                      -6.4303e-03,  1.4082e-02, -2.6921e-02, -4.4425e-03, -3.9540e-03,\n",
       "                       1.9316e-02, -1.8818e-03,  3.1660e-02,  1.8838e-02,  3.9563e-03,\n",
       "                       1.7328e-03, -4.0410e-03, -8.8783e-03,  1.1411e-02,  6.3740e-03,\n",
       "                       1.5084e-02,  3.4976e-03, -1.8638e-03, -1.1935e-02,  2.4509e-02,\n",
       "                      -5.2593e-03, -1.0887e-02, -1.1968e-02,  1.3031e-02,  2.7713e-03,\n",
       "                       1.2650e-02,  2.7056e-02,  1.4068e-02,  6.1304e-03, -1.4987e-02,\n",
       "                      -1.1806e-02, -1.9014e-02, -8.0227e-03, -7.1948e-05, -1.1240e-02,\n",
       "                       1.2937e-02,  3.7865e-02,  8.6800e-03, -3.2822e-03,  9.5636e-03,\n",
       "                       9.1999e-03,  7.2032e-03, -8.4579e-03, -2.9012e-02, -1.2621e-02,\n",
       "                       2.5122e-02, -2.3948e-02,  4.8700e-03,  2.4026e-02,  1.5113e-02,\n",
       "                      -2.4001e-03,  2.1075e-02, -1.4309e-02, -1.2829e-03,  9.0337e-03,\n",
       "                       2.1165e-02, -2.2734e-02, -3.0207e-02, -1.0320e-02,  7.3526e-03,\n",
       "                       6.4736e-03,  2.6652e-02, -1.9457e-02, -3.1551e-03,  1.0888e-03,\n",
       "                       2.4900e-02, -8.3311e-03, -2.0085e-02, -2.9346e-02,  3.2392e-02,\n",
       "                      -7.1085e-03,  1.7417e-02,  3.0163e-04,  1.1735e-02,  1.2737e-02,\n",
       "                       9.7878e-03, -1.2596e-02,  6.5754e-03, -6.8883e-03, -6.9918e-03,\n",
       "                       1.5063e-02, -3.4436e-04,  1.0457e-02,  5.1713e-03, -1.5299e-03,\n",
       "                      -1.2024e-02, -8.3386e-03, -6.2699e-03, -3.1304e-03,  1.9407e-03,\n",
       "                      -1.1990e-02, -4.9779e-03,  6.1188e-03,  3.6706e-02, -8.2020e-03,\n",
       "                       3.2623e-02,  7.3272e-03, -3.2361e-02, -6.7002e-03,  2.0427e-02,\n",
       "                       2.8486e-02, -1.4819e-02, -2.1197e-02, -3.9605e-03, -1.9034e-02,\n",
       "                      -2.1399e-02, -1.9218e-02, -6.2479e-03, -1.3726e-02,  3.9683e-02,\n",
       "                      -1.5759e-02, -1.0924e-02,  1.9010e-02, -2.0322e-02,  5.3309e-03,\n",
       "                      -4.6366e-03,  4.0333e-02, -1.2688e-02, -2.3206e-02,  1.6870e-02,\n",
       "                      -9.2558e-03,  1.1702e-02, -3.3760e-02,  5.5469e-03,  1.9802e-02,\n",
       "                      -1.0114e-02,  2.1604e-02,  3.4144e-03,  2.3347e-02,  8.1407e-03,\n",
       "                       3.2627e-02, -2.9002e-02,  6.2143e-02, -1.8841e-02, -1.9688e-02,\n",
       "                      -2.9187e-02, -1.0048e-02, -8.9351e-03,  2.0898e-02, -3.1334e-02,\n",
       "                      -7.1842e-03, -2.1740e-02, -2.4021e-02, -1.4344e-02,  1.1811e-02,\n",
       "                      -1.6421e-02, -3.5197e-03])),\n",
       "             ('bert.encoder.layer.0.attention.self.query.weight',\n",
       "              tensor([[-0.0146, -0.0120, -0.0004,  ...,  0.0223, -0.0160, -0.0020],\n",
       "                      [-0.0395,  0.0171,  0.0405,  ...,  0.0027,  0.1093, -0.0177],\n",
       "                      [-0.0242,  0.0049, -0.0070,  ...,  0.0052,  0.0232, -0.0217],\n",
       "                      ...,\n",
       "                      [-0.0252, -0.0567,  0.0151,  ..., -0.0369, -0.0490,  0.0114],\n",
       "                      [ 0.0445,  0.0097, -0.0139,  ...,  0.0037, -0.0252, -0.0109],\n",
       "                      [ 0.0025,  0.0147, -0.0053,  ...,  0.0043,  0.0530, -0.0305]])),\n",
       "             ('bert.encoder.layer.0.attention.self.query.bias',\n",
       "              tensor([-0.0428,  0.0740,  0.0412, -0.0738, -0.0211,  0.0459, -0.0192, -0.0243,\n",
       "                       0.0680, -0.0137,  0.0232,  0.0574,  0.0614, -0.0451, -0.0639, -0.0432,\n",
       "                       0.0192, -0.0326, -0.0348,  0.0573,  0.0241,  0.0326,  0.0616, -0.0363,\n",
       "                       0.0063,  0.0083, -0.0353,  0.0196,  0.0182,  0.0283, -0.0475, -0.0201,\n",
       "                       0.0169,  0.0586,  0.0584,  0.0550, -0.0693, -0.0182,  0.0498, -0.0323,\n",
       "                       0.0559, -0.0305,  0.0589,  0.0160,  0.0503, -0.0127,  0.0675, -0.0188,\n",
       "                      -0.0721,  0.0651, -0.0534,  0.0670,  0.0175, -0.0307, -0.0425, -0.0134,\n",
       "                      -0.0341,  0.0310, -0.0298,  0.0169, -0.0355,  0.0423, -0.0325,  0.0585,\n",
       "                      -0.0467, -0.0187, -0.0614,  0.0169, -0.0002, -0.0470, -0.0155,  0.0642,\n",
       "                       0.0464,  0.0321, -0.0516, -0.0003,  0.0002, -0.0371, -0.0259,  0.0458,\n",
       "                      -0.0050, -0.0438, -0.0486, -0.0354, -0.0078,  0.0037, -0.0268,  0.0009,\n",
       "                      -0.0666,  0.0482,  0.0107,  0.0577, -0.0449, -0.0398,  0.0522,  0.0236,\n",
       "                      -0.0466, -0.0658,  0.0689, -0.0375, -0.0458,  0.0266,  0.0750, -0.0840,\n",
       "                       0.0654,  0.0385,  0.0868,  0.0632, -0.0759, -0.1023, -0.0584, -0.0167,\n",
       "                       0.0416, -0.0626,  0.0337,  0.0439,  0.0592,  0.0648,  0.0140, -0.0570,\n",
       "                      -0.0688, -0.0153, -0.0595,  0.0527, -0.0667,  0.0652,  0.0563, -0.0231,\n",
       "                       0.0171, -0.0406,  0.0269, -0.0527, -0.0034,  0.0286, -0.0187, -0.0207,\n",
       "                      -0.0218,  0.0334,  0.0261, -0.0037, -0.0236, -0.0226,  0.0204,  0.0530,\n",
       "                      -0.0228,  0.0139,  0.0153,  0.0043, -0.0306, -0.0221,  0.0117,  0.0045,\n",
       "                      -0.0173, -0.0224, -0.0150,  0.0253, -0.0075, -0.0240, -0.0197,  0.0070,\n",
       "                       0.0400,  0.0397, -0.0395, -0.0386, -0.0043,  0.0757, -0.0112, -0.0214,\n",
       "                       0.0288,  0.0664, -0.0677, -0.0390,  0.0160,  0.0507,  0.0555,  0.0408,\n",
       "                       0.0519,  0.0415,  0.0077, -0.0700,  0.0470, -0.0517,  0.0546, -0.0368,\n",
       "                      -0.0457,  0.0602,  0.0561,  0.0596,  0.0119, -0.0317,  0.0390, -0.0091])),\n",
       "             ('bert.encoder.layer.0.attention.self.key.weight',\n",
       "              tensor([[ 0.0140,  0.0355,  0.0155,  ...,  0.0163, -0.0443,  0.0092],\n",
       "                      [-0.0222,  0.0018, -0.0038,  ...,  0.0287,  0.0293, -0.0229],\n",
       "                      [-0.0044, -0.0062, -0.0341,  ..., -0.0040,  0.0162, -0.0398],\n",
       "                      ...,\n",
       "                      [ 0.0032, -0.0406,  0.0156,  ...,  0.0127,  0.0166,  0.0443],\n",
       "                      [-0.0330, -0.0238,  0.0055,  ...,  0.0335,  0.0180, -0.0643],\n",
       "                      [-0.0203,  0.0020, -0.0168,  ..., -0.0002, -0.0081,  0.0381]])),\n",
       "             ('bert.encoder.layer.0.attention.self.key.bias',\n",
       "              tensor([-2.9545e-08, -1.9508e-07, -1.7178e-07,  4.5495e-08,  1.3572e-08,\n",
       "                      -6.6046e-08,  1.0050e-07,  2.3379e-08, -1.0108e-07, -1.2992e-07,\n",
       "                      -6.9635e-09, -7.1436e-08,  1.0555e-09,  2.3343e-07,  7.6871e-08,\n",
       "                       9.2470e-08, -1.6006e-07,  7.3145e-08,  2.7720e-08, -5.3340e-08,\n",
       "                      -1.3707e-07, -1.2385e-07, -2.7170e-07, -3.7785e-08, -1.5469e-07,\n",
       "                      -6.7036e-08,  1.4762e-07,  2.7799e-08,  2.4864e-08, -7.7311e-08,\n",
       "                       8.2026e-08,  8.4656e-08,  8.6643e-08,  8.8632e-08, -5.5513e-08,\n",
       "                      -4.6821e-08,  1.6954e-09,  5.6272e-08, -1.0811e-07,  6.2012e-08,\n",
       "                      -6.6957e-08,  1.2976e-07,  2.7189e-08, -1.3257e-07, -1.4682e-07,\n",
       "                      -1.1405e-07,  2.5858e-08,  2.4174e-08, -2.6696e-08,  8.5207e-08,\n",
       "                      -1.1237e-07, -1.0193e-07, -1.1746e-08, -1.9723e-07,  1.2740e-07,\n",
       "                      -1.2830e-07, -1.0731e-08,  1.5801e-07, -1.3284e-09, -7.9189e-08,\n",
       "                      -5.6228e-08, -2.6088e-08, -1.0017e-07,  3.5201e-08,  3.8501e-08,\n",
       "                       5.4457e-08, -5.9075e-08, -9.7431e-08,  2.6468e-08,  1.4119e-07,\n",
       "                      -1.5851e-08,  6.9578e-08, -4.6022e-08, -1.3375e-07, -1.9081e-08,\n",
       "                       3.2909e-08, -7.6652e-09, -2.2305e-08, -3.0405e-08, -1.3052e-07,\n",
       "                       3.9697e-08,  1.1839e-07,  1.9927e-07,  1.7566e-07, -3.8784e-08,\n",
       "                      -2.7878e-08,  2.2483e-08,  2.9832e-08,  4.4272e-08, -1.4010e-07,\n",
       "                      -2.0480e-09, -7.0118e-09,  1.1869e-07,  9.5602e-08,  4.5227e-08,\n",
       "                      -7.7080e-08,  5.2662e-08,  1.5387e-09, -3.2007e-08,  5.2671e-08,\n",
       "                       4.7324e-08, -2.0240e-08, -5.9621e-08,  4.3165e-08, -9.0670e-08,\n",
       "                      -5.2477e-08, -1.2281e-07, -1.1840e-07,  8.0979e-08,  1.0270e-07,\n",
       "                       7.8041e-08, -2.9018e-08, -1.6998e-07,  4.7705e-08,  1.9684e-10,\n",
       "                      -7.1929e-08,  2.5367e-10, -9.2966e-08,  1.0047e-07,  1.2396e-07,\n",
       "                       8.4610e-08,  4.2382e-08,  1.2451e-07, -6.0107e-08,  7.5708e-08,\n",
       "                       9.6407e-10, -1.3498e-07,  7.3421e-08,  1.6389e-08,  3.5940e-08,\n",
       "                       1.5942e-07, -1.0870e-07, -1.0022e-07,  1.2125e-07, -1.1187e-07,\n",
       "                      -3.7305e-09, -1.0169e-07, -5.1799e-08,  6.3340e-08, -7.7639e-08,\n",
       "                       7.1511e-08, -5.7020e-08,  6.8388e-08, -1.4039e-07, -1.2703e-07,\n",
       "                       5.3213e-08,  7.7106e-08,  1.2569e-07, -5.9355e-08, -8.4036e-08,\n",
       "                       2.6689e-08,  5.6750e-08, -1.5022e-07, -7.3120e-08, -1.9098e-07,\n",
       "                       4.2111e-08, -1.9775e-08, -7.6724e-08,  7.3242e-10,  3.1431e-08,\n",
       "                       1.2860e-07,  3.3226e-08,  8.3187e-09, -3.9881e-08, -1.7554e-08,\n",
       "                       1.7711e-07,  9.7647e-08,  1.8212e-08, -5.3477e-08,  1.1821e-07,\n",
       "                      -8.9034e-08, -8.2883e-08,  5.6058e-08,  1.0693e-08,  5.1479e-08,\n",
       "                       4.7238e-08,  1.4653e-07,  1.8129e-08, -1.1242e-08,  7.1900e-09,\n",
       "                      -1.2331e-07, -9.1020e-08,  4.0720e-08, -6.0292e-08, -1.1274e-07,\n",
       "                       5.0349e-08,  8.2432e-08,  1.3481e-08,  1.3545e-07, -1.5549e-07,\n",
       "                       1.1066e-07,  9.9229e-08])),\n",
       "             ('bert.encoder.layer.0.attention.self.value.weight',\n",
       "              tensor([[ 0.0154,  0.0072,  0.0412,  ...,  0.0205,  0.0419,  0.0097],\n",
       "                      [ 0.0054, -0.0011,  0.0059,  ...,  0.0194, -0.0109,  0.0412],\n",
       "                      [ 0.0028,  0.0081,  0.0213,  ...,  0.0123,  0.0051,  0.0101],\n",
       "                      ...,\n",
       "                      [ 0.0134, -0.0203, -0.0012,  ..., -0.0190, -0.0040,  0.0272],\n",
       "                      [ 0.0256,  0.0260, -0.0395,  ...,  0.0342, -0.0082, -0.0063],\n",
       "                      [ 0.0421, -0.0191,  0.0165,  ...,  0.0072, -0.0283, -0.0261]])),\n",
       "             ('bert.encoder.layer.0.attention.self.value.bias',\n",
       "              tensor([ 5.0083e-03,  5.8359e-03,  6.5707e-03,  1.5994e-02,  9.1555e-03,\n",
       "                       3.2765e-03, -1.1497e-02, -1.5580e-02,  6.7473e-03,  1.0182e-02,\n",
       "                       8.9450e-03, -1.0520e-02, -4.4791e-03,  7.8413e-03, -1.4671e-03,\n",
       "                      -8.8981e-03, -1.2277e-02,  5.1235e-03, -4.2740e-03,  4.4258e-03,\n",
       "                       5.0266e-03,  3.1391e-03, -5.4741e-03, -2.7532e-04,  8.4733e-03,\n",
       "                      -8.2478e-04,  1.2761e-02,  1.2217e-02,  1.2801e-02, -5.8248e-03,\n",
       "                       2.1602e-02,  1.4712e-02,  3.3526e-03, -9.0076e-03, -4.3722e-03,\n",
       "                       1.1011e-02, -8.7456e-03,  4.1867e-03,  4.7792e-03, -3.0058e-03,\n",
       "                      -3.1627e-04,  1.1291e-02, -6.0462e-03,  1.2974e-02,  8.0424e-03,\n",
       "                       4.4962e-04,  1.2661e-02, -4.0880e-03,  1.7148e-03,  1.6344e-04,\n",
       "                       7.6688e-03,  3.5623e-03, -9.8303e-03,  5.4261e-03, -1.0945e-02,\n",
       "                       2.1472e-03,  4.1791e-03,  2.1111e-03,  1.1822e-02, -2.9739e-03,\n",
       "                       8.6600e-04,  4.2018e-03, -3.8564e-03,  1.2044e-02, -1.0919e-02,\n",
       "                      -6.2001e-03, -1.4304e-04,  4.3416e-03,  5.3599e-03,  1.9238e-03,\n",
       "                      -5.2399e-03, -1.4696e-03,  5.5511e-03,  2.0652e-03,  1.1619e-02,\n",
       "                      -9.1772e-04,  9.7527e-03, -1.5364e-02,  2.0226e-04,  4.9789e-03,\n",
       "                       3.7031e-04, -1.5283e-02,  4.7359e-03, -7.7539e-03, -4.9568e-03,\n",
       "                      -3.9149e-03, -1.0752e-03,  6.8383e-03,  3.9491e-03, -1.4029e-03,\n",
       "                      -2.6259e-03, -7.2083e-03, -1.2051e-02,  1.1254e-03,  4.2777e-03,\n",
       "                       7.0655e-03, -3.1114e-03,  8.1747e-03, -5.6765e-03, -1.3923e-03,\n",
       "                      -1.0985e-02,  1.4763e-03, -7.0217e-03,  1.6183e-03, -2.8864e-03,\n",
       "                       8.4335e-04,  2.6119e-03, -7.9610e-03,  8.9359e-03,  3.9434e-03,\n",
       "                      -2.9994e-03,  1.0322e-02,  7.8549e-05, -8.5570e-03,  7.6898e-03,\n",
       "                       3.5967e-04, -5.5557e-03, -2.4785e-03, -4.9161e-03, -3.9916e-03,\n",
       "                      -4.1541e-03,  1.0524e-02, -9.3542e-03,  4.4458e-03,  4.7859e-03,\n",
       "                       3.1754e-03, -6.5178e-03, -1.2002e-02,  3.4616e-03,  5.8277e-03,\n",
       "                      -2.3105e-03, -9.2842e-03, -6.2936e-03, -4.5358e-03,  1.2782e-02,\n",
       "                       1.2022e-02, -1.1046e-03, -4.9033e-03, -1.5939e-02,  6.1194e-03,\n",
       "                       5.9389e-03, -2.1770e-02,  1.0917e-02,  3.3752e-03, -3.5155e-03,\n",
       "                       8.1641e-03,  4.9548e-03, -3.1105e-03, -1.6523e-03,  3.7500e-03,\n",
       "                       2.1566e-03, -1.3271e-02,  6.1276e-03,  3.9082e-03, -1.3469e-02,\n",
       "                       1.1256e-02, -1.4912e-03, -8.0701e-03, -1.0625e-02,  6.5804e-03,\n",
       "                      -1.2196e-02, -3.2427e-03, -7.7879e-03,  2.4509e-03,  5.5704e-03,\n",
       "                       3.5686e-03,  1.3994e-02,  7.1118e-04,  6.7893e-03, -5.6636e-04,\n",
       "                      -9.3237e-03,  3.3044e-03,  7.3908e-03,  4.9750e-04, -1.0537e-03,\n",
       "                      -2.7962e-03, -3.4294e-03, -5.8778e-04,  7.1919e-04,  4.0085e-04,\n",
       "                      -1.7178e-04, -2.2694e-03, -9.4397e-03, -2.1710e-03, -1.5121e-02,\n",
       "                      -4.4573e-04, -1.0544e-02,  3.0793e-03, -1.8314e-02,  3.1052e-03,\n",
       "                      -7.1443e-04,  1.2549e-02])),\n",
       "             ('bert.encoder.layer.0.attention.output.dense.weight',\n",
       "              tensor([[ 0.0200,  0.0319, -0.0070,  ...,  0.0012, -0.0045, -0.0088],\n",
       "                      [ 0.0043, -0.0196, -0.0098,  ..., -0.0061,  0.0034, -0.0176],\n",
       "                      [ 0.0002, -0.0133, -0.0164,  ...,  0.0142,  0.0597,  0.0131],\n",
       "                      ...,\n",
       "                      [-0.0080, -0.0182, -0.0165,  ..., -0.0580,  0.0150,  0.0446],\n",
       "                      [-0.0200, -0.0298,  0.0070,  ..., -0.0163, -0.0200,  0.0133],\n",
       "                      [-0.0035,  0.0094, -0.0020,  ..., -0.0170, -0.0131,  0.0050]])),\n",
       "             ('bert.encoder.layer.0.attention.output.dense.bias',\n",
       "              tensor([ 3.8116e-03,  1.6772e-03,  3.6864e-03,  6.0624e-03, -1.8587e-03,\n",
       "                       2.1028e-03, -7.1036e-03, -1.5084e-03, -1.1517e-02, -1.0922e-02,\n",
       "                      -7.4636e-03,  4.5996e-03,  4.1747e-03, -7.2991e-03,  1.0957e-04,\n",
       "                       1.5912e-03,  2.9980e-03, -6.3410e-03, -2.3513e-03, -7.0422e-04,\n",
       "                       1.5718e-02, -4.9201e-03,  8.5015e-04,  5.4174e-03, -1.3627e-02,\n",
       "                       5.2632e-03,  2.8545e-03,  9.0363e-03, -9.5040e-03,  2.8214e-03,\n",
       "                      -7.8659e-03,  6.2011e-03,  1.7814e-03,  4.5812e-03, -7.1022e-03,\n",
       "                       5.1632e-03, -8.6919e-03, -1.2589e-02, -2.1590e-03,  4.7765e-03,\n",
       "                      -6.9248e-03, -7.0326e-03, -1.1617e-02, -4.8208e-03,  6.9228e-03,\n",
       "                      -2.2612e-03,  2.0899e-02,  4.5084e-03,  1.4595e-04, -1.4365e-03,\n",
       "                       2.4001e-03, -5.6992e-03, -1.4706e-02,  1.1284e-02,  1.8941e-03,\n",
       "                       5.1246e-03,  8.6898e-04, -2.0549e-03,  5.1207e-03,  8.1164e-03,\n",
       "                       4.7232e-03, -7.8498e-04,  4.0585e-03,  2.6443e-03,  5.1776e-03,\n",
       "                       5.7047e-03,  3.4301e-03,  1.2685e-02,  6.5702e-03,  6.1940e-04,\n",
       "                       5.0424e-03,  1.9678e-03,  6.7522e-03, -3.8054e-03, -1.9767e-03,\n",
       "                      -4.7989e-03, -2.3487e-03,  5.3003e-03, -2.5540e-03, -1.0238e-02,\n",
       "                      -2.1291e-03,  8.3706e-04,  6.5571e-03, -3.2670e-04,  1.8221e-04,\n",
       "                       2.2832e-03, -4.2383e-03, -1.4147e-03, -5.3800e-03,  5.8182e-03,\n",
       "                       5.3538e-03, -1.2189e-03, -8.5157e-03, -1.1553e-04,  1.5130e-03,\n",
       "                       3.0267e-02, -1.4447e-03,  8.1171e-03, -1.3098e-03, -1.4188e-03,\n",
       "                       3.3962e-03, -7.6795e-03, -2.6118e-03, -4.5024e-03, -5.5634e-03,\n",
       "                      -5.8212e-03,  5.4786e-03, -2.0573e-03,  1.8611e-03,  9.3079e-04,\n",
       "                      -5.7703e-04, -4.8242e-03, -3.8686e-03,  1.0114e-03, -2.8165e-02,\n",
       "                      -2.1967e-04, -7.9361e-04,  7.9586e-03,  5.9469e-03,  4.2406e-04,\n",
       "                      -9.1166e-03,  1.6575e-02,  1.1164e-03,  2.5816e-04, -2.6649e-03,\n",
       "                       5.1842e-03,  2.3967e-03,  4.9339e-03,  3.5957e-03,  6.5036e-03,\n",
       "                       7.9471e-03,  2.4307e-03, -4.6311e-03,  7.3201e-03, -6.7374e-04,\n",
       "                      -3.0009e-03, -9.7659e-03,  5.7855e-03, -8.6138e-03, -2.1025e-04,\n",
       "                       4.2356e-03,  1.2738e-02,  1.2077e-02, -3.7144e-04, -2.1941e-03,\n",
       "                       1.2417e-02,  5.9513e-05, -7.9490e-03,  3.1560e-03, -7.7480e-04,\n",
       "                      -7.6653e-04,  7.7733e-03, -6.0632e-04,  2.0302e-03,  9.5968e-03,\n",
       "                       6.8806e-03, -9.3536e-03,  7.0449e-03, -1.6450e-02, -1.2836e-02,\n",
       "                      -3.2430e-03,  6.0580e-04,  9.3338e-04,  3.0145e-03,  1.7073e-02,\n",
       "                      -1.4646e-02, -3.6076e-03, -1.8465e-03,  5.5727e-04,  4.6625e-03,\n",
       "                       2.6285e-04,  8.5987e-03,  4.3289e-05, -2.0645e-03, -1.7924e-03,\n",
       "                       4.1412e-03, -7.8518e-03, -1.4680e-02, -1.5473e-03,  1.6101e-03,\n",
       "                       3.0427e-02,  1.2741e-03,  3.0907e-03,  8.5666e-03, -7.0522e-03,\n",
       "                      -4.2761e-03, -2.2347e-03, -7.3432e-03,  6.6824e-04,  1.8420e-02,\n",
       "                      -8.4795e-03,  4.5697e-03])),\n",
       "             ('bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9925, 1.0084, 0.9867, 0.9971, 0.9981, 1.0091, 1.0009, 1.0141, 0.9895,\n",
       "                      0.9979, 1.0210, 0.9819, 0.9804, 0.9984, 0.9889, 1.0045, 1.0098, 0.9789,\n",
       "                      1.0041, 1.0094, 0.9775, 1.0055, 1.0037, 1.0030, 1.0056, 1.0108, 0.9903,\n",
       "                      0.9934, 1.0235, 0.9878, 0.9850, 1.0124, 0.9763, 1.0043, 0.9984, 1.0050,\n",
       "                      1.0061, 1.0141, 1.0181, 0.9968, 0.9904, 0.9975, 1.0006, 1.0010, 0.9832,\n",
       "                      1.0102, 0.9804, 0.9992, 1.0095, 1.0019, 0.9929, 1.0046, 1.0183, 0.9881,\n",
       "                      1.0087, 1.0108, 1.0034, 1.0192, 0.9927, 0.9929, 1.0049, 1.0054, 1.0087,\n",
       "                      1.0204, 0.9935, 0.9894, 1.0073, 1.0327, 0.9943, 1.0107, 0.9777, 0.9813,\n",
       "                      0.9848, 0.9891, 1.0038, 1.0108, 0.9877, 0.9967, 0.9859, 0.9964, 0.9942,\n",
       "                      0.9729, 1.0075, 1.0025, 0.9992, 1.0118, 0.9862, 1.0012, 0.9914, 0.9992,\n",
       "                      0.9803, 1.0273, 0.9919, 0.9992, 1.0131, 1.0193, 1.0009, 1.0160, 0.9995,\n",
       "                      1.0039, 0.9900, 0.9993, 0.9969, 1.0051, 0.9991, 1.0007, 0.9831, 1.0177,\n",
       "                      1.0114, 1.0088, 0.9893, 1.0098, 0.9955, 1.0291, 1.0814, 1.0069, 1.0102,\n",
       "                      0.9997, 0.9746, 1.0019, 1.0203, 1.0028, 0.9918, 0.9966, 0.9854, 0.9953,\n",
       "                      0.9978, 1.0033, 0.9799, 0.9873, 1.0207, 1.0168, 1.0001, 0.9944, 0.9925,\n",
       "                      1.0179, 1.0199, 1.0069, 1.0012, 1.0070, 0.9964, 1.0116, 0.9991, 0.9938,\n",
       "                      1.0058, 0.9993, 1.0177, 1.0070, 1.0086, 0.9915, 1.0112, 1.0334, 1.0042,\n",
       "                      1.0010, 0.9980, 1.0008, 0.9918, 0.9945, 1.0071, 1.0036, 0.9992, 0.9842,\n",
       "                      1.0209, 0.9958, 0.9972, 1.0150, 0.9843, 1.0032, 1.0015, 0.9853, 1.0124,\n",
       "                      0.9703, 0.9929, 0.9885, 1.0012, 0.9931, 0.9992, 0.9987, 1.0050, 1.0089,\n",
       "                      1.0459, 1.0073, 1.0178, 1.0022, 0.9988, 1.0096, 1.0119, 0.9976, 1.0180,\n",
       "                      0.9995, 1.0110, 1.0014])),\n",
       "             ('bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       "              tensor([ 0.0011,  0.0092,  0.0136,  0.0015, -0.0048,  0.0047,  0.0008, -0.0063,\n",
       "                      -0.0088, -0.0088, -0.0133,  0.0074,  0.0136, -0.0102, -0.0015, -0.0056,\n",
       "                       0.0105, -0.0073, -0.0072,  0.0021,  0.0271, -0.0016, -0.0029,  0.0160,\n",
       "                      -0.0210,  0.0023,  0.0045,  0.0195, -0.0145,  0.0055, -0.0030,  0.0040,\n",
       "                       0.0034,  0.0023, -0.0015,  0.0075, -0.0144, -0.0169, -0.0040,  0.0020,\n",
       "                      -0.0097, -0.0086, -0.0054, -0.0023,  0.0055,  0.0006,  0.0362, -0.0031,\n",
       "                      -0.0022,  0.0021,  0.0011, -0.0221, -0.0204,  0.0163,  0.0032,  0.0021,\n",
       "                      -0.0036,  0.0046,  0.0026,  0.0100, -0.0025, -0.0033,  0.0079, -0.0096,\n",
       "                      -0.0015,  0.0044, -0.0032,  0.0239,  0.0049,  0.0051,  0.0092, -0.0007,\n",
       "                       0.0091,  0.0050, -0.0006, -0.0043, -0.0049,  0.0089,  0.0003, -0.0072,\n",
       "                       0.0089,  0.0022, -0.0012, -0.0048,  0.0036, -0.0057, -0.0013, -0.0102,\n",
       "                      -0.0084,  0.0045,  0.0080,  0.0103, -0.0046, -0.0062, -0.0069,  0.0430,\n",
       "                      -0.0042,  0.0133, -0.0008, -0.0071,  0.0008, -0.0032, -0.0065, -0.0074,\n",
       "                      -0.0111,  0.0017,  0.0087,  0.0023, -0.0016,  0.0059, -0.0078,  0.0006,\n",
       "                       0.0005,  0.0019, -0.0300,  0.0061,  0.0074,  0.0087,  0.0189,  0.0010,\n",
       "                      -0.0171,  0.0239,  0.0045,  0.0017, -0.0095,  0.0040,  0.0076,  0.0143,\n",
       "                       0.0081,  0.0066,  0.0146,  0.0071, -0.0116,  0.0130, -0.0061, -0.0029,\n",
       "                      -0.0219,  0.0146, -0.0113,  0.0035,  0.0032,  0.0085,  0.0121, -0.0020,\n",
       "                      -0.0076,  0.0052,  0.0052, -0.0051,  0.0059, -0.0042, -0.0006,  0.0230,\n",
       "                      -0.0090, -0.0011,  0.0173,  0.0034, -0.0161,  0.0147, -0.0219, -0.0240,\n",
       "                      -0.0061, -0.0038, -0.0052, -0.0073,  0.0155, -0.0084,  0.0082, -0.0058,\n",
       "                       0.0013, -0.0008, -0.0137,  0.0109, -0.0082, -0.0046, -0.0052,  0.0163,\n",
       "                      -0.0177, -0.0220,  0.0018,  0.0059,  0.0433, -0.0034, -0.0056, -0.0026,\n",
       "                      -0.0117,  0.0008, -0.0108, -0.0127,  0.0091,  0.0303, -0.0125, -0.0051])),\n",
       "             ('bert.encoder.layer.0.intermediate.dense.weight',\n",
       "              tensor([[ 0.0105,  0.0292, -0.0311,  ..., -0.0180,  0.0316,  0.0179],\n",
       "                      [ 0.0126, -0.0294,  0.0054,  ..., -0.0362,  0.0027,  0.0272],\n",
       "                      [-0.0436,  0.0101, -0.0140,  ...,  0.0532,  0.0494,  0.0199],\n",
       "                      ...,\n",
       "                      [-0.0135, -0.0350, -0.0136,  ...,  0.0175,  0.0316,  0.0215],\n",
       "                      [-0.0331, -0.0293,  0.0069,  ..., -0.0034, -0.0197,  0.0216],\n",
       "                      [-0.0265, -0.0024, -0.0049,  ...,  0.0540,  0.0015,  0.0354]])),\n",
       "             ('bert.encoder.layer.0.intermediate.dense.bias',\n",
       "              tensor([-3.2676e-03,  1.5740e-02,  2.5168e-03, -4.4320e-03, -1.7908e-02,\n",
       "                      -1.4207e-02, -4.6492e-03,  2.5546e-03,  2.0954e-03,  6.6505e-03,\n",
       "                      -1.0801e-02, -1.7033e-02, -7.9235e-03,  1.3635e-03, -2.4826e-02,\n",
       "                      -2.1213e-02, -2.1545e-02, -2.1433e-02,  7.3253e-03,  2.7945e-03,\n",
       "                      -1.2160e-02,  4.2447e-04, -1.6700e-02, -1.4234e-03, -8.0554e-03,\n",
       "                       6.3586e-03, -2.8682e-03,  4.1546e-03,  1.5343e-03,  9.1187e-03,\n",
       "                       4.5775e-03,  1.7218e-05, -1.6978e-03,  7.4146e-03,  7.3831e-03,\n",
       "                       6.3125e-05,  1.2460e-04, -1.0196e-02, -1.9113e-02, -1.9991e-03,\n",
       "                      -9.4491e-03, -6.5111e-03, -4.6159e-03,  8.6926e-03,  2.5704e-02,\n",
       "                      -4.2394e-03, -1.0733e-02,  2.3712e-04, -1.3447e-02, -4.9313e-03,\n",
       "                       1.0035e-03, -3.1578e-03, -1.6162e-02, -2.1415e-02, -8.3641e-03,\n",
       "                       2.0528e-03, -1.8347e-02, -1.3777e-02, -3.1622e-03,  1.8576e-03,\n",
       "                      -1.7951e-02, -2.0124e-02, -1.1639e-02,  1.8948e-03])),\n",
       "             ('bert.encoder.layer.0.output.dense.weight',\n",
       "              tensor([[ 0.0007,  0.0242,  0.0201,  ..., -0.0191,  0.0343, -0.0077],\n",
       "                      [-0.0310,  0.0032, -0.0133,  ..., -0.0273,  0.0281, -0.0325],\n",
       "                      [-0.0139,  0.0295, -0.0226,  ..., -0.0182,  0.0285, -0.0051],\n",
       "                      ...,\n",
       "                      [ 0.0010, -0.0225, -0.0305,  ..., -0.0096,  0.0427, -0.0481],\n",
       "                      [-0.0113,  0.0010, -0.0067,  ..., -0.0075, -0.0039,  0.0244],\n",
       "                      [-0.0237, -0.0203, -0.0437,  ...,  0.0190,  0.0215,  0.0205]])),\n",
       "             ('bert.encoder.layer.0.output.dense.bias',\n",
       "              tensor([-1.1541e-03,  2.8083e-03,  8.4689e-03,  7.8023e-03, -3.1392e-03,\n",
       "                      -7.4482e-04,  1.6012e-03, -5.9817e-03, -6.1237e-04, -2.9313e-03,\n",
       "                      -8.0040e-03,  8.5565e-03, -1.4522e-04,  5.2200e-03,  4.1833e-03,\n",
       "                      -5.8069e-03,  8.2707e-03, -1.8176e-03, -3.2056e-03, -1.6719e-03,\n",
       "                       3.7285e-03, -3.8127e-03,  4.4154e-03,  6.2890e-04, -1.2367e-02,\n",
       "                      -4.6936e-04,  7.2805e-03,  7.1557e-03, -4.7719e-03,  2.7404e-03,\n",
       "                      -6.2292e-04,  1.5635e-03,  4.5201e-03, -5.8806e-03, -4.6220e-03,\n",
       "                      -2.1767e-03, -5.7005e-03, -1.2770e-02, -5.2035e-03,  4.6032e-03,\n",
       "                      -2.3275e-03, -1.5994e-02, -2.8904e-03, -1.9153e-03,  3.8457e-03,\n",
       "                      -4.2325e-03,  1.0140e-02,  6.4553e-03, -3.9696e-03,  3.4662e-03,\n",
       "                      -5.3907e-03, -6.9111e-03, -1.2481e-02,  3.3951e-03,  3.7496e-03,\n",
       "                       1.9912e-03, -5.9045e-03, -1.3115e-02,  4.6206e-03,  7.5099e-03,\n",
       "                      -6.2640e-03,  9.1977e-04,  6.4330e-03, -4.5354e-03,  9.0350e-03,\n",
       "                       6.5401e-03, -2.8602e-03,  1.4057e-02,  2.7492e-03,  7.4691e-04,\n",
       "                       1.4137e-03,  1.9437e-03, -9.8289e-03,  7.1227e-03, -1.2035e-03,\n",
       "                       2.1392e-03, -8.2753e-04,  8.6093e-03,  8.8582e-04, -2.6926e-03,\n",
       "                       3.2863e-03,  2.8492e-03,  3.5852e-03, -6.0705e-03,  7.4491e-03,\n",
       "                      -4.0605e-03, -7.8663e-03, -2.8296e-03, -2.2653e-03,  2.4339e-03,\n",
       "                      -1.8622e-03,  4.5385e-03, -2.9594e-03,  3.7103e-03, -2.5929e-03,\n",
       "                       2.8032e-02, -3.9511e-04,  4.8739e-03, -3.3103e-03, -4.2394e-03,\n",
       "                       5.7444e-04, -2.0627e-03, -4.8808e-03, -2.9688e-03, -4.7696e-03,\n",
       "                      -3.6657e-03,  6.7958e-03, -8.4046e-05, -5.3713e-03,  8.6438e-03,\n",
       "                       3.5921e-03,  2.1478e-03,  5.2189e-03, -2.9879e-03, -2.4102e-02,\n",
       "                       5.6554e-03, -6.5150e-04,  7.7739e-03,  5.2723e-03,  1.9572e-03,\n",
       "                      -3.1278e-03,  1.3843e-02, -1.2241e-03,  1.9605e-03, -9.8605e-03,\n",
       "                       6.4532e-04,  5.2868e-03, -4.8394e-04, -5.8444e-03,  2.2925e-03,\n",
       "                       5.1965e-03,  4.2180e-03, -1.0333e-02,  3.2390e-03, -1.6436e-03,\n",
       "                       4.6552e-03, -4.7990e-03,  4.6055e-03, -8.2663e-03,  4.4874e-03,\n",
       "                       6.2418e-03,  9.9904e-03,  4.7146e-03, -2.3858e-04, -1.7664e-03,\n",
       "                       4.0254e-03, -4.9507e-03, -2.3578e-03,  4.5227e-03, -4.7647e-03,\n",
       "                       6.3824e-04,  7.7556e-03, -1.8908e-03,  4.4356e-03,  7.8154e-03,\n",
       "                      -2.1529e-03, -1.0359e-02, -3.3988e-03, -1.2163e-02, -9.1683e-03,\n",
       "                      -5.2532e-03, -2.9347e-03, -1.2756e-03,  2.4777e-03,  7.9140e-03,\n",
       "                      -1.5277e-03,  1.9642e-03, -6.0516e-03,  2.6351e-03, -1.9903e-03,\n",
       "                      -1.8167e-03,  4.8880e-03, -3.6292e-03,  1.1912e-03,  1.0462e-02,\n",
       "                       9.2070e-04, -9.5774e-03, -1.4510e-02,  2.0948e-03,  1.5968e-03,\n",
       "                       3.0147e-02, -8.1495e-04,  1.5845e-03, -5.5101e-04, -4.4856e-03,\n",
       "                       1.6054e-03, -5.9828e-03, -1.9548e-03, -2.2913e-03,  1.1910e-02,\n",
       "                       2.2277e-03,  4.5404e-03])),\n",
       "             ('bert.encoder.layer.0.output.LayerNorm.weight',\n",
       "              tensor([1.0051, 1.0188, 1.0021, 1.0029, 1.0052, 1.0099, 1.0103, 1.0256, 0.9993,\n",
       "                      1.0066, 1.0155, 0.9932, 0.9889, 1.0035, 1.0055, 1.0135, 1.0217, 0.9826,\n",
       "                      1.0092, 1.0182, 0.9919, 1.0112, 1.0135, 1.0120, 1.0150, 1.0180, 1.0026,\n",
       "                      0.9991, 1.0189, 1.0006, 0.9986, 1.0192, 0.9906, 1.0139, 1.0107, 1.0116,\n",
       "                      1.0150, 1.0057, 1.0259, 1.0084, 1.0035, 1.0091, 1.0175, 1.0129, 0.9979,\n",
       "                      1.0177, 0.9893, 1.0136, 1.0147, 1.0148, 1.0030, 1.0010, 1.0085, 1.0007,\n",
       "                      1.0192, 1.0196, 1.0133, 1.0208, 1.0026, 1.0037, 1.0173, 1.0152, 1.0161,\n",
       "                      1.0235, 1.0045, 1.0019, 1.0155, 1.0219, 1.0044, 1.0161, 0.9893, 0.9950,\n",
       "                      0.9957, 1.0018, 1.0153, 1.0204, 0.9988, 1.0058, 0.9997, 1.0058, 0.9997,\n",
       "                      0.9894, 1.0190, 1.0134, 1.0116, 1.0191, 0.9989, 1.0087, 1.0033, 1.0084,\n",
       "                      0.9931, 1.0269, 1.0060, 1.0065, 1.0203, 1.0142, 1.0083, 1.0083, 1.0095,\n",
       "                      1.0105, 1.0006, 1.0107, 1.0071, 1.0146, 1.0121, 1.0098, 0.9962, 1.0257,\n",
       "                      1.0220, 1.0229, 0.9919, 1.0195, 1.0079, 1.0307, 1.0853, 1.0141, 1.0190,\n",
       "                      1.0109, 0.9849, 1.0191, 1.0206, 1.0061, 1.0029, 1.0052, 0.9973, 1.0053,\n",
       "                      1.0107, 1.0111, 0.9962, 1.0008, 1.0111, 1.0186, 1.0082, 1.0026, 1.0011,\n",
       "                      1.0298, 1.0211, 1.0178, 1.0014, 1.0165, 1.0097, 1.0228, 1.0102, 1.0056,\n",
       "                      1.0109, 1.0096, 1.0176, 1.0165, 1.0188, 1.0047, 1.0221, 1.0187, 1.0171,\n",
       "                      1.0107, 1.0085, 1.0119, 1.0039, 0.9999, 1.0202, 1.0017, 1.0117, 0.9905,\n",
       "                      1.0266, 1.0084, 1.0103, 1.0275, 0.9993, 1.0146, 1.0092, 1.0037, 1.0188,\n",
       "                      0.9869, 1.0059, 0.9990, 1.0088, 1.0016, 1.0095, 0.9945, 1.0122, 1.0175,\n",
       "                      1.0487, 1.0189, 1.0265, 1.0102, 1.0106, 1.0177, 1.0202, 1.0122, 1.0236,\n",
       "                      1.0007, 1.0138, 1.0147])),\n",
       "             ('bert.encoder.layer.0.output.LayerNorm.bias',\n",
       "              tensor([ 2.4965e-03,  9.9289e-03,  1.4612e-02, -1.8387e-03, -4.5989e-03,\n",
       "                       5.9319e-03,  9.7949e-04, -6.4234e-03, -7.7372e-03, -9.6409e-03,\n",
       "                      -1.1581e-02,  7.7522e-03,  1.0613e-02, -1.0374e-02,  3.9659e-03,\n",
       "                      -6.3791e-03,  1.0847e-02, -4.9689e-03, -1.0826e-02,  5.5542e-03,\n",
       "                       2.3975e-02,  1.5858e-03, -5.4230e-03,  1.5191e-02, -2.1409e-02,\n",
       "                       3.9596e-03,  2.6640e-03,  2.3155e-02, -1.4755e-02,  5.9651e-03,\n",
       "                      -1.9005e-03,  2.9955e-03, -2.3146e-03, -2.6220e-03, -4.4517e-04,\n",
       "                       7.9241e-03, -1.3159e-02, -1.8491e-02, -1.9479e-03, -6.5231e-04,\n",
       "                      -7.0530e-03, -6.7508e-03, -3.8177e-03, -1.8869e-03,  4.5619e-03,\n",
       "                       2.3755e-04,  2.9794e-02, -2.8506e-03, -2.0783e-03,  1.1217e-03,\n",
       "                      -2.0292e-04, -2.7616e-02, -1.8587e-02,  1.4897e-02,  3.4851e-03,\n",
       "                       3.1814e-03, -4.5617e-03,  5.2952e-03,  2.8130e-03,  8.8014e-03,\n",
       "                      -4.8223e-03, -3.5239e-03,  7.6556e-03, -1.2581e-02, -2.7235e-03,\n",
       "                       3.9602e-03, -2.6663e-03,  2.7993e-02,  3.7521e-03,  6.9152e-03,\n",
       "                       9.2580e-03, -5.9755e-03,  6.0532e-03,  6.3041e-03,  8.1887e-04,\n",
       "                      -3.5579e-03, -6.1349e-03,  8.1283e-03,  2.3709e-03, -5.5032e-03,\n",
       "                       1.0910e-02,  3.6297e-03, -3.7467e-03, -3.2223e-03,  2.2943e-03,\n",
       "                      -8.2635e-03, -2.2977e-04, -1.2236e-02, -3.3859e-03,  3.2151e-03,\n",
       "                       3.0545e-03,  1.3007e-02, -3.8157e-03, -1.1201e-03, -1.3805e-02,\n",
       "                       4.3768e-02, -4.9118e-03,  1.3196e-02, -2.4276e-03, -7.5538e-03,\n",
       "                       2.4784e-04,  3.1240e-03, -6.1701e-03, -5.7597e-03, -7.4147e-03,\n",
       "                       6.8276e-03,  6.8316e-03,  5.7707e-03, -2.0149e-03,  7.7508e-03,\n",
       "                      -9.7367e-03,  3.0144e-03,  2.2646e-03,  1.2649e-03, -3.5865e-02,\n",
       "                       5.1569e-03,  1.0846e-02,  6.8343e-03,  1.9823e-02, -1.8833e-03,\n",
       "                      -1.9113e-02,  2.1236e-02,  5.2729e-03,  5.6793e-04, -7.7928e-03,\n",
       "                       2.7660e-03,  8.9066e-03,  1.4760e-02,  1.1422e-02,  5.5066e-03,\n",
       "                       1.5357e-02,  3.6758e-03, -7.6264e-03,  8.2930e-03, -8.6535e-03,\n",
       "                      -3.0826e-03, -2.4012e-02,  1.2803e-02, -9.6555e-03,  3.9518e-03,\n",
       "                       7.2228e-04,  6.7408e-03,  1.4308e-02, -5.9388e-05, -7.0997e-03,\n",
       "                       2.3825e-03,  8.7147e-03, -4.4665e-03,  5.5107e-03, -4.6774e-03,\n",
       "                      -9.1186e-04,  2.3721e-02, -9.7420e-03, -3.2208e-03,  1.6280e-02,\n",
       "                       5.0146e-03, -1.6099e-02,  1.3009e-02, -1.9796e-02, -2.6376e-02,\n",
       "                      -6.7449e-03, -3.3654e-03, -7.5900e-03, -5.5938e-03,  1.0854e-02,\n",
       "                      -4.3641e-03,  6.3417e-03, -8.4304e-04,  8.4358e-04, -2.7515e-03,\n",
       "                      -1.5226e-02,  7.6739e-03, -5.8820e-03, -4.9675e-03, -3.7272e-03,\n",
       "                       1.4422e-02, -1.5219e-02, -2.7035e-02,  4.2537e-03,  6.4363e-03,\n",
       "                       5.4620e-02, -2.9861e-03, -9.8101e-03, -4.7977e-03, -1.0407e-02,\n",
       "                       7.4940e-04, -1.2044e-02, -1.0359e-02,  1.1275e-02,  2.9873e-02,\n",
       "                      -1.4933e-02, -4.4175e-03])),\n",
       "             ('bert.encoder.layer.1.attention.self.query.weight',\n",
       "              tensor([[ 0.0121, -0.0148, -0.0280,  ...,  0.0270, -0.0185,  0.0720],\n",
       "                      [-0.0092,  0.0286,  0.0199,  ...,  0.0213, -0.0055, -0.0382],\n",
       "                      [ 0.0242,  0.0078,  0.0033,  ..., -0.0394, -0.0113,  0.0357],\n",
       "                      ...,\n",
       "                      [ 0.0264, -0.0763,  0.0497,  ...,  0.0199, -0.0455,  0.0247],\n",
       "                      [ 0.0200, -0.0034,  0.0214,  ..., -0.0393, -0.0265, -0.0096],\n",
       "                      [-0.0028, -0.0324, -0.0440,  ..., -0.0338, -0.0296,  0.0298]])),\n",
       "             ('bert.encoder.layer.1.attention.self.query.bias',\n",
       "              tensor([-0.0246, -0.0333,  0.0120, -0.0184,  0.0505,  0.0289,  0.0164,  0.0679,\n",
       "                      -0.0379,  0.0265, -0.0382,  0.0098, -0.0446, -0.0556, -0.0287,  0.0178,\n",
       "                      -0.0559, -0.0285, -0.0375,  0.0027, -0.0637,  0.0338, -0.0318, -0.0218,\n",
       "                       0.0212,  0.0216,  0.0030, -0.0006, -0.0641, -0.0428,  0.0441,  0.0462,\n",
       "                       0.0361,  0.0153,  0.0130, -0.0108,  0.0349, -0.0539, -0.0419,  0.0137,\n",
       "                       0.0465,  0.0327,  0.0849,  0.0500,  0.0641, -0.0108,  0.0398,  0.0415,\n",
       "                       0.0864, -0.0133, -0.0528,  0.0225,  0.0334,  0.0388, -0.0336, -0.0317,\n",
       "                      -0.0515, -0.0647, -0.0185, -0.0605, -0.0252, -0.0188,  0.0589,  0.0427,\n",
       "                      -0.1093,  0.0853, -0.0713, -0.0755, -0.0994,  0.0609,  0.0850, -0.0413,\n",
       "                       0.0699,  0.0866, -0.0552, -0.0840, -0.0674,  0.0546, -0.0468, -0.0527,\n",
       "                      -0.0968,  0.0866, -0.0123,  0.0527,  0.0642, -0.0511,  0.0320, -0.1049,\n",
       "                      -0.0818,  0.0547,  0.0985,  0.0547, -0.0721, -0.0708, -0.0864,  0.0470,\n",
       "                       0.0026, -0.0070,  0.0313, -0.0124,  0.0583,  0.0048,  0.0340, -0.0169,\n",
       "                       0.0052, -0.0271, -0.0421, -0.0265,  0.0042, -0.0156,  0.0126, -0.0180,\n",
       "                      -0.0318,  0.0201,  0.0288,  0.0057, -0.0296,  0.0511,  0.0244, -0.0369,\n",
       "                      -0.0335, -0.0584, -0.0342, -0.0134, -0.0256,  0.0515,  0.0461,  0.0048,\n",
       "                       0.0253,  0.0102,  0.0193, -0.0266,  0.0481,  0.0236,  0.0349, -0.0295,\n",
       "                      -0.0470,  0.0385,  0.0444, -0.0127, -0.0004, -0.0215,  0.0203, -0.0094,\n",
       "                       0.0710, -0.0212, -0.0158, -0.0149, -0.0341, -0.0060,  0.0217,  0.0481,\n",
       "                       0.0314,  0.0213,  0.0252, -0.0294, -0.0611,  0.0390,  0.0094,  0.0210,\n",
       "                      -0.0058,  0.0637, -0.0491,  0.0117,  0.0410, -0.0544, -0.0445, -0.0331,\n",
       "                      -0.0432, -0.0236, -0.0238, -0.0004,  0.0067,  0.0152,  0.0355, -0.0157,\n",
       "                       0.0170, -0.0243, -0.0568,  0.0296, -0.0012, -0.0565, -0.0143, -0.0074,\n",
       "                       0.0233,  0.0112, -0.0065, -0.0088,  0.0149,  0.0169, -0.0421, -0.0423])),\n",
       "             ('bert.encoder.layer.1.attention.self.key.weight',\n",
       "              tensor([[ 0.0153, -0.0145, -0.0194,  ..., -0.0262,  0.0105, -0.0319],\n",
       "                      [ 0.0067,  0.0500, -0.0133,  ...,  0.0253,  0.0122,  0.0109],\n",
       "                      [ 0.0115, -0.0337, -0.0183,  ..., -0.0266,  0.0183,  0.0072],\n",
       "                      ...,\n",
       "                      [-0.0192,  0.0017,  0.0204,  ...,  0.0048, -0.0209,  0.0228],\n",
       "                      [-0.0030,  0.0005,  0.0102,  ...,  0.0174, -0.0414, -0.0066],\n",
       "                      [ 0.0056,  0.0099, -0.0077,  ..., -0.0215, -0.0040,  0.0365]])),\n",
       "             ('bert.encoder.layer.1.attention.self.key.bias',\n",
       "              tensor([ 1.7278e-09,  2.8299e-08, -4.2695e-08,  5.7883e-08, -3.8401e-08,\n",
       "                       1.2252e-07, -3.7698e-08,  1.3317e-07,  4.9798e-08, -4.8591e-08,\n",
       "                      -1.5867e-08,  5.6281e-08, -2.7406e-07, -7.7629e-08,  5.0530e-08,\n",
       "                      -1.1695e-07,  5.3209e-08, -9.3465e-09, -3.6348e-08, -1.2045e-07,\n",
       "                       6.6943e-08,  3.2412e-08,  8.3262e-08,  3.0897e-08, -1.5200e-08,\n",
       "                      -2.8109e-08, -7.7940e-08, -6.3100e-08,  9.2838e-08,  1.4817e-07,\n",
       "                       6.1493e-08,  1.1218e-09,  1.4876e-07,  1.4707e-09, -5.3203e-08,\n",
       "                       7.8938e-08,  2.6753e-08,  5.4591e-08, -1.3481e-08, -3.3979e-09,\n",
       "                      -4.3827e-08, -1.9251e-07,  4.3561e-08,  9.1805e-08, -3.7739e-08,\n",
       "                      -4.8627e-08, -2.5264e-08, -2.1783e-07, -8.0118e-08,  5.6339e-08,\n",
       "                       6.2654e-08, -2.8503e-08, -4.5995e-08,  1.2572e-07,  3.5186e-08,\n",
       "                      -4.7592e-08,  7.7872e-08,  1.5698e-07,  1.5538e-07, -5.7922e-08,\n",
       "                       4.7649e-08,  1.9140e-08, -2.3249e-08,  2.4166e-08, -1.0161e-07,\n",
       "                      -8.3065e-08,  2.0512e-08,  6.4457e-08, -1.3777e-08,  1.6428e-07,\n",
       "                      -2.0331e-07,  1.3957e-07, -2.4845e-07, -8.3527e-08,  2.7633e-10,\n",
       "                       8.6448e-09,  9.1088e-08, -3.7945e-07, -5.9800e-08,  1.8994e-07,\n",
       "                       1.6393e-07,  6.6211e-08, -2.7363e-07, -2.1476e-07,  5.0421e-08,\n",
       "                      -2.6133e-07,  3.5035e-08,  9.1488e-08,  4.5140e-08, -2.5021e-07,\n",
       "                      -8.6917e-08,  1.7661e-07,  1.5306e-07,  3.6402e-07, -5.2474e-08,\n",
       "                       1.5421e-07, -2.3518e-07,  2.0242e-07, -8.1467e-08, -2.9717e-07,\n",
       "                      -3.0158e-09,  9.8084e-08,  2.6315e-07, -7.9069e-08, -1.8554e-07,\n",
       "                      -2.4398e-07, -1.1597e-07, -2.2761e-07,  3.2487e-07, -5.3442e-08,\n",
       "                      -1.1844e-07,  2.9556e-07,  1.8860e-07, -2.5295e-07,  5.4911e-07,\n",
       "                       2.2744e-07,  6.8110e-08,  1.6553e-07, -2.2070e-07,  1.9622e-07,\n",
       "                       1.2044e-07,  1.4340e-07, -1.6823e-07, -1.5586e-07,  1.7456e-07,\n",
       "                       5.7062e-09, -2.1127e-07, -1.8141e-07,  2.6568e-07,  1.2021e-07,\n",
       "                       1.1137e-07,  3.3375e-08, -2.6827e-08, -1.2239e-07, -1.7073e-08,\n",
       "                      -6.1245e-08,  1.0802e-07,  7.8774e-10, -1.7258e-07,  1.1796e-07,\n",
       "                       5.8795e-08, -1.0802e-08, -9.7460e-08, -3.7602e-08, -7.4079e-08,\n",
       "                       1.4871e-07, -2.2828e-07,  1.2815e-07,  2.7902e-07,  6.1509e-08,\n",
       "                      -6.0872e-08,  1.0926e-07, -1.6075e-07, -1.8937e-07, -1.4980e-08,\n",
       "                       6.5935e-08,  8.0571e-09, -1.1742e-07, -2.7802e-07, -1.0543e-07,\n",
       "                       9.7049e-08,  4.3215e-08, -9.4291e-08, -8.6896e-08, -4.7383e-08,\n",
       "                       1.1824e-07,  1.1774e-07,  1.5675e-07, -3.6874e-09,  1.3147e-07,\n",
       "                       7.6269e-08,  1.4688e-09,  3.2477e-08,  8.5254e-08,  2.1537e-08,\n",
       "                      -1.8745e-07, -2.1070e-09,  1.4236e-08,  4.0043e-08,  5.7924e-08,\n",
       "                       7.2302e-08, -3.6953e-09,  7.2665e-08,  4.7579e-09, -3.3049e-07,\n",
       "                      -2.0417e-08, -8.8313e-08,  5.5553e-08,  1.0818e-07, -7.5785e-08,\n",
       "                      -3.6790e-08, -3.2666e-08])),\n",
       "             ('bert.encoder.layer.1.attention.self.value.weight',\n",
       "              tensor([[-0.0309, -0.0059,  0.0457,  ..., -0.0306, -0.0260, -0.0340],\n",
       "                      [-0.0073, -0.0216,  0.0043,  ..., -0.0124,  0.0153,  0.0363],\n",
       "                      [-0.0494,  0.0249, -0.0291,  ...,  0.0165,  0.0010,  0.0219],\n",
       "                      ...,\n",
       "                      [ 0.0150, -0.0174, -0.0076,  ...,  0.0169, -0.0012,  0.0200],\n",
       "                      [ 0.0156,  0.0299, -0.0179,  ..., -0.0003, -0.0005, -0.0265],\n",
       "                      [ 0.0009, -0.0488, -0.0093,  ..., -0.0021,  0.0129, -0.0309]])),\n",
       "             ('bert.encoder.layer.1.attention.self.value.bias',\n",
       "              tensor([-0.0105,  0.0034, -0.0017, -0.0081, -0.0043,  0.0097,  0.0028, -0.0071,\n",
       "                       0.0012,  0.0020, -0.0152, -0.0016,  0.0041, -0.0059,  0.0073, -0.0012,\n",
       "                      -0.0034, -0.0003,  0.0023,  0.0080, -0.0107, -0.0005, -0.0018,  0.0025,\n",
       "                       0.0050,  0.0030,  0.0077, -0.0007, -0.0092, -0.0057, -0.0005, -0.0013,\n",
       "                      -0.0068, -0.0126, -0.0064,  0.0005, -0.0019,  0.0016,  0.0012, -0.0010,\n",
       "                      -0.0047,  0.0034,  0.0133, -0.0082, -0.0053,  0.0016, -0.0037,  0.0033,\n",
       "                      -0.0034,  0.0060,  0.0061, -0.0042,  0.0089, -0.0008, -0.0117, -0.0090,\n",
       "                      -0.0068,  0.0046,  0.0040,  0.0107, -0.0020,  0.0079,  0.0029,  0.0052,\n",
       "                      -0.0146, -0.0141, -0.0003,  0.0108,  0.0015, -0.0003,  0.0221, -0.0166,\n",
       "                      -0.0030, -0.0121, -0.0050, -0.0078,  0.0025,  0.0029,  0.0007, -0.0004,\n",
       "                       0.0046,  0.0024,  0.0036, -0.0016, -0.0030, -0.0181, -0.0060,  0.0094,\n",
       "                       0.0064,  0.0005, -0.0119,  0.0059, -0.0016,  0.0134, -0.0015, -0.0131,\n",
       "                       0.0006,  0.0020, -0.0016,  0.0007, -0.0052,  0.0037,  0.0072,  0.0030,\n",
       "                      -0.0028,  0.0115,  0.0087, -0.0034, -0.0030, -0.0104, -0.0099,  0.0052,\n",
       "                      -0.0092, -0.0235,  0.0062, -0.0068, -0.0017,  0.0017,  0.0090, -0.0081,\n",
       "                      -0.0172,  0.0040, -0.0095, -0.0105, -0.0010, -0.0072, -0.0091,  0.0003,\n",
       "                       0.0077,  0.0051,  0.0063,  0.0025,  0.0129, -0.0086, -0.0011, -0.0054,\n",
       "                      -0.0039,  0.0095, -0.0036, -0.0041, -0.0095, -0.0037, -0.0039, -0.0040,\n",
       "                      -0.0110,  0.0042, -0.0044,  0.0035, -0.0075,  0.0012, -0.0071, -0.0090,\n",
       "                      -0.0110,  0.0139,  0.0095, -0.0006,  0.0018,  0.0165, -0.0004, -0.0002,\n",
       "                      -0.0034, -0.0090,  0.0032,  0.0006, -0.0124,  0.0047,  0.0007,  0.0012,\n",
       "                      -0.0009, -0.0003,  0.0083,  0.0013,  0.0018, -0.0010, -0.0115,  0.0049,\n",
       "                       0.0033, -0.0002,  0.0007,  0.0008, -0.0059, -0.0080, -0.0060, -0.0040,\n",
       "                      -0.0047,  0.0047, -0.0076, -0.0049, -0.0012,  0.0022,  0.0033, -0.0041])),\n",
       "             ('bert.encoder.layer.1.attention.output.dense.weight',\n",
       "              tensor([[ 0.0146,  0.0179, -0.0378,  ...,  0.0101,  0.0024,  0.0129],\n",
       "                      [-0.0314, -0.0156,  0.0436,  ..., -0.0212,  0.0104, -0.0326],\n",
       "                      [ 0.0058,  0.0092,  0.0279,  ..., -0.0281, -0.0310, -0.0164],\n",
       "                      ...,\n",
       "                      [ 0.0356, -0.0020,  0.0375,  ...,  0.0032,  0.0291, -0.0205],\n",
       "                      [ 0.0398, -0.0113,  0.0189,  ..., -0.0027, -0.0161,  0.0025],\n",
       "                      [-0.0197,  0.0376,  0.0342,  ...,  0.0377,  0.0041,  0.0180]])),\n",
       "             ('bert.encoder.layer.1.attention.output.dense.bias',\n",
       "              tensor([ 0.0113,  0.0055,  0.0099,  0.0018, -0.0088, -0.0047, -0.0027, -0.0044,\n",
       "                      -0.0053, -0.0033, -0.0081,  0.0039,  0.0019, -0.0088, -0.0102, -0.0019,\n",
       "                       0.0024, -0.0062,  0.0021,  0.0005,  0.0154,  0.0006,  0.0021,  0.0061,\n",
       "                      -0.0126,  0.0011,  0.0034,  0.0172, -0.0158,  0.0007, -0.0083,  0.0015,\n",
       "                       0.0042, -0.0006,  0.0014,  0.0021, -0.0088, -0.0152, -0.0062,  0.0016,\n",
       "                      -0.0041, -0.0029, -0.0114, -0.0070,  0.0029, -0.0026,  0.0170, -0.0031,\n",
       "                       0.0012, -0.0035,  0.0021, -0.0154, -0.0163,  0.0083,  0.0005,  0.0024,\n",
       "                      -0.0052, -0.0042,  0.0040,  0.0072,  0.0057, -0.0058, -0.0021, -0.0037,\n",
       "                       0.0042,  0.0018, -0.0019,  0.0246,  0.0033,  0.0013,  0.0071,  0.0092,\n",
       "                       0.0029, -0.0024, -0.0007, -0.0072, -0.0090,  0.0024,  0.0023, -0.0079,\n",
       "                       0.0037,  0.0017,  0.0046,  0.0056,  0.0004, -0.0060,  0.0021, -0.0066,\n",
       "                      -0.0115,  0.0031,  0.0124, -0.0007, -0.0040,  0.0013, -0.0022,  0.0339,\n",
       "                      -0.0036,  0.0126,  0.0036, -0.0066, -0.0008, -0.0006, -0.0018, -0.0005,\n",
       "                      -0.0138,  0.0050,  0.0041, -0.0022,  0.0031,  0.0024, -0.0059,  0.0047,\n",
       "                      -0.0044, -0.0008, -0.0344, -0.0016,  0.0034,  0.0043,  0.0087,  0.0057,\n",
       "                      -0.0104,  0.0151,  0.0039, -0.0005, -0.0048,  0.0020, -0.0012,  0.0069,\n",
       "                       0.0032,  0.0071,  0.0090,  0.0041, -0.0052,  0.0043, -0.0011, -0.0053,\n",
       "                      -0.0136,  0.0090, -0.0065, -0.0002,  0.0036,  0.0066,  0.0077, -0.0027,\n",
       "                      -0.0014,  0.0018,  0.0068, -0.0057,  0.0027, -0.0061, -0.0038,  0.0140,\n",
       "                      -0.0019, -0.0013,  0.0116, -0.0005, -0.0065,  0.0095, -0.0131, -0.0181,\n",
       "                      -0.0027,  0.0052, -0.0064, -0.0068,  0.0083, -0.0091,  0.0106, -0.0012,\n",
       "                       0.0023, -0.0015, -0.0023,  0.0067, -0.0043,  0.0008,  0.0005,  0.0077,\n",
       "                      -0.0093, -0.0134, -0.0019,  0.0008,  0.0449, -0.0040, -0.0026,  0.0037,\n",
       "                      -0.0109, -0.0061, -0.0020, -0.0046, -0.0008,  0.0178, -0.0095, -0.0050])),\n",
       "             ('bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9991, 1.0088, 0.9925, 0.9986, 0.9961, 0.9953, 1.0039, 1.0121, 0.9819,\n",
       "                      1.0073, 0.9998, 0.9865, 0.9876, 1.0010, 0.9833, 1.0040, 1.0126, 0.9644,\n",
       "                      0.9997, 1.0138, 0.9850, 1.0115, 1.0063, 1.0071, 1.0051, 1.0164, 0.9945,\n",
       "                      0.9900, 1.0087, 0.9990, 0.9821, 1.0146, 0.9773, 1.0076, 1.0022, 0.9967,\n",
       "                      1.0020, 0.9935, 1.0084, 1.0044, 0.9921, 1.0029, 1.0070, 0.9979, 0.9925,\n",
       "                      1.0163, 0.9851, 1.0053, 1.0094, 1.0049, 0.9933, 0.9922, 1.0031, 0.9935,\n",
       "                      1.0124, 1.0137, 1.0015, 1.0190, 1.0030, 0.9922, 1.0009, 1.0018, 1.0128,\n",
       "                      1.0072, 0.9961, 0.9997, 1.0011, 1.0107, 0.9965, 1.0127, 0.9826, 0.9764,\n",
       "                      0.9907, 0.9964, 1.0068, 1.0004, 0.9989, 0.9959, 0.9972, 0.9972, 0.9947,\n",
       "                      0.9862, 1.0083, 1.0064, 1.0132, 1.0056, 0.9949, 1.0041, 0.9903, 1.0019,\n",
       "                      0.9789, 1.0132, 1.0008, 0.9995, 1.0092, 1.0028, 1.0025, 0.9931, 1.0031,\n",
       "                      1.0068, 0.9896, 1.0052, 1.0045, 1.0068, 1.0029, 0.9966, 0.9924, 1.0083,\n",
       "                      1.0145, 1.0186, 0.9854, 1.0120, 1.0007, 1.0182, 1.0806, 1.0031, 1.0075,\n",
       "                      1.0062, 0.9871, 1.0092, 1.0122, 0.9873, 0.9960, 0.9955, 0.9966, 0.9984,\n",
       "                      0.9979, 1.0049, 0.9912, 0.9947, 1.0046, 1.0062, 1.0026, 0.9927, 0.9979,\n",
       "                      1.0164, 1.0123, 1.0097, 0.9839, 1.0134, 0.9994, 1.0097, 1.0027, 1.0044,\n",
       "                      0.9996, 0.9990, 1.0032, 1.0069, 1.0098, 0.9997, 1.0070, 1.0093, 1.0039,\n",
       "                      1.0055, 1.0018, 1.0140, 0.9971, 0.9861, 1.0091, 0.9929, 0.9987, 0.9783,\n",
       "                      1.0146, 0.9993, 1.0036, 1.0172, 0.9808, 1.0110, 1.0021, 0.9779, 1.0047,\n",
       "                      0.9721, 1.0029, 0.9919, 1.0025, 0.9909, 1.0027, 0.9823, 1.0074, 1.0016,\n",
       "                      1.0445, 1.0085, 1.0131, 1.0099, 1.0008, 1.0085, 1.0116, 1.0030, 1.0113,\n",
       "                      0.9916, 1.0110, 1.0030])),\n",
       "             ('bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       "              tensor([ 4.6567e-03,  1.1426e-02,  1.7615e-02, -1.0493e-04, -1.0929e-02,\n",
       "                      -1.2592e-03,  4.9358e-03, -6.8695e-03, -3.2709e-03, -1.4050e-02,\n",
       "                      -1.3854e-02,  8.1499e-03,  1.1625e-02, -1.0953e-02, -3.5545e-03,\n",
       "                      -7.0247e-03,  7.9568e-03, -5.9367e-03, -6.2988e-03,  1.5027e-03,\n",
       "                       2.5181e-02,  1.6039e-03, -6.0946e-03,  1.4691e-02, -2.2751e-02,\n",
       "                       2.7107e-03,  5.1689e-03,  2.2187e-02, -1.9826e-02,  2.6938e-03,\n",
       "                      -5.8102e-03,  8.0372e-04,  5.1729e-03, -5.4825e-04,  1.2556e-03,\n",
       "                       1.2307e-02, -1.2607e-02, -2.1822e-02, -3.4489e-03, -3.3975e-03,\n",
       "                      -8.0884e-03, -6.5244e-03, -4.1549e-03, -3.8142e-03,  2.4421e-03,\n",
       "                      -2.5587e-03,  3.0798e-02, -5.8685e-03, -6.3805e-04,  3.2830e-03,\n",
       "                       4.5943e-03, -3.0478e-02, -2.0790e-02,  1.3390e-02, -8.8787e-04,\n",
       "                       2.7060e-03, -7.3103e-03,  4.3377e-03,  2.5342e-03,  1.0774e-02,\n",
       "                      -3.0169e-03, -2.4602e-03,  4.3731e-03, -1.4978e-02,  1.4094e-03,\n",
       "                       2.1052e-03, -6.3426e-03,  2.9992e-02,  5.5445e-03,  7.0480e-03,\n",
       "                       1.0812e-02,  2.2550e-03,  8.2163e-03,  3.8872e-03,  6.4337e-04,\n",
       "                      -3.1150e-03, -5.7264e-03,  8.9725e-03,  1.5234e-03, -7.4519e-03,\n",
       "                       1.0284e-02,  1.5266e-03, -6.8218e-04, -3.5564e-03,  4.7392e-04,\n",
       "                      -1.2310e-02,  8.6565e-04, -1.2356e-02, -1.0996e-02, -1.1845e-03,\n",
       "                       1.0630e-02,  1.2946e-02, -2.6120e-03, -4.3699e-03, -1.2487e-02,\n",
       "                       4.8086e-02,  3.1617e-04,  1.7356e-02,  8.4355e-04, -7.6341e-03,\n",
       "                      -1.9356e-03,  2.0499e-03, -6.7299e-03, -6.3223e-03, -1.1525e-02,\n",
       "                       6.8112e-03,  6.8404e-03,  2.3665e-03,  1.5718e-03,  7.1154e-03,\n",
       "                      -9.0604e-03,  2.6828e-03,  6.1383e-04, -1.1991e-03, -4.7157e-02,\n",
       "                       5.8849e-03,  1.3415e-02,  5.8932e-03,  1.9717e-02,  1.3009e-03,\n",
       "                      -1.7944e-02,  2.2673e-02,  8.1233e-03,  2.1510e-03, -9.8563e-03,\n",
       "                       3.7611e-03,  6.9540e-03,  1.2731e-02,  6.6753e-03,  5.1067e-03,\n",
       "                       2.0281e-02,  9.2704e-03, -1.2784e-02,  8.6820e-03, -4.7860e-03,\n",
       "                      -5.3452e-03, -2.5838e-02,  1.5866e-02, -7.1452e-03,  3.8926e-03,\n",
       "                       4.9030e-03,  3.9685e-03,  8.5157e-03, -2.4593e-03, -6.5407e-03,\n",
       "                       5.1684e-05,  8.1105e-03, -4.4477e-03,  4.6119e-03, -6.9298e-03,\n",
       "                      -3.2611e-03,  2.3294e-02, -1.2582e-02, -4.1587e-03,  2.0101e-02,\n",
       "                       6.6396e-04, -1.4043e-02,  2.0416e-02, -1.6970e-02, -2.6693e-02,\n",
       "                      -6.4742e-03, -1.1131e-03, -8.8920e-03, -1.0518e-02,  1.3575e-02,\n",
       "                      -4.1972e-03,  1.3424e-02, -3.1029e-03,  4.3044e-04, -5.7201e-03,\n",
       "                      -1.4331e-02,  1.0370e-02, -7.5623e-03, -1.5648e-03, -5.3000e-03,\n",
       "                       1.5363e-02, -1.4065e-02, -2.3828e-02,  2.0101e-03,  5.5367e-03,\n",
       "                       5.9286e-02, -2.1760e-03, -1.0383e-02, -1.1923e-04, -9.8837e-03,\n",
       "                       1.3286e-03, -1.0750e-02, -1.2353e-02,  1.2038e-02,  2.8720e-02,\n",
       "                      -1.9549e-02, -6.8650e-03])),\n",
       "             ('bert.encoder.layer.1.intermediate.dense.weight',\n",
       "              tensor([[ 0.0017,  0.0220,  0.0015,  ...,  0.0009, -0.0266,  0.0099],\n",
       "                      [ 0.0151,  0.0290,  0.0078,  ...,  0.0469,  0.0112,  0.0035],\n",
       "                      [-0.0033, -0.0134, -0.0294,  ..., -0.0020,  0.0058, -0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0138,  0.0124,  0.0270,  ..., -0.0116, -0.0441, -0.0312],\n",
       "                      [ 0.0106,  0.0225, -0.0204,  ..., -0.0201, -0.0010,  0.0079],\n",
       "                      [-0.0047, -0.0228,  0.0036,  ...,  0.0195,  0.0271,  0.0459]])),\n",
       "             ('bert.encoder.layer.1.intermediate.dense.bias',\n",
       "              tensor([-2.1708e-03, -2.8875e-03, -6.9701e-03,  1.2859e-02, -8.9607e-03,\n",
       "                      -4.0310e-04, -3.4221e-03,  4.9135e-03,  1.1593e-02,  4.0530e-03,\n",
       "                       5.3280e-03,  2.2134e-02,  4.2970e-04, -1.2732e-02,  3.4590e-04,\n",
       "                      -1.4397e-02, -8.5449e-04, -1.2381e-02, -1.0339e-02,  4.2784e-03,\n",
       "                      -2.3496e-03,  1.3284e-02,  3.1484e-03,  7.8775e-03,  9.6858e-03,\n",
       "                       7.8692e-03, -1.5826e-02,  1.2955e-02,  3.5781e-03, -7.9990e-03,\n",
       "                      -1.9524e-05, -7.2202e-03, -7.1471e-03, -6.6254e-03,  4.9196e-03,\n",
       "                       8.7553e-03, -4.6637e-03, -1.0644e-02, -8.3703e-03,  1.0566e-02,\n",
       "                      -1.6694e-02, -3.1253e-03, -1.6489e-02, -3.3667e-03,  1.0528e-02,\n",
       "                      -3.4996e-03, -2.9655e-03, -6.3279e-03, -4.4249e-04, -6.0534e-03,\n",
       "                      -2.6128e-03,  1.1245e-02, -1.3536e-04,  1.8982e-03, -4.9873e-03,\n",
       "                      -3.0052e-03,  5.6974e-03, -6.1524e-03, -7.4845e-03,  8.9226e-03,\n",
       "                       1.1007e-03,  3.9097e-03,  4.8221e-03,  5.7029e-03])),\n",
       "             ('bert.encoder.layer.1.output.dense.weight',\n",
       "              tensor([[-0.0074, -0.0170,  0.0075,  ..., -0.0012,  0.0472,  0.0083],\n",
       "                      [ 0.0377, -0.0310, -0.0071,  ..., -0.0081,  0.0271,  0.0145],\n",
       "                      [-0.0210, -0.0406,  0.0057,  ..., -0.0082, -0.0305,  0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0312, -0.0115,  0.0311,  ..., -0.0300,  0.0411,  0.0365],\n",
       "                      [-0.0207, -0.0067, -0.0058,  ..., -0.0047,  0.0069, -0.0405],\n",
       "                      [-0.0078,  0.0076,  0.0125,  ..., -0.0207, -0.0265, -0.0167]])),\n",
       "             ('bert.encoder.layer.1.output.dense.bias',\n",
       "              tensor([ 4.9147e-03,  1.5967e-03,  1.0557e-02,  3.2440e-03, -7.3011e-04,\n",
       "                      -2.3971e-03,  8.6296e-04, -3.9624e-03, -1.1629e-03, -8.0903e-03,\n",
       "                      -8.1150e-03,  7.9219e-03,  1.2876e-02, -2.0480e-03, -8.2965e-04,\n",
       "                       1.2939e-04,  2.1394e-03,  3.7159e-03,  3.7205e-04,  2.2744e-03,\n",
       "                       9.4320e-03, -4.4102e-04,  9.1937e-04, -1.0608e-03, -9.3021e-03,\n",
       "                      -5.4745e-03,  1.2902e-03,  8.0526e-03, -7.8466e-03,  2.2542e-03,\n",
       "                       5.4486e-03, -6.4215e-03, -3.6908e-03,  3.5482e-04, -3.8146e-03,\n",
       "                       2.0514e-03, -3.8112e-03, -1.2359e-02, -5.3261e-03, -1.1992e-02,\n",
       "                      -1.1970e-03, -4.8613e-03, -5.9912e-03, -3.7496e-03,  1.9999e-03,\n",
       "                       8.9281e-04,  1.5034e-02, -4.4457e-03, -4.5706e-03,  4.7438e-03,\n",
       "                       1.6809e-03, -1.0418e-02, -1.1913e-02,  3.4752e-03,  3.0220e-03,\n",
       "                      -1.5480e-03,  2.6629e-03, -6.1265e-03,  2.4220e-03,  1.5429e-03,\n",
       "                       7.9535e-04,  1.3175e-03,  6.8080e-03,  1.7772e-04,  4.9506e-03,\n",
       "                       1.9622e-03, -5.4037e-03,  1.8710e-02,  2.1382e-03, -1.9073e-03,\n",
       "                       8.0102e-03,  6.0341e-03,  4.6134e-03,  8.2562e-03, -6.9269e-03,\n",
       "                      -3.2262e-03, -2.8967e-03,  4.1409e-03, -4.3718e-03, -1.0213e-03,\n",
       "                      -4.2106e-04,  6.4502e-03,  4.6540e-03, -5.6771e-03, -2.6180e-03,\n",
       "                      -1.3266e-02, -4.2423e-03, -9.8534e-04, -1.3088e-03, -2.5386e-03,\n",
       "                       3.1972e-03, -1.0048e-03, -8.2076e-03, -9.1562e-04, -1.0929e-03,\n",
       "                       2.9192e-02,  3.5730e-03,  5.7118e-03,  4.0574e-03,  3.0757e-03,\n",
       "                      -5.6813e-04,  6.9661e-04, -2.0057e-03, -8.5972e-03, -8.8646e-03,\n",
       "                      -1.0040e-03,  4.4822e-03, -3.8214e-03, -1.6841e-03,  5.8666e-03,\n",
       "                       5.6620e-04,  1.7981e-03,  3.0357e-03, -5.7140e-03, -3.9111e-02,\n",
       "                       7.3722e-03,  3.0988e-03,  3.4307e-03,  1.5619e-02,  6.2887e-03,\n",
       "                      -6.6377e-03,  8.9743e-03, -8.5931e-04,  5.7650e-03, -1.8721e-03,\n",
       "                       3.1171e-03,  3.3601e-03,  3.3343e-03,  2.6444e-03,  1.1591e-03,\n",
       "                       9.3667e-03,  6.9341e-03, -5.2289e-03,  6.7915e-03, -2.8622e-03,\n",
       "                      -3.8132e-03, -1.2929e-02,  1.1604e-02, -2.5298e-03,  5.9203e-03,\n",
       "                       1.9558e-03,  7.0384e-03,  6.2567e-03,  1.7364e-03, -4.7774e-03,\n",
       "                       2.6791e-03,  5.1787e-05,  6.2241e-03,  2.1761e-04, -2.6320e-03,\n",
       "                      -1.2165e-03,  1.1063e-02, -5.7397e-03, -4.1972e-03,  1.0688e-02,\n",
       "                       2.8361e-03, -1.1212e-02,  1.0985e-02, -8.8178e-03, -1.3510e-02,\n",
       "                      -2.8895e-03,  3.4365e-03, -1.4901e-03,  1.1578e-03,  1.3235e-02,\n",
       "                      -5.0684e-03,  4.0452e-03, -1.5149e-03,  2.5080e-03, -2.2583e-03,\n",
       "                      -4.2634e-03,  3.3498e-03, -5.9955e-03, -5.5655e-03, -2.4412e-03,\n",
       "                       4.7313e-03, -1.2742e-02, -9.6739e-03, -1.9231e-03,  5.2667e-03,\n",
       "                       4.0277e-02, -2.2843e-03, -6.3794e-03, -2.7629e-03, -9.7185e-03,\n",
       "                      -5.2139e-04, -1.1152e-02, -1.5167e-03, -2.6306e-03,  1.1338e-02,\n",
       "                      -5.7400e-04, -3.2870e-03])),\n",
       "             ('bert.encoder.layer.1.output.LayerNorm.weight',\n",
       "              tensor([1.0106, 1.0245, 1.0083, 1.0068, 1.0091, 1.0007, 1.0172, 1.0285, 0.9922,\n",
       "                      1.0174, 1.0020, 0.9991, 1.0037, 1.0088, 1.0027, 1.0127, 1.0227, 0.9829,\n",
       "                      0.9994, 1.0256, 1.0022, 1.0246, 1.0128, 1.0194, 1.0154, 1.0274, 1.0118,\n",
       "                      0.9925, 1.0037, 1.0171, 1.0018, 1.0255, 1.0003, 1.0193, 1.0172, 1.0031,\n",
       "                      1.0126, 0.9883, 1.0210, 1.0146, 1.0074, 1.0149, 1.0196, 1.0154, 1.0091,\n",
       "                      1.0278, 0.9966, 1.0229, 1.0194, 1.0200, 1.0077, 0.9829, 0.9839, 1.0134,\n",
       "                      1.0243, 1.0195, 1.0160, 1.0227, 1.0179, 1.0070, 1.0129, 1.0130, 1.0249,\n",
       "                      1.0095, 1.0124, 1.0149, 1.0161, 0.9845, 1.0096, 1.0153, 0.9948, 0.9973,\n",
       "                      1.0069, 1.0120, 1.0206, 1.0102, 1.0134, 1.0076, 1.0099, 1.0117, 1.0029,\n",
       "                      1.0043, 1.0214, 1.0168, 1.0255, 1.0171, 1.0091, 1.0114, 1.0060, 1.0130,\n",
       "                      0.9963, 1.0181, 1.0143, 1.0118, 1.0124, 0.9872, 1.0124, 0.9892, 1.0165,\n",
       "                      1.0159, 1.0026, 1.0178, 1.0159, 1.0186, 1.0138, 1.0118, 1.0121, 1.0199,\n",
       "                      1.0260, 1.0325, 0.9959, 1.0235, 1.0172, 1.0227, 1.0032, 1.0165, 1.0170,\n",
       "                      1.0158, 1.0004, 1.0271, 1.0135, 0.9900, 1.0088, 1.0091, 1.0097, 1.0114,\n",
       "                      1.0126, 1.0199, 1.0075, 1.0094, 0.9921, 1.0178, 1.0170, 0.9999, 1.0078,\n",
       "                      1.0289, 1.0116, 1.0232, 0.9918, 1.0278, 1.0162, 1.0163, 1.0143, 1.0171,\n",
       "                      1.0012, 1.0124, 1.0058, 1.0176, 1.0214, 1.0164, 1.0175, 0.9925, 1.0205,\n",
       "                      1.0199, 1.0220, 1.0276, 1.0088, 0.9989, 1.0207, 0.9844, 1.0100, 0.9903,\n",
       "                      1.0200, 1.0137, 1.0157, 1.0284, 1.0026, 1.0235, 1.0144, 0.9975, 1.0107,\n",
       "                      0.9946, 1.0160, 1.0098, 1.0153, 1.0039, 1.0201, 0.9765, 1.0178, 1.0109,\n",
       "                      0.9871, 1.0220, 1.0232, 1.0214, 1.0154, 1.0214, 1.0222, 1.0168, 1.0170,\n",
       "                      0.9970, 1.0175, 1.0191])),\n",
       "             ('bert.encoder.layer.1.output.LayerNorm.bias',\n",
       "              tensor([ 4.7838e-03,  9.2875e-03,  1.5588e-02, -1.1936e-03, -6.5596e-03,\n",
       "                      -2.3390e-03,  4.6002e-03, -7.6771e-03,  8.3929e-04, -1.2531e-02,\n",
       "                      -7.0834e-03,  6.7596e-03,  5.6777e-03, -5.4789e-03,  4.4231e-03,\n",
       "                      -4.3397e-03,  7.1818e-03,  2.3758e-03, -6.8010e-03,  4.4563e-03,\n",
       "                       1.6272e-02,  2.6625e-03, -3.5433e-03,  9.3269e-03, -1.5590e-02,\n",
       "                       3.7159e-04,  3.8490e-03,  1.8406e-02, -1.2694e-02,  1.8037e-03,\n",
       "                       1.9439e-03,  7.8224e-05, -1.2720e-03, -4.1243e-03,  1.1673e-03,\n",
       "                       8.2445e-03, -1.0949e-02, -1.8405e-02, -8.1656e-04, -7.9526e-03,\n",
       "                      -4.5551e-03, -3.2771e-03, -4.0931e-04,  1.7777e-03,  6.0147e-05,\n",
       "                      -2.7246e-03,  1.8954e-02, -2.1575e-03, -3.6151e-04,  3.0750e-03,\n",
       "                      -1.6758e-03, -2.0992e-02, -1.2262e-02,  8.1553e-03, -2.7313e-04,\n",
       "                       3.3530e-04, -5.1829e-03,  3.1338e-03, -1.0630e-03,  6.7070e-03,\n",
       "                      -5.0095e-03, -1.3352e-03,  9.4812e-03, -1.2195e-02,  2.1334e-04,\n",
       "                      -7.1667e-04, -5.0779e-03,  2.3775e-02,  2.8310e-03,  5.5980e-03,\n",
       "                       8.7873e-03, -3.9402e-03,  2.0190e-03,  4.7332e-03,  2.8578e-03,\n",
       "                       1.0013e-04, -3.0281e-03,  8.7997e-03,  4.8269e-04, -4.4506e-03,\n",
       "                       2.9802e-03,  1.0649e-03, -4.1210e-03, -1.6752e-03, -4.9637e-04,\n",
       "                      -1.2101e-02, -1.2527e-03, -8.9903e-03, -1.5297e-03, -7.5474e-04,\n",
       "                       1.9755e-04,  1.0610e-02,  5.4583e-04, -3.5086e-03, -1.3418e-02,\n",
       "                       3.3512e-02,  4.4276e-05,  1.1468e-02,  9.1766e-04, -7.1152e-03,\n",
       "                      -7.3256e-03,  8.8283e-03, -5.9651e-03, -4.7070e-03, -6.7600e-03,\n",
       "                       4.9340e-03,  2.5161e-03,  6.0451e-03,  3.1706e-04,  6.6769e-03,\n",
       "                      -1.0414e-02,  4.1434e-03,  3.5273e-03, -3.8538e-03, -2.5601e-02,\n",
       "                       7.1071e-03,  1.0356e-02,  4.2122e-03,  1.7512e-02, -3.8300e-03,\n",
       "                      -1.3688e-02,  1.0256e-02,  6.0881e-03,  3.0463e-03, -1.0022e-02,\n",
       "                       1.5341e-03,  8.8213e-03,  9.7604e-03,  7.8621e-03,  1.4402e-03,\n",
       "                       1.5050e-02,  9.9926e-03, -5.3964e-03,  4.6129e-03, -3.4691e-03,\n",
       "                      -2.7020e-03, -1.7917e-02,  1.0017e-02, -4.9841e-03,  6.3815e-03,\n",
       "                       2.4935e-03,  6.0441e-03,  9.1135e-03,  1.4490e-03, -4.5292e-03,\n",
       "                      -2.6756e-03,  5.0388e-03, -1.4199e-03,  5.4633e-03, -5.0610e-03,\n",
       "                      -1.6378e-03,  1.6677e-02, -8.4573e-03, -3.9287e-03,  1.2297e-02,\n",
       "                       2.0968e-03, -1.4358e-02,  9.1655e-03, -1.1390e-02, -1.8608e-02,\n",
       "                      -6.2014e-03, -1.5260e-03, -5.3729e-03, -7.1456e-03,  8.1209e-03,\n",
       "                       2.0281e-05,  6.1759e-03,  3.8615e-03,  2.8710e-03, -8.5374e-03,\n",
       "                      -1.1390e-02,  2.8684e-03, -2.5413e-03, -5.4253e-03, -5.3328e-04,\n",
       "                       7.2471e-03, -9.4070e-03, -1.8876e-02,  3.3137e-03,  6.5132e-03,\n",
       "                       3.5275e-02, -1.0587e-03, -1.0075e-02, -3.3714e-03, -6.2587e-03,\n",
       "                       4.3743e-03, -1.1047e-02, -5.3022e-03,  1.1838e-02,  1.9324e-02,\n",
       "                      -1.4477e-02, -2.5188e-03])),\n",
       "             ('bert.encoder.layer.2.attention.self.query.weight',\n",
       "              tensor([[-1.2574e-02, -1.3522e-02, -6.1337e-02,  ..., -2.9003e-02,\n",
       "                        2.2076e-02,  6.8969e-03],\n",
       "                      [-1.5904e-02,  4.6852e-02,  4.4527e-02,  ...,  5.5211e-02,\n",
       "                       -7.0165e-02, -2.0977e-02],\n",
       "                      [ 2.6699e-02,  8.7610e-05, -5.5250e-02,  ..., -3.8837e-02,\n",
       "                        3.0681e-02,  9.4270e-03],\n",
       "                      ...,\n",
       "                      [-8.9360e-03, -5.0026e-02,  1.3437e-02,  ..., -5.0383e-02,\n",
       "                       -2.5833e-02,  3.4112e-02],\n",
       "                      [ 2.3810e-02,  1.0665e-02, -1.5869e-02,  ..., -5.2982e-02,\n",
       "                        5.1571e-02,  1.6428e-02],\n",
       "                      [-2.9066e-02, -3.8527e-02, -4.6691e-02,  ...,  7.5747e-03,\n",
       "                       -1.9795e-02,  6.9674e-03]])),\n",
       "             ('bert.encoder.layer.2.attention.self.query.bias',\n",
       "              tensor([-0.0500,  0.0801, -0.0535, -0.0474,  0.0632,  0.0630, -0.0597,  0.0450,\n",
       "                       0.0741,  0.0695,  0.0643, -0.0469, -0.0640,  0.0593, -0.0432, -0.0183,\n",
       "                      -0.0512,  0.0566, -0.0622,  0.0347, -0.0634,  0.0607,  0.0634, -0.0853,\n",
       "                      -0.0122,  0.0060,  0.0491,  0.0751,  0.0042,  0.0556,  0.0495, -0.0438,\n",
       "                      -0.0370,  0.0251,  0.0322,  0.0452,  0.0691, -0.0406, -0.0120, -0.0418,\n",
       "                       0.0117, -0.0607,  0.0518, -0.0552, -0.0623, -0.0205, -0.0404,  0.0504,\n",
       "                      -0.0420,  0.0358, -0.0406, -0.0456,  0.0056, -0.0303,  0.0097,  0.0342,\n",
       "                      -0.0188,  0.0362, -0.0631, -0.0287, -0.0507, -0.0275,  0.0107,  0.0147,\n",
       "                      -0.0439, -0.0587,  0.0607,  0.0548, -0.0635,  0.0215,  0.0460, -0.0331,\n",
       "                       0.0552, -0.0618,  0.0189, -0.0635, -0.0442,  0.0028, -0.0064,  0.0010,\n",
       "                       0.0241, -0.0132, -0.0294,  0.0090,  0.0156,  0.0632, -0.0286,  0.0464,\n",
       "                       0.0791,  0.0067, -0.0664,  0.0261,  0.0364,  0.0733,  0.0715,  0.0151,\n",
       "                       0.0260,  0.0135, -0.0377, -0.0599, -0.0149, -0.0164,  0.0135, -0.0004,\n",
       "                      -0.0076, -0.0456, -0.0320,  0.0548,  0.0026, -0.0093,  0.0270,  0.0184,\n",
       "                      -0.0347, -0.0085, -0.0154, -0.0420,  0.0001, -0.0164, -0.0164, -0.0187,\n",
       "                       0.0376,  0.0439,  0.0437,  0.0168, -0.0193, -0.0123,  0.0300, -0.0184,\n",
       "                      -0.0211, -0.0792, -0.0925, -0.0998, -0.0620, -0.0681,  0.0720, -0.0850,\n",
       "                      -0.0858, -0.0892,  0.0746, -0.0724,  0.0026, -0.0672,  0.0266, -0.0816,\n",
       "                      -0.0743, -0.0867, -0.0607,  0.0618,  0.0173, -0.0842,  0.0485, -0.0001,\n",
       "                      -0.0825,  0.0302,  0.0869,  0.0596, -0.0720, -0.0582, -0.0374, -0.0925,\n",
       "                      -0.0296,  0.0213, -0.0314, -0.0208, -0.0371,  0.0149,  0.0258,  0.0250,\n",
       "                       0.0217, -0.0238, -0.0213, -0.0030,  0.0026, -0.0134, -0.0004,  0.0025,\n",
       "                      -0.0161,  0.0406,  0.0083, -0.0329, -0.0066, -0.0256, -0.0099, -0.0137,\n",
       "                       0.0410, -0.0224,  0.0267,  0.0085, -0.0325, -0.0235, -0.0007, -0.0288])),\n",
       "             ('bert.encoder.layer.2.attention.self.key.weight',\n",
       "              tensor([[ 0.0215,  0.0212, -0.0187,  ...,  0.0431, -0.0145,  0.0180],\n",
       "                      [-0.0013, -0.0073, -0.0047,  ..., -0.0325,  0.0118,  0.0191],\n",
       "                      [-0.0087, -0.0035,  0.0030,  ...,  0.0526, -0.0143, -0.0493],\n",
       "                      ...,\n",
       "                      [ 0.0113,  0.0264,  0.0179,  ...,  0.0280, -0.0033,  0.0113],\n",
       "                      [-0.0344,  0.0419,  0.0042,  ...,  0.0295, -0.0240, -0.0010],\n",
       "                      [ 0.0191, -0.0289, -0.0088,  ...,  0.0244,  0.0034,  0.0417]])),\n",
       "             ('bert.encoder.layer.2.attention.self.key.bias',\n",
       "              tensor([ 2.3807e-08, -3.6490e-07,  2.6833e-07,  4.4148e-07, -3.7282e-07,\n",
       "                      -2.0726e-07, -5.5506e-08, -3.5841e-07, -4.2737e-07, -3.4275e-07,\n",
       "                      -4.5654e-07,  2.5364e-08,  2.9728e-07, -2.6307e-07,  2.4482e-07,\n",
       "                       1.2881e-07,  1.5052e-07, -2.1906e-07,  2.9660e-07,  4.3693e-08,\n",
       "                       4.8779e-07, -3.4627e-07, -2.2586e-07,  8.1846e-08,  7.9647e-09,\n",
       "                      -3.9623e-08, -8.1263e-08, -2.1527e-07,  4.3083e-07, -1.9280e-07,\n",
       "                      -1.7498e-07,  2.0192e-07,  3.5540e-08,  1.2338e-07,  3.5806e-07,\n",
       "                       2.4534e-07, -1.6037e-07,  2.5115e-07,  2.1020e-07, -3.2019e-07,\n",
       "                      -3.0760e-07, -1.0147e-07,  1.9588e-07, -2.4085e-07,  3.6577e-08,\n",
       "                      -7.5107e-08, -2.6710e-07, -8.8543e-08, -1.5235e-07,  3.4709e-07,\n",
       "                      -8.3118e-08, -3.4885e-07,  2.2233e-07, -9.0446e-08, -2.4806e-07,\n",
       "                       3.3873e-07,  4.3423e-07,  4.4275e-08, -2.0709e-07,  3.9633e-08,\n",
       "                      -3.1511e-08,  2.4840e-07,  2.8163e-07, -3.4057e-07, -8.8942e-08,\n",
       "                       6.3816e-08,  5.9827e-08,  1.3214e-07, -1.4959e-07,  2.1912e-07,\n",
       "                      -1.2598e-07, -1.7641e-07,  1.5362e-07, -2.3041e-08,  2.2610e-08,\n",
       "                       1.0443e-07, -5.4823e-08, -1.9823e-07,  5.6622e-08,  4.8568e-08,\n",
       "                      -4.2823e-08,  3.0357e-07, -1.6502e-07,  8.2832e-08,  3.0715e-08,\n",
       "                       7.6459e-08, -8.9241e-09, -1.0067e-07, -1.0172e-07, -6.4372e-08,\n",
       "                       2.8501e-08, -8.9314e-08,  7.3685e-08, -1.8857e-08,  4.4999e-08,\n",
       "                      -7.1605e-08, -2.0934e-07, -5.6173e-08,  4.1727e-08,  1.0430e-07,\n",
       "                       2.4979e-07,  4.5455e-09, -1.5597e-07, -1.7146e-07, -7.7716e-08,\n",
       "                       1.0084e-07, -1.7558e-08,  1.3014e-07,  3.3827e-08, -1.9400e-08,\n",
       "                       6.8879e-08, -2.1742e-07,  9.5722e-09, -1.3291e-07, -1.3128e-07,\n",
       "                      -8.2995e-08, -1.7240e-07,  1.8164e-07,  9.9154e-08,  1.2041e-07,\n",
       "                       2.0205e-08, -2.1857e-08,  1.1281e-07,  1.3162e-08, -1.4102e-07,\n",
       "                      -4.8241e-08, -2.0475e-09,  1.6323e-07,  9.7087e-07,  5.8044e-07,\n",
       "                       7.9279e-07,  1.7105e-06,  9.8411e-07,  3.1612e-08, -2.6420e-07,\n",
       "                       1.5691e-06,  9.9436e-07,  1.5156e-06, -7.3319e-07,  1.1895e-06,\n",
       "                       6.4160e-07,  9.8883e-07, -6.1319e-08,  1.0314e-06,  1.2501e-06,\n",
       "                       1.1603e-06,  1.2269e-06, -8.3107e-08, -1.0798e-06,  1.0475e-06,\n",
       "                      -2.0197e-07, -8.1997e-07,  6.5325e-07, -7.3021e-07, -7.7471e-07,\n",
       "                      -2.9624e-07,  7.7085e-07,  2.1382e-07,  1.4019e-07,  9.5979e-07,\n",
       "                      -6.5133e-08,  3.9806e-07,  9.0069e-08, -5.2629e-08, -1.5075e-08,\n",
       "                      -2.0077e-07,  9.7878e-08, -1.2532e-07,  3.3237e-07,  9.8998e-08,\n",
       "                      -3.5406e-07, -7.2726e-08, -3.9200e-08,  1.6535e-07, -2.5153e-07,\n",
       "                      -1.8292e-07, -1.7856e-08, -2.1195e-07, -8.5752e-08, -1.8259e-07,\n",
       "                       1.6636e-07,  2.3118e-07, -1.7066e-07,  1.4180e-07, -1.2704e-07,\n",
       "                       2.0735e-07,  3.5725e-07,  7.6126e-08,  1.6445e-07,  1.4679e-07,\n",
       "                      -2.2092e-07, -7.2159e-09])),\n",
       "             ('bert.encoder.layer.2.attention.self.value.weight',\n",
       "              tensor([[-0.0017, -0.0017,  0.0092,  ..., -0.0064,  0.0126,  0.0113],\n",
       "                      [-0.0331,  0.0457,  0.0174,  ...,  0.0173, -0.0028,  0.0026],\n",
       "                      [ 0.0261, -0.0125, -0.0152,  ...,  0.0526, -0.0108, -0.0273],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0004, -0.0070,  ..., -0.0280, -0.0120,  0.0077],\n",
       "                      [ 0.0353, -0.0127, -0.0257,  ...,  0.0150,  0.0127,  0.0573],\n",
       "                      [-0.0185, -0.0157,  0.0186,  ...,  0.0256, -0.0201, -0.0117]])),\n",
       "             ('bert.encoder.layer.2.attention.self.value.bias',\n",
       "              tensor([-6.1203e-03,  1.5749e-03,  1.3736e-02, -3.1362e-03,  3.0792e-03,\n",
       "                      -8.0274e-03,  6.6394e-03, -1.7808e-03, -8.0275e-04,  1.5520e-02,\n",
       "                       1.4235e-03, -4.0208e-04,  4.9350e-03,  3.5171e-03,  2.5401e-03,\n",
       "                       5.7035e-03,  4.2536e-03,  5.6843e-03,  8.6992e-03, -3.3763e-03,\n",
       "                       4.2730e-03, -1.0847e-02,  4.6361e-03,  2.2404e-03,  9.5751e-03,\n",
       "                      -8.8333e-03, -2.2022e-04,  1.1624e-03,  5.1054e-03,  4.8555e-04,\n",
       "                      -1.4310e-03,  1.1601e-02,  3.1064e-03,  1.0150e-02, -7.1250e-03,\n",
       "                      -8.2728e-03,  7.7956e-04, -1.7584e-04,  9.0539e-03,  3.5477e-03,\n",
       "                       6.6197e-03, -3.4587e-03, -4.7379e-03,  1.0044e-02,  3.1564e-03,\n",
       "                       1.2809e-03, -6.4757e-03,  2.1506e-05, -3.1291e-03, -6.2689e-03,\n",
       "                      -3.5918e-03, -2.7249e-03,  3.0525e-03,  1.3380e-02,  3.4463e-03,\n",
       "                       2.5070e-03,  2.5159e-03,  4.5943e-03, -1.4167e-03,  5.2827e-03,\n",
       "                      -8.0778e-03, -3.3368e-03, -4.9641e-03, -8.2626e-03,  2.4128e-04,\n",
       "                      -1.4208e-03, -9.8169e-03,  3.1131e-03,  6.7223e-03,  1.2824e-03,\n",
       "                       6.0811e-03,  1.6649e-03,  3.9662e-03,  1.9130e-03,  1.3027e-02,\n",
       "                      -9.3953e-03, -1.1012e-03,  3.1281e-03, -1.4562e-03, -1.8281e-03,\n",
       "                       8.6566e-04, -7.5417e-04, -9.3568e-03,  1.3734e-03, -5.2447e-03,\n",
       "                       4.4798e-03,  1.1281e-02, -7.9397e-03, -5.2452e-03,  4.6141e-03,\n",
       "                      -3.7367e-03,  7.1793e-03, -1.7480e-03, -1.0159e-02, -2.3504e-03,\n",
       "                       2.5041e-03,  1.9783e-03, -5.5852e-03, -3.6490e-03,  2.7246e-03,\n",
       "                      -9.7249e-03,  2.2395e-03, -8.2974e-03,  8.8578e-03, -2.1270e-03,\n",
       "                       6.3976e-03, -1.1143e-03,  1.0060e-03, -1.4321e-03, -1.1575e-02,\n",
       "                       1.3729e-02,  2.4705e-03,  7.3730e-03,  4.2582e-04,  1.2148e-04,\n",
       "                      -1.1052e-02, -4.1793e-03, -8.2457e-03, -3.4696e-03,  7.2811e-03,\n",
       "                       1.1515e-02, -6.1337e-03, -1.5103e-03, -6.5767e-04,  1.5088e-03,\n",
       "                      -1.7873e-03,  4.7839e-03, -8.9833e-04, -5.2971e-04,  6.4702e-03,\n",
       "                       9.1218e-03,  7.5860e-03, -1.4914e-02,  8.2525e-03, -1.4581e-02,\n",
       "                      -8.8605e-03, -1.2759e-02, -5.3989e-03, -2.0981e-03, -2.8988e-04,\n",
       "                       3.5431e-03,  5.1413e-04, -2.1964e-03, -4.2987e-03,  5.1754e-03,\n",
       "                       5.8596e-03,  6.2181e-04,  5.8562e-03,  9.2265e-03, -1.6663e-02,\n",
       "                      -1.1396e-03, -4.3969e-03, -6.6926e-04,  1.3604e-02,  1.3688e-03,\n",
       "                      -6.9448e-03, -7.3391e-04,  4.4048e-03,  2.8672e-03,  4.1782e-03,\n",
       "                      -1.1154e-02,  6.5111e-04, -7.1964e-03,  6.8172e-04, -1.1126e-03,\n",
       "                       2.0446e-03, -1.4573e-03, -8.2868e-03, -5.9292e-03, -1.3839e-02,\n",
       "                       8.3862e-03,  1.4832e-03,  4.8643e-03, -1.0027e-02,  3.6430e-03,\n",
       "                       4.0104e-03,  7.0090e-03, -5.1571e-03, -4.1436e-03,  2.8961e-03,\n",
       "                      -3.2851e-03,  6.5103e-03, -3.7050e-03, -7.7685e-03, -3.2450e-03,\n",
       "                       3.1083e-04,  8.3179e-03, -5.7089e-03,  1.3170e-02, -5.3545e-03,\n",
       "                       5.8450e-05,  7.5299e-04])),\n",
       "             ('bert.encoder.layer.2.attention.output.dense.weight',\n",
       "              tensor([[ 0.0357, -0.0477, -0.0080,  ...,  0.0125,  0.0145, -0.0160],\n",
       "                      [-0.0301,  0.0426, -0.0061,  ...,  0.0033, -0.0321,  0.0236],\n",
       "                      [ 0.0292,  0.0203,  0.0132,  ...,  0.0327,  0.0299, -0.0288],\n",
       "                      ...,\n",
       "                      [-0.0096, -0.0007, -0.0446,  ...,  0.0053, -0.0200,  0.0267],\n",
       "                      [ 0.0462, -0.0027, -0.0144,  ..., -0.0514, -0.0189, -0.0170],\n",
       "                      [ 0.0096,  0.0062, -0.0098,  ...,  0.0154, -0.0612, -0.0010]])),\n",
       "             ('bert.encoder.layer.2.attention.output.dense.bias',\n",
       "              tensor([ 4.6935e-03,  5.0308e-03,  6.0777e-03, -3.6876e-03, -4.0258e-03,\n",
       "                      -7.5838e-03,  5.6245e-03, -8.1512e-03,  3.5593e-03,  2.0706e-03,\n",
       "                      -5.3713e-03,  3.4562e-03,  9.8343e-04, -5.6933e-03, -1.8067e-03,\n",
       "                       7.8039e-04,  5.8223e-03, -1.5497e-04,  3.0137e-03,  4.9496e-03,\n",
       "                       5.2403e-03, -3.8633e-04,  1.8857e-03, -3.3873e-03,  1.1133e-03,\n",
       "                      -6.2412e-03, -1.2783e-04,  1.2097e-02, -9.3171e-03,  4.1395e-03,\n",
       "                      -6.7368e-03,  2.4236e-03,  2.5915e-03, -4.0279e-03,  7.5563e-04,\n",
       "                       4.2600e-03, -8.2971e-04, -1.2576e-02, -3.4547e-03, -2.0164e-03,\n",
       "                      -5.1707e-03, -2.8598e-03, -2.0898e-03, -1.9491e-03,  4.3949e-03,\n",
       "                       5.3430e-04,  1.0115e-02,  7.9214e-05,  4.9336e-03, -1.0764e-03,\n",
       "                       4.6255e-03, -7.9620e-03, -1.1316e-02,  1.0008e-02, -3.9782e-04,\n",
       "                       4.7069e-03, -6.8741e-03, -9.7499e-04, -4.7951e-04,  2.5475e-03,\n",
       "                       5.7891e-04,  1.9139e-03,  3.8320e-03, -1.2950e-03, -9.1838e-04,\n",
       "                       1.1693e-03, -3.9080e-03,  2.2116e-02,  2.8334e-03,  5.2969e-03,\n",
       "                       8.6187e-03,  9.7099e-04,  2.0085e-03,  1.4707e-03, -2.6603e-04,\n",
       "                       1.9063e-03, -3.8664e-03,  3.5667e-03, -2.6871e-03, -4.8647e-03,\n",
       "                       1.6606e-03,  4.0860e-03,  1.3452e-03,  2.2136e-03, -5.7457e-03,\n",
       "                      -4.1387e-03, -6.8131e-04, -2.7414e-03, -8.4025e-03,  1.9428e-03,\n",
       "                       3.9155e-03,  4.9837e-03, -6.4095e-04, -1.2716e-03, -1.3816e-02,\n",
       "                       2.5983e-02, -3.3589e-03,  5.8731e-03,  4.0824e-04, -3.6461e-03,\n",
       "                      -4.1912e-03,  8.8738e-04, -1.2482e-03, -3.9733e-03, -4.0781e-03,\n",
       "                       1.6349e-03,  3.4994e-03,  5.3388e-03,  2.5533e-03, -1.3432e-03,\n",
       "                      -6.4071e-03, -2.2049e-04, -6.2831e-04, -6.2863e-04, -2.0772e-02,\n",
       "                       6.8760e-05,  7.3169e-04, -1.0678e-03,  6.1998e-03,  1.0352e-03,\n",
       "                      -5.7821e-03,  8.9516e-03,  6.6535e-03,  5.0808e-04, -2.9750e-03,\n",
       "                       3.5125e-03,  1.0237e-03,  6.6705e-03,  5.7283e-03,  2.4848e-03,\n",
       "                       8.3635e-03,  5.5824e-03,  1.3888e-03, -3.3481e-03, -2.6104e-03,\n",
       "                      -5.4557e-03, -9.2601e-03,  3.0781e-03, -4.2645e-04,  2.1788e-03,\n",
       "                       7.3094e-03,  6.6308e-04,  4.0988e-03, -2.9161e-03,  6.2183e-03,\n",
       "                       3.9160e-03,  6.8887e-04, -2.2143e-03,  2.7560e-03, -4.4939e-03,\n",
       "                      -8.5349e-04,  7.5221e-03, -2.9200e-03,  2.3349e-04,  9.4602e-03,\n",
       "                       1.7702e-03, -5.9834e-03,  1.0689e-02, -1.0163e-02, -1.2445e-02,\n",
       "                       1.6767e-03,  1.6883e-03, -5.1267e-03, -8.8032e-03,  5.6145e-03,\n",
       "                       7.9905e-05,  7.3493e-03,  7.8481e-03,  1.2244e-03, -3.2563e-03,\n",
       "                      -6.0360e-03,  7.9072e-03, -1.0334e-04,  2.8885e-04, -1.4059e-03,\n",
       "                       9.4339e-03, -5.4435e-04, -4.0162e-03, -1.1298e-03, -2.0818e-04,\n",
       "                       2.8869e-02,  1.2944e-03, -5.2075e-03,  2.4490e-03, -6.6130e-03,\n",
       "                      -4.8910e-03, -3.1891e-03, -3.7702e-03,  8.2120e-03,  1.1629e-02,\n",
       "                      -6.0980e-03,  5.6619e-04])),\n",
       "             ('bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0120, 1.0158, 0.9962, 0.9966, 1.0044, 0.9917, 1.0067, 1.0145, 0.9824,\n",
       "                      1.0159, 0.9870, 0.9927, 1.0000, 1.0039, 0.9863, 1.0077, 1.0156, 0.9752,\n",
       "                      0.9915, 1.0224, 1.0013, 1.0196, 0.9996, 1.0053, 1.0027, 1.0182, 1.0034,\n",
       "                      0.9850, 0.9981, 1.0099, 0.9901, 1.0162, 0.9859, 1.0132, 1.0081, 0.9934,\n",
       "                      1.0025, 0.9786, 1.0101, 1.0069, 1.0049, 1.0034, 1.0075, 0.9941, 1.0046,\n",
       "                      1.0276, 0.9962, 1.0153, 1.0029, 1.0146, 1.0047, 0.9759, 0.9800, 1.0107,\n",
       "                      1.0145, 1.0036, 1.0061, 1.0182, 1.0055, 0.9994, 1.0012, 0.9950, 1.0204,\n",
       "                      1.0077, 1.0076, 1.0002, 1.0077, 0.9742, 1.0006, 1.0074, 0.9976, 0.9811,\n",
       "                      0.9954, 1.0035, 1.0107, 1.0006, 1.0036, 1.0056, 1.0089, 1.0105, 1.0025,\n",
       "                      0.9969, 1.0145, 1.0115, 1.0184, 1.0056, 1.0052, 1.0022, 1.0027, 0.9994,\n",
       "                      0.9854, 1.0049, 1.0118, 1.0078, 1.0006, 0.9814, 1.0037, 0.9827, 1.0068,\n",
       "                      1.0120, 0.9947, 1.0191, 1.0101, 1.0127, 1.0054, 0.9987, 1.0025, 1.0109,\n",
       "                      1.0187, 1.0191, 0.9903, 1.0161, 1.0133, 1.0091, 0.9929, 1.0058, 1.0090,\n",
       "                      1.0099, 0.9918, 1.0144, 1.0037, 0.9797, 1.0009, 0.9987, 1.0097, 1.0106,\n",
       "                      1.0042, 1.0067, 1.0053, 1.0021, 0.9860, 1.0026, 1.0083, 0.9898, 1.0026,\n",
       "                      1.0197, 1.0060, 1.0140, 0.9818, 1.0100, 1.0087, 1.0109, 0.9992, 1.0113,\n",
       "                      0.9892, 1.0066, 0.9961, 1.0088, 1.0126, 1.0070, 1.0014, 0.9825, 1.0101,\n",
       "                      1.0162, 1.0108, 1.0250, 0.9982, 0.9940, 1.0102, 0.9742, 1.0001, 0.9854,\n",
       "                      1.0068, 1.0105, 1.0062, 1.0166, 0.9966, 1.0129, 1.0074, 0.9889, 0.9985,\n",
       "                      0.9873, 1.0065, 0.9971, 1.0124, 0.9998, 1.0112, 0.9680, 1.0109, 1.0023,\n",
       "                      0.9821, 1.0101, 1.0051, 1.0152, 1.0120, 1.0024, 1.0124, 1.0080, 1.0048,\n",
       "                      0.9908, 1.0085, 1.0106])),\n",
       "             ('bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       "              tensor([ 0.0050,  0.0099,  0.0125, -0.0035, -0.0112, -0.0035,  0.0045, -0.0055,\n",
       "                       0.0039, -0.0121, -0.0064,  0.0123,  0.0022, -0.0063,  0.0013, -0.0078,\n",
       "                       0.0065,  0.0031, -0.0078,  0.0016,  0.0163,  0.0017, -0.0082,  0.0079,\n",
       "                      -0.0101, -0.0006,  0.0047,  0.0206, -0.0151,  0.0039, -0.0008, -0.0005,\n",
       "                       0.0002, -0.0059, -0.0023,  0.0082, -0.0064, -0.0194,  0.0002, -0.0096,\n",
       "                      -0.0020, -0.0024, -0.0013,  0.0041, -0.0002, -0.0056,  0.0176, -0.0040,\n",
       "                       0.0017,  0.0014,  0.0024, -0.0193, -0.0156,  0.0078, -0.0013,  0.0025,\n",
       "                      -0.0088,  0.0036, -0.0006,  0.0067, -0.0038,  0.0020,  0.0056, -0.0153,\n",
       "                       0.0025, -0.0034, -0.0073,  0.0275,  0.0030,  0.0092,  0.0083,  0.0011,\n",
       "                       0.0051,  0.0032,  0.0021,  0.0067, -0.0039,  0.0063, -0.0020, -0.0041,\n",
       "                       0.0041,  0.0009, -0.0043, -0.0003, -0.0040, -0.0121,  0.0010, -0.0062,\n",
       "                      -0.0054, -0.0011,  0.0023,  0.0137, -0.0004, -0.0007, -0.0138,  0.0381,\n",
       "                      -0.0003,  0.0115,  0.0015, -0.0044, -0.0083,  0.0106, -0.0079, -0.0015,\n",
       "                      -0.0067,  0.0081,  0.0017,  0.0071,  0.0014,  0.0059, -0.0089,  0.0033,\n",
       "                       0.0030, -0.0038, -0.0339,  0.0116,  0.0107, -0.0020,  0.0147, -0.0058,\n",
       "                      -0.0127,  0.0085,  0.0063,  0.0033, -0.0057,  0.0002,  0.0071,  0.0074,\n",
       "                       0.0096, -0.0015,  0.0187,  0.0094, -0.0062,  0.0032, -0.0024, -0.0035,\n",
       "                      -0.0175,  0.0097, -0.0036,  0.0066,  0.0009,  0.0027,  0.0026, -0.0020,\n",
       "                      -0.0039, -0.0025,  0.0062, -0.0026,  0.0026, -0.0071, -0.0029,  0.0198,\n",
       "                      -0.0096, -0.0055,  0.0119,  0.0002, -0.0115,  0.0160, -0.0074, -0.0184,\n",
       "                      -0.0062, -0.0015, -0.0089, -0.0107,  0.0059,  0.0002,  0.0083,  0.0015,\n",
       "                       0.0029, -0.0068, -0.0111,  0.0035, -0.0008, -0.0041, -0.0015,  0.0113,\n",
       "                      -0.0075, -0.0135,  0.0019,  0.0028,  0.0424, -0.0004, -0.0106, -0.0051,\n",
       "                      -0.0034,  0.0011, -0.0128, -0.0063,  0.0142,  0.0197, -0.0159, -0.0020])),\n",
       "             ('bert.encoder.layer.2.intermediate.dense.weight',\n",
       "              tensor([[-0.0320, -0.0336, -0.0015,  ...,  0.0230,  0.0415, -0.0268],\n",
       "                      [-0.0039, -0.0068,  0.0485,  ..., -0.0179, -0.0144,  0.0077],\n",
       "                      [ 0.0284, -0.0066, -0.0234,  ..., -0.0354, -0.0091,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0044, -0.0514,  0.0242,  ..., -0.0048,  0.0416, -0.0434],\n",
       "                      [-0.0116, -0.0149,  0.0098,  ..., -0.0007, -0.0393, -0.0337],\n",
       "                      [ 0.0153, -0.0262, -0.0121,  ..., -0.0337,  0.0270,  0.0208]])),\n",
       "             ('bert.encoder.layer.2.intermediate.dense.bias',\n",
       "              tensor([-0.0081, -0.0105,  0.0005,  0.0210, -0.0078, -0.0063,  0.0030, -0.0189,\n",
       "                      -0.0059,  0.0048,  0.0094, -0.0055, -0.0082, -0.0085,  0.0102, -0.0136,\n",
       "                       0.0077, -0.0086,  0.0055,  0.0013,  0.0014,  0.0068, -0.0040,  0.0014,\n",
       "                      -0.0009, -0.0074, -0.0133, -0.0170, -0.0015, -0.0016, -0.0192, -0.0048,\n",
       "                      -0.0089, -0.0211,  0.0163, -0.0003,  0.0037,  0.0104, -0.0002, -0.0065,\n",
       "                      -0.0233, -0.0077, -0.0034, -0.0054, -0.0201, -0.0164, -0.0077,  0.0006,\n",
       "                      -0.0039, -0.0097,  0.0023,  0.0009,  0.0039,  0.0076, -0.0077, -0.0164,\n",
       "                       0.0054,  0.0046, -0.0156, -0.0168, -0.0050,  0.0055, -0.0033, -0.0053])),\n",
       "             ('bert.encoder.layer.2.output.dense.weight',\n",
       "              tensor([[ 0.0020,  0.0419, -0.0123,  ...,  0.0184, -0.0313, -0.0184],\n",
       "                      [ 0.0250, -0.0098, -0.0013,  ..., -0.0221,  0.0183,  0.0041],\n",
       "                      [ 0.0233, -0.0390,  0.0268,  ..., -0.0542, -0.0058,  0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0349, -0.0097,  0.0605,  ..., -0.0313,  0.0372,  0.0264],\n",
       "                      [ 0.0086,  0.0118,  0.0015,  ..., -0.0440,  0.0035,  0.0227],\n",
       "                      [ 0.0133,  0.0402,  0.0290,  ...,  0.0519, -0.0365, -0.0162]])),\n",
       "             ('bert.encoder.layer.2.output.dense.bias',\n",
       "              tensor([ 2.1823e-03,  6.7116e-03,  1.2704e-02, -2.3155e-03, -9.9336e-03,\n",
       "                      -3.9426e-03,  1.2531e-03, -6.2298e-03,  2.9201e-03, -5.7606e-03,\n",
       "                       3.5482e-03,  1.0180e-02,  4.1485e-04,  1.6579e-03, -1.8932e-03,\n",
       "                      -7.2143e-03,  1.2732e-03,  1.1813e-03, -4.9149e-03,  2.0036e-03,\n",
       "                       7.8071e-03,  1.5414e-03, -1.0039e-02,  8.7046e-03, -6.7112e-03,\n",
       "                      -1.4837e-03,  2.0407e-03,  1.6915e-02, -1.4201e-02,  5.3542e-03,\n",
       "                       5.5476e-04, -3.3299e-03, -3.2254e-04, -5.2180e-03, -4.3836e-04,\n",
       "                       1.1700e-02,  2.2470e-03, -1.4369e-02,  4.5484e-04, -5.7541e-03,\n",
       "                      -2.6257e-03, -2.2439e-03, -1.6325e-03,  7.7926e-03,  1.5040e-03,\n",
       "                      -1.5567e-03,  8.1435e-03, -1.1852e-03, -2.9397e-03,  1.6583e-03,\n",
       "                      -3.1105e-03, -1.5308e-02, -1.2954e-02,  1.9241e-03,  3.0357e-03,\n",
       "                       2.3867e-03, -3.8626e-03,  3.3762e-03,  5.0033e-04,  1.1219e-03,\n",
       "                      -4.0159e-03, -2.4707e-04,  1.4872e-03, -1.0896e-02,  3.5045e-03,\n",
       "                      -1.3419e-03, -5.7721e-03,  2.2291e-02,  2.3085e-03,  7.2689e-03,\n",
       "                       5.5731e-03, -1.8823e-03,  7.4372e-03,  5.7312e-03,  5.3253e-04,\n",
       "                       4.8321e-03, -6.1038e-03,  6.9181e-03, -1.4189e-03, -6.1021e-03,\n",
       "                       8.9835e-03,  5.0800e-03, -3.9200e-03, -4.6759e-03, -9.1324e-03,\n",
       "                      -1.2901e-02,  4.6957e-03, -8.1624e-03, -3.4440e-03, -3.7136e-03,\n",
       "                       9.0541e-03,  5.2773e-03,  2.9927e-03,  1.1578e-05, -1.5563e-02,\n",
       "                       2.8072e-02,  1.2487e-03,  8.8785e-03,  4.7999e-03, -2.4269e-03,\n",
       "                      -9.5283e-03,  4.4689e-03, -7.5395e-03,  1.8939e-03, -2.0008e-03,\n",
       "                       9.1678e-03,  2.7114e-03,  1.0188e-02,  2.2862e-03,  9.3103e-03,\n",
       "                      -4.4861e-03,  7.2991e-03,  3.2751e-04, -2.7185e-03, -3.3558e-02,\n",
       "                       7.2138e-03,  1.1499e-02, -4.4646e-04,  9.3124e-03, -5.9252e-03,\n",
       "                      -6.3088e-03,  8.0512e-03,  4.8593e-03, -1.0386e-03, -3.6733e-03,\n",
       "                       2.8375e-03,  7.4881e-03,  5.6591e-04,  6.0997e-03, -1.6415e-03,\n",
       "                       1.5007e-02,  7.9227e-03, -5.3053e-03,  1.9586e-03, -3.5426e-03,\n",
       "                      -1.9163e-04, -1.5803e-02,  3.0777e-03,  1.0036e-04,  8.9051e-03,\n",
       "                       7.4148e-03,  2.9688e-03,  4.3280e-04, -2.7608e-03,  6.6411e-04,\n",
       "                      -2.4595e-03,  4.5315e-03,  1.7159e-03,  4.9969e-03, -1.1542e-02,\n",
       "                      -4.0834e-03,  9.7642e-03, -1.1036e-02, -5.5086e-03,  3.4306e-03,\n",
       "                      -1.2262e-02, -1.3073e-02,  9.4034e-03, -1.9874e-03, -1.4390e-02,\n",
       "                      -8.2292e-03, -1.9813e-03, -7.4356e-03, -7.9218e-03,  2.8291e-03,\n",
       "                       3.1243e-04,  8.5731e-03, -1.1134e-03,  6.8955e-03, -8.1067e-03,\n",
       "                      -8.2831e-03,  2.0134e-03, -1.0684e-03, -4.9893e-03,  2.0596e-03,\n",
       "                       8.2528e-03, -9.9920e-03, -8.8664e-03, -1.1838e-03,  2.4478e-04,\n",
       "                       3.6016e-02, -5.1380e-04, -4.9852e-03, -8.5846e-03, -6.3456e-03,\n",
       "                      -3.9168e-04, -1.0218e-02, -8.0220e-03,  1.1333e-02,  1.7585e-02,\n",
       "                      -1.1816e-02,  7.5592e-04])),\n",
       "             ('bert.encoder.layer.2.output.LayerNorm.weight',\n",
       "              tensor([1.0261, 1.0291, 1.0091, 1.0046, 1.0172, 0.9965, 1.0193, 1.0279, 0.9985,\n",
       "                      1.0227, 0.9921, 1.0078, 1.0130, 1.0136, 1.0049, 1.0196, 1.0251, 0.9926,\n",
       "                      0.9991, 1.0314, 1.0146, 1.0289, 1.0102, 1.0172, 1.0127, 1.0309, 1.0192,\n",
       "                      0.9900, 0.9981, 1.0283, 1.0089, 1.0305, 1.0003, 1.0254, 1.0217, 1.0010,\n",
       "                      1.0186, 0.9767, 1.0240, 1.0189, 1.0173, 1.0177, 1.0220, 1.0105, 1.0208,\n",
       "                      1.0360, 1.0072, 1.0344, 1.0129, 1.0282, 1.0197, 0.9704, 0.9689, 1.0263,\n",
       "                      1.0281, 1.0175, 1.0198, 1.0240, 1.0195, 1.0124, 1.0132, 1.0058, 1.0350,\n",
       "                      1.0140, 1.0228, 1.0158, 1.0205, 0.9588, 1.0180, 1.0124, 1.0091, 0.9934,\n",
       "                      1.0119, 1.0201, 1.0258, 1.0121, 1.0207, 1.0165, 1.0225, 1.0297, 1.0119,\n",
       "                      1.0139, 1.0271, 1.0258, 1.0345, 1.0172, 1.0205, 1.0125, 1.0188, 1.0118,\n",
       "                      1.0011, 1.0112, 1.0289, 1.0193, 1.0078, 0.9737, 1.0127, 0.9813, 1.0232,\n",
       "                      1.0246, 1.0143, 1.0284, 1.0246, 1.0235, 1.0263, 1.0133, 1.0181, 1.0208,\n",
       "                      1.0337, 1.0341, 0.9948, 1.0286, 1.0270, 1.0180, 0.9453, 1.0161, 1.0169,\n",
       "                      1.0210, 1.0058, 1.0293, 1.0079, 0.9818, 1.0134, 1.0126, 1.0261, 1.0242,\n",
       "                      1.0212, 1.0206, 1.0223, 1.0161, 0.9822, 1.0122, 1.0219, 1.0069, 1.0164,\n",
       "                      1.0337, 1.0069, 1.0250, 0.9894, 1.0288, 1.0232, 1.0212, 1.0150, 1.0265,\n",
       "                      0.9943, 1.0156, 1.0002, 1.0201, 1.0263, 1.0241, 1.0147, 0.9745, 1.0217,\n",
       "                      1.0275, 1.0241, 1.0376, 1.0109, 1.0049, 1.0220, 0.9719, 1.0141, 0.9957,\n",
       "                      1.0153, 1.0246, 1.0154, 1.0306, 1.0114, 1.0236, 1.0230, 1.0026, 1.0020,\n",
       "                      1.0029, 1.0219, 1.0123, 1.0244, 1.0095, 1.0241, 0.9695, 1.0232, 1.0150,\n",
       "                      0.9555, 1.0226, 1.0168, 1.0295, 1.0254, 1.0155, 1.0238, 1.0241, 1.0112,\n",
       "                      0.9951, 1.0133, 1.0277])),\n",
       "             ('bert.encoder.layer.2.output.LayerNorm.bias',\n",
       "              tensor([ 3.3512e-03,  8.8127e-03,  1.1079e-02, -2.1774e-03, -7.6391e-03,\n",
       "                      -5.7844e-03,  2.1004e-03, -6.3386e-03,  5.7248e-03, -1.0400e-02,\n",
       "                       3.4213e-03,  1.1898e-02, -1.3744e-03, -4.8509e-04,  5.2485e-03,\n",
       "                      -3.2251e-03,  6.7184e-03,  7.8344e-03, -5.9025e-03,  5.0458e-04,\n",
       "                       1.1237e-02,  1.6407e-05, -6.8733e-03,  5.7231e-03, -4.9072e-03,\n",
       "                      -2.1673e-03,  5.8187e-03,  1.8759e-02, -7.9952e-03,  4.1126e-03,\n",
       "                       5.0888e-03, -5.7879e-04, -1.9313e-03, -8.7078e-03, -2.8839e-03,\n",
       "                       5.2217e-03, -6.2616e-03, -1.4000e-02,  2.8068e-03, -1.0579e-02,\n",
       "                       8.7289e-04, -3.3214e-03, -3.0084e-03,  7.0316e-03, -8.1127e-04,\n",
       "                      -4.6352e-03,  1.0855e-02, -2.0037e-03, -7.6606e-05,  1.5065e-03,\n",
       "                      -1.1415e-03, -1.2679e-02, -5.8900e-03,  5.7137e-03, -1.0445e-03,\n",
       "                      -1.3402e-03, -6.9456e-03,  1.5331e-03, -1.3555e-03,  3.9589e-03,\n",
       "                      -6.0459e-03, -1.3023e-03,  8.3202e-03, -1.2386e-02,  1.1924e-03,\n",
       "                      -3.5431e-03, -6.2267e-03,  1.9579e-02,  4.4950e-04,  5.3629e-03,\n",
       "                       6.6139e-03, -3.5413e-03,  7.1701e-04,  4.8453e-03,  1.0209e-03,\n",
       "                       7.8815e-03, -1.4818e-03,  6.8524e-03, -6.8403e-04, -2.4926e-03,\n",
       "                       1.0990e-03,  3.1836e-03, -6.6351e-03, -3.7197e-03, -2.6400e-03,\n",
       "                      -1.2280e-02, -2.8555e-03, -6.7070e-03,  1.0534e-03, -3.4360e-03,\n",
       "                      -5.0756e-03,  1.0823e-02,  2.2922e-03,  8.2443e-04, -1.4221e-02,\n",
       "                       2.8686e-02,  4.2564e-04,  2.2809e-03,  1.9577e-03, -4.0882e-03,\n",
       "                      -9.6998e-03,  1.1948e-02, -6.1557e-03, -1.8709e-03, -1.9979e-03,\n",
       "                       8.4086e-03, -5.5107e-04,  8.5797e-03, -2.7344e-03,  6.2103e-03,\n",
       "                      -8.0141e-03,  3.4647e-03,  4.4044e-03, -6.8791e-03, -1.8696e-02,\n",
       "                       1.1568e-02,  7.0576e-03, -2.0883e-03,  1.1952e-02, -9.5230e-03,\n",
       "                      -9.9010e-03, -4.8517e-03,  5.8113e-03,  1.1308e-03, -4.4222e-03,\n",
       "                       9.3849e-04,  8.8469e-03,  6.4021e-03,  1.0976e-02, -2.6073e-03,\n",
       "                       1.0523e-02,  6.8640e-03, -1.3714e-04, -1.0117e-03, -1.8070e-03,\n",
       "                       1.2943e-03, -1.1813e-02,  8.8105e-03,  7.4961e-04,  9.4783e-03,\n",
       "                       5.8836e-04,  3.5473e-03,  5.8790e-03, -4.7261e-04, -2.5436e-03,\n",
       "                      -5.9839e-03,  4.5318e-03, -8.0461e-04,  3.2724e-03, -6.2537e-03,\n",
       "                      -2.0234e-03,  1.1730e-02, -7.5721e-03, -5.3491e-03,  3.5350e-03,\n",
       "                       6.0966e-04, -1.2151e-02,  9.4887e-03, -3.5813e-03, -1.2714e-02,\n",
       "                      -6.2153e-03,  5.3165e-04, -5.9954e-03, -6.1385e-03,  3.1338e-03,\n",
       "                       4.3252e-03,  4.4835e-03,  4.5967e-03,  4.6114e-03, -7.4222e-03,\n",
       "                      -8.3858e-03, -4.4001e-03,  1.8684e-03, -5.4057e-03,  2.8061e-03,\n",
       "                       4.6258e-03, -6.7209e-03, -1.0152e-02,  2.8504e-03,  2.9119e-03,\n",
       "                       2.7621e-02, -7.0954e-04, -1.0383e-02, -5.9674e-03, -1.6554e-03,\n",
       "                       2.6146e-03, -1.1885e-02, -1.9005e-03,  9.7558e-03,  1.4189e-02,\n",
       "                      -1.1359e-02, -5.0127e-04])),\n",
       "             ('bert.encoder.layer.3.attention.self.query.weight',\n",
       "              tensor([[ 0.0110,  0.0406,  0.0098,  ...,  0.0119, -0.0229,  0.0372],\n",
       "                      [ 0.0235, -0.0536, -0.0077,  ..., -0.0757,  0.0138, -0.0166],\n",
       "                      [-0.0259,  0.0031,  0.0402,  ...,  0.0053,  0.0150, -0.0190],\n",
       "                      ...,\n",
       "                      [-0.0049,  0.0500,  0.0032,  ...,  0.0011,  0.0102, -0.0074],\n",
       "                      [ 0.0437, -0.0411, -0.0487,  ..., -0.0246,  0.0724,  0.0092],\n",
       "                      [-0.0577,  0.0056,  0.0158,  ..., -0.0189,  0.0365, -0.0312]])),\n",
       "             ('bert.encoder.layer.3.attention.self.query.bias',\n",
       "              tensor([ 3.2901e-02, -4.9334e-02,  2.5452e-02,  1.6293e-02,  4.4585e-02,\n",
       "                       1.1363e-02,  3.0512e-02, -4.0213e-02, -1.6464e-03,  7.8905e-04,\n",
       "                       4.8068e-02, -2.6590e-02,  3.6246e-02, -2.5093e-02,  4.6015e-02,\n",
       "                       1.7574e-02, -2.8924e-02, -1.9503e-02,  1.1198e-02,  3.2882e-02,\n",
       "                      -3.8286e-02, -3.8549e-03, -5.8971e-03, -2.5265e-03,  5.8645e-02,\n",
       "                       8.0232e-03, -1.9114e-02,  3.3959e-03, -3.0322e-03,  3.5608e-03,\n",
       "                       1.7200e-05,  1.4635e-02,  5.8760e-02, -6.0225e-02, -4.7268e-02,\n",
       "                      -3.1246e-02,  3.2353e-02, -1.7711e-02,  5.7022e-02, -3.3240e-02,\n",
       "                       4.6604e-03,  3.7349e-02,  3.8205e-02,  2.4539e-02, -4.9396e-02,\n",
       "                      -7.6584e-03, -6.6734e-02, -6.4709e-02, -6.9279e-02,  7.7576e-02,\n",
       "                      -4.2825e-02, -8.8381e-03,  2.2474e-02, -6.6566e-02, -5.8422e-02,\n",
       "                       4.2459e-02,  7.0508e-02,  3.4418e-03,  5.5254e-02,  7.1903e-02,\n",
       "                      -4.7509e-02,  2.4753e-02,  3.0810e-02,  5.5326e-02, -6.0268e-02,\n",
       "                      -1.3401e-02, -2.2086e-02,  4.9004e-02,  2.1529e-03,  4.4209e-03,\n",
       "                       4.0599e-02, -1.8622e-02,  3.6232e-02,  2.9126e-03,  4.8000e-02,\n",
       "                      -2.4404e-02, -3.1610e-03,  5.4400e-02, -1.9127e-02, -3.4440e-02,\n",
       "                       5.0261e-02, -2.0059e-03,  1.6901e-02, -4.5130e-02, -3.9578e-02,\n",
       "                       2.1306e-02,  1.7576e-02, -4.3185e-02,  3.7396e-02,  4.1597e-02,\n",
       "                      -5.2699e-02, -3.2559e-02, -4.1314e-02,  5.7899e-02, -1.6816e-02,\n",
       "                       2.1415e-02, -3.0000e-02,  4.8942e-02, -2.7042e-02,  3.9021e-02,\n",
       "                       4.0421e-02,  3.5248e-02, -2.9782e-03,  8.2449e-03, -8.6755e-05,\n",
       "                       2.1400e-02,  2.9645e-02,  1.2691e-02,  3.1908e-02, -8.5846e-03,\n",
       "                      -3.7249e-02, -9.6624e-03, -2.2115e-02,  1.3330e-02,  3.8750e-02,\n",
       "                      -5.5364e-02, -6.7697e-03, -4.2183e-02,  1.4613e-02,  3.1915e-02,\n",
       "                      -2.4727e-02, -3.7478e-02, -2.2740e-02, -1.0781e-02, -3.4125e-02,\n",
       "                       2.2843e-02,  1.5559e-03, -5.0710e-03, -6.0826e-03, -1.3012e-02,\n",
       "                      -4.4828e-03, -1.2566e-02, -2.6976e-03, -4.3738e-03,  1.8742e-02,\n",
       "                       2.2853e-02, -1.4404e-02, -8.9888e-05, -2.9523e-02,  2.3092e-02,\n",
       "                       4.8299e-03,  1.7605e-02,  2.9965e-02, -1.5475e-03,  2.2210e-02,\n",
       "                      -1.3031e-03, -1.4312e-02,  4.6779e-03, -2.7967e-02,  1.0320e-02,\n",
       "                      -4.3592e-03, -1.6641e-02,  9.1669e-03,  1.7780e-03,  1.1202e-02,\n",
       "                      -8.4453e-03,  3.5242e-03,  8.9441e-03, -1.1205e-02, -5.4037e-03,\n",
       "                       2.0616e-02, -9.3902e-03,  3.5607e-02,  1.4446e-02,  1.2137e-02,\n",
       "                       1.3063e-02, -2.9421e-02, -1.0368e-02, -2.4799e-02, -1.5067e-02,\n",
       "                       2.7057e-02,  2.6636e-02,  1.4994e-02, -4.0596e-03, -3.4710e-02,\n",
       "                      -3.3708e-02,  2.1217e-02, -2.8080e-02,  2.0037e-02,  2.6049e-02,\n",
       "                       1.0594e-02, -2.6400e-02, -1.9938e-02,  4.7311e-03, -3.0988e-02,\n",
       "                      -2.3656e-02, -2.9184e-02,  2.3544e-02,  3.5406e-02,  1.1100e-02,\n",
       "                      -2.8895e-02, -1.3023e-02])),\n",
       "             ('bert.encoder.layer.3.attention.self.key.weight',\n",
       "              tensor([[ 0.0177, -0.0230, -0.0351,  ..., -0.0391,  0.0333,  0.0256],\n",
       "                      [-0.0288,  0.0536, -0.0041,  ...,  0.0195, -0.0117, -0.0002],\n",
       "                      [-0.0445, -0.0389,  0.0101,  ..., -0.0268, -0.0007, -0.0383],\n",
       "                      ...,\n",
       "                      [-0.0040,  0.0338, -0.0065,  ...,  0.0180,  0.0051,  0.0354],\n",
       "                      [-0.0055,  0.0280, -0.0449,  ..., -0.0089,  0.0156,  0.0059],\n",
       "                      [ 0.0394,  0.0219,  0.0027,  ...,  0.0138,  0.0064, -0.0181]])),\n",
       "             ('bert.encoder.layer.3.attention.self.key.bias',\n",
       "              tensor([ 5.3563e-10, -1.6085e-07,  1.2939e-07,  4.3942e-08,  5.0748e-08,\n",
       "                       1.6224e-08, -6.6816e-08, -2.2554e-08,  1.4262e-09,  8.7609e-08,\n",
       "                       7.4574e-08,  2.7148e-08, -2.3052e-08, -8.7300e-10, -5.3753e-08,\n",
       "                      -1.1004e-07, -1.7770e-08,  1.0519e-08,  7.8285e-08, -2.4086e-08,\n",
       "                      -1.2275e-07, -2.1477e-07,  6.8743e-08,  2.3552e-08, -1.0911e-07,\n",
       "                       6.7188e-08,  5.5589e-08, -2.2339e-08,  2.0400e-08,  9.6739e-08,\n",
       "                      -9.1353e-08, -4.5930e-08,  5.6597e-08, -3.5141e-07,  1.5809e-08,\n",
       "                      -1.8578e-07,  6.0804e-08, -1.5012e-07,  9.1533e-08,  8.3632e-08,\n",
       "                       1.7167e-07, -1.0962e-07,  5.1810e-07,  4.3196e-08, -9.6417e-09,\n",
       "                       7.1597e-08, -1.4568e-07, -2.7695e-08, -2.4699e-07,  5.2898e-07,\n",
       "                      -5.5736e-08, -2.2391e-08, -3.0631e-08,  4.6873e-08, -3.1657e-07,\n",
       "                       2.9216e-07,  5.2325e-08,  8.5813e-09,  5.9471e-07,  1.5512e-07,\n",
       "                      -2.2444e-07,  5.5688e-08,  1.2846e-07, -5.2294e-08,  4.4439e-07,\n",
       "                       1.3684e-07,  3.2503e-07, -7.1299e-08,  2.0151e-07,  1.9959e-07,\n",
       "                      -1.8904e-07,  2.7277e-07, -5.6985e-07,  1.2478e-07, -1.7752e-07,\n",
       "                       1.2408e-07, -1.5399e-07, -5.1650e-07,  2.3308e-07,  6.4127e-08,\n",
       "                      -2.4029e-07, -2.3148e-07, -3.6219e-07,  4.1882e-07,  1.6120e-07,\n",
       "                      -4.1035e-07, -1.7950e-07,  4.4446e-07, -5.0789e-07, -3.4104e-07,\n",
       "                       1.8680e-07,  2.7647e-07,  1.9832e-07, -1.4439e-07, -1.2890e-07,\n",
       "                      -1.7469e-07,  2.1942e-07, -5.5538e-08, -4.8875e-08, -1.2444e-08,\n",
       "                       9.9798e-08,  1.6766e-07, -6.9852e-08, -1.5045e-07,  1.4068e-07,\n",
       "                       6.1129e-08, -7.4726e-08,  1.8481e-07, -1.3153e-07,  1.1632e-07,\n",
       "                      -9.1448e-09,  1.0375e-07, -6.4898e-08, -1.3408e-07, -4.4638e-08,\n",
       "                       1.9798e-07,  1.7086e-07,  4.3683e-10,  4.4537e-08,  7.3824e-08,\n",
       "                      -2.6307e-08, -1.7064e-08, -2.4165e-07,  4.0079e-08, -7.2160e-08,\n",
       "                      -6.9494e-08,  2.4145e-07, -3.8945e-08, -5.1898e-08, -7.6980e-08,\n",
       "                      -7.7216e-08, -3.0658e-07, -1.2276e-07, -1.3014e-07, -8.9628e-08,\n",
       "                      -3.6159e-09, -5.3750e-08,  8.8832e-08, -1.1036e-07,  9.0500e-08,\n",
       "                      -1.3540e-09, -4.3703e-08,  1.6751e-07, -4.2738e-08,  2.3774e-07,\n",
       "                       1.0146e-07, -2.4956e-07,  3.4273e-08, -2.4180e-07, -8.3279e-08,\n",
       "                      -2.5668e-08, -8.7027e-08, -1.2627e-07, -2.3913e-08, -4.5487e-08,\n",
       "                      -4.1831e-08,  1.1381e-07, -1.6146e-08,  1.0137e-07,  1.5459e-09,\n",
       "                      -1.6850e-07,  3.8429e-08,  1.8465e-07,  2.6109e-07,  3.3813e-07,\n",
       "                       2.0579e-07, -3.5166e-07,  1.7126e-08, -4.5112e-09, -2.4239e-07,\n",
       "                       3.3021e-07,  3.1580e-07,  2.3082e-07,  1.6754e-07, -2.9404e-07,\n",
       "                      -1.6188e-07,  5.7684e-08, -2.5888e-07,  4.8006e-07,  3.2660e-07,\n",
       "                       4.1979e-07, -3.1477e-07, -2.1494e-07,  3.0562e-08, -2.1136e-07,\n",
       "                       7.4604e-08, -3.1284e-07,  7.0958e-08,  4.7593e-07, -1.5302e-07,\n",
       "                      -5.4376e-07, -3.4538e-07])),\n",
       "             ('bert.encoder.layer.3.attention.self.value.weight',\n",
       "              tensor([[-0.0299, -0.0214, -0.0276,  ...,  0.0228, -0.0211,  0.0197],\n",
       "                      [ 0.0173,  0.0047, -0.0018,  ...,  0.0173,  0.0133,  0.0199],\n",
       "                      [-0.0005,  0.0094, -0.0489,  ...,  0.0184,  0.0459,  0.0060],\n",
       "                      ...,\n",
       "                      [-0.0006, -0.0186, -0.0038,  ...,  0.0357, -0.0158, -0.0052],\n",
       "                      [-0.0103,  0.0440,  0.0129,  ...,  0.0076, -0.0146,  0.0095],\n",
       "                      [-0.0221, -0.0151,  0.0156,  ...,  0.0074,  0.0277,  0.0396]])),\n",
       "             ('bert.encoder.layer.3.attention.self.value.bias',\n",
       "              tensor([ 1.0894e-02, -6.2145e-04,  4.2166e-03,  4.2378e-03, -5.9663e-03,\n",
       "                      -7.8271e-03,  1.1173e-02, -2.6743e-03, -3.5950e-03,  7.0469e-04,\n",
       "                      -9.5947e-05,  1.1416e-02,  2.6199e-03, -1.3213e-02,  1.8703e-02,\n",
       "                      -7.8746e-03,  9.8747e-03,  7.2780e-03,  9.1517e-03, -2.1390e-03,\n",
       "                       6.8756e-03,  6.1301e-03, -7.9001e-03,  2.3845e-03, -5.0652e-03,\n",
       "                      -1.7227e-03, -5.6862e-03,  3.6285e-03, -8.0403e-03, -6.6759e-03,\n",
       "                       1.9593e-02,  1.1217e-02,  1.0021e-03,  2.8888e-03, -1.2646e-03,\n",
       "                       4.9359e-03, -5.4225e-04, -6.4773e-03, -6.3523e-03,  1.0257e-03,\n",
       "                       4.1695e-03, -5.3843e-03, -1.0175e-02, -6.4957e-03,  1.8747e-03,\n",
       "                      -2.0485e-04, -7.2867e-03, -1.3397e-03,  8.0873e-03,  1.7935e-03,\n",
       "                      -9.5811e-03,  8.2366e-03, -8.0056e-03, -1.1170e-02, -1.1883e-02,\n",
       "                       3.1263e-03, -2.0730e-03, -7.0817e-03, -2.5321e-03,  4.4335e-03,\n",
       "                       3.8507e-03,  1.5111e-02, -1.7800e-03,  7.7976e-03, -1.0789e-02,\n",
       "                      -2.9693e-03, -1.0564e-02, -3.2817e-03, -2.9575e-03,  1.7290e-02,\n",
       "                       6.4951e-03,  6.8433e-03, -5.6951e-03, -1.3433e-03, -3.0443e-03,\n",
       "                      -1.6045e-02,  5.9730e-03, -4.3585e-03,  5.2116e-03,  3.6606e-03,\n",
       "                      -1.9097e-03,  2.0892e-03,  9.8158e-03,  6.4093e-03,  4.8687e-03,\n",
       "                      -1.2207e-02, -4.3839e-03,  1.4009e-02,  6.1161e-03,  2.3729e-03,\n",
       "                      -3.8167e-03, -7.6904e-03, -3.8983e-03,  3.4703e-03, -3.3389e-03,\n",
       "                       6.8303e-03,  7.3844e-04, -5.8052e-03, -7.8351e-03,  1.5574e-03,\n",
       "                       5.3700e-04,  7.4153e-03,  1.4272e-03,  2.8782e-03, -9.0815e-04,\n",
       "                       3.2956e-03, -2.0219e-03,  5.9701e-03,  1.0758e-03,  1.4271e-03,\n",
       "                      -5.5153e-03,  2.3545e-03,  4.5626e-03, -7.2818e-04,  2.7517e-03,\n",
       "                      -1.6697e-02,  5.8910e-03,  4.4620e-03,  6.1108e-03,  2.8632e-03,\n",
       "                      -1.1115e-03, -1.0985e-02,  9.3088e-04,  1.9707e-03, -1.6024e-02,\n",
       "                      -2.3296e-03, -7.9012e-03,  2.8248e-03, -5.4187e-03, -6.2757e-03,\n",
       "                      -1.1676e-02,  7.0333e-03,  1.5120e-02,  3.4615e-03,  9.9775e-03,\n",
       "                      -8.8135e-03, -1.3032e-02, -1.1849e-02,  3.0375e-03, -4.5224e-03,\n",
       "                       5.1975e-03, -9.7422e-03,  5.4366e-04,  1.1508e-02,  1.5197e-03,\n",
       "                      -1.4065e-02, -5.1282e-04, -2.7967e-03, -3.2267e-03,  8.3499e-03,\n",
       "                      -4.4898e-03,  4.3462e-03,  1.0045e-02, -6.7469e-03,  1.9518e-03,\n",
       "                       9.5866e-03, -3.5262e-03,  1.7222e-02, -1.8897e-04, -9.4702e-03,\n",
       "                       1.3031e-02,  1.9377e-03, -7.0928e-03,  1.8955e-03,  4.5047e-03,\n",
       "                      -4.2103e-03,  4.5606e-03,  1.1067e-02, -1.6309e-02, -6.6555e-03,\n",
       "                      -4.3718e-03,  1.5302e-03,  2.1065e-03, -4.1065e-03,  9.1776e-03,\n",
       "                       5.0702e-03, -2.5795e-04, -2.3721e-03, -4.7142e-03,  8.0128e-04,\n",
       "                       1.1872e-02,  1.0499e-02,  6.7437e-03,  1.1501e-02,  8.3773e-03,\n",
       "                       1.0160e-02, -3.1037e-03,  4.0496e-03, -1.1967e-02, -8.8712e-03,\n",
       "                      -9.0065e-03,  1.4386e-02])),\n",
       "             ('bert.encoder.layer.3.attention.output.dense.weight',\n",
       "              tensor([[-0.0083,  0.0020,  0.0134,  ...,  0.0245,  0.0061, -0.0498],\n",
       "                      [ 0.0334,  0.0214, -0.0330,  ..., -0.0192, -0.0377,  0.0388],\n",
       "                      [-0.0091, -0.0209, -0.0170,  ...,  0.0423, -0.0406,  0.0350],\n",
       "                      ...,\n",
       "                      [-0.0174, -0.0275, -0.0237,  ..., -0.0279,  0.0112,  0.0071],\n",
       "                      [ 0.0164, -0.0749, -0.0021,  ...,  0.0030,  0.0269, -0.0278],\n",
       "                      [ 0.0084, -0.0249, -0.0358,  ...,  0.0180, -0.0032,  0.0223]])),\n",
       "             ('bert.encoder.layer.3.attention.output.dense.bias',\n",
       "              tensor([ 2.3225e-04,  1.6640e-03,  3.1334e-03,  2.4139e-03, -7.3706e-03,\n",
       "                      -5.9715e-03,  2.4601e-03, -7.9567e-03,  1.1938e-03, -3.5097e-03,\n",
       "                      -1.3968e-03,  1.0586e-02, -5.5155e-03, -2.8794e-03,  1.3879e-03,\n",
       "                      -1.7956e-03,  3.9840e-03,  1.0319e-03,  3.3521e-04, -2.5925e-03,\n",
       "                       7.9735e-03, -2.8958e-03, -5.5829e-03,  5.4149e-03, -3.2634e-03,\n",
       "                      -4.4541e-03,  1.2114e-04,  1.6795e-02, -8.7339e-03,  3.1209e-03,\n",
       "                       4.1399e-03,  5.0874e-04, -1.0354e-03, -6.9124e-03, -2.4031e-03,\n",
       "                       1.3773e-03, -1.2388e-03, -7.2909e-03,  9.5041e-04,  1.0271e-03,\n",
       "                      -7.7741e-03, -5.7827e-04, -3.8917e-03,  2.5098e-03, -3.8213e-03,\n",
       "                      -2.4459e-03,  6.2959e-03,  1.6324e-03,  5.7860e-05, -7.3182e-04,\n",
       "                       1.9719e-03, -9.3367e-03, -1.1124e-02,  3.6541e-03,  2.6230e-03,\n",
       "                       1.4525e-03, -8.3776e-03, -5.0954e-03, -1.6043e-03, -3.9970e-03,\n",
       "                      -5.5812e-04, -2.4202e-03,  7.3062e-03, -7.2186e-03, -8.4411e-03,\n",
       "                      -1.0715e-03, -8.1668e-03,  1.2317e-02,  4.1704e-04,  1.2013e-03,\n",
       "                      -1.0869e-04, -3.0201e-04, -2.6445e-03, -1.2294e-03, -6.7031e-03,\n",
       "                       3.8226e-03, -1.6065e-03,  3.2350e-03, -3.2463e-03, -5.9908e-03,\n",
       "                      -3.5177e-03,  1.6705e-03, -2.3968e-03,  2.8569e-03, -4.6850e-03,\n",
       "                      -5.8688e-03,  1.3387e-03, -4.9099e-03, -1.1991e-03, -2.5619e-03,\n",
       "                       9.1621e-04,  8.4014e-03,  3.5667e-03,  2.6699e-03, -1.2043e-02,\n",
       "                       2.2087e-02,  1.9846e-03, -1.6198e-03,  3.3048e-03, -1.7843e-03,\n",
       "                       3.7579e-04,  3.1591e-03, -1.2129e-02,  2.3334e-03,  2.9726e-04,\n",
       "                       6.0750e-03, -4.2576e-03, -8.5552e-05,  2.2039e-03, -4.8627e-04,\n",
       "                       3.2417e-04, -1.4839e-03,  2.1051e-03,  1.5584e-03, -1.5658e-02,\n",
       "                       3.8740e-03,  5.6011e-03, -1.7737e-03,  9.0968e-03, -6.9550e-03,\n",
       "                      -7.6870e-03,  2.8955e-03,  5.8981e-03, -9.3170e-04,  3.6248e-04,\n",
       "                       1.8341e-04,  1.3129e-05,  1.5959e-03,  6.6662e-03, -9.0194e-04,\n",
       "                       9.0224e-03,  5.0490e-03,  2.8115e-03, -1.2074e-03, -3.3564e-03,\n",
       "                      -2.9144e-05, -8.2692e-03,  1.4143e-03,  1.7701e-03,  3.3878e-03,\n",
       "                       5.2963e-04,  4.4364e-03,  5.4755e-03, -1.2158e-03, -2.7154e-03,\n",
       "                      -2.9468e-03,  2.0425e-05, -3.2144e-03, -2.2862e-03, -8.0371e-03,\n",
       "                      -4.1524e-03,  8.6665e-03, -4.8287e-03, -1.2343e-03,  1.0928e-04,\n",
       "                      -2.3734e-03, -1.1883e-02,  3.7286e-03, -1.4754e-03, -5.1617e-03,\n",
       "                      -3.8247e-03,  1.7207e-03, -6.6069e-03, -6.9270e-03,  1.2132e-03,\n",
       "                       3.2418e-03,  5.6375e-03,  1.9679e-03,  1.0039e-03, -2.0273e-03,\n",
       "                      -1.3317e-03,  1.5760e-03,  4.1896e-03, -1.5743e-03,  2.4038e-03,\n",
       "                       2.3954e-03, -5.4689e-03, -7.3114e-03, -5.7708e-04, -1.0641e-03,\n",
       "                       1.4148e-02, -4.9430e-03, -4.4194e-03,  7.1228e-04,  3.9515e-03,\n",
       "                      -2.3017e-03, -4.9791e-04, -5.2738e-03,  5.6459e-03,  8.1240e-03,\n",
       "                      -7.9021e-03,  1.0018e-03])),\n",
       "             ('bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0158, 1.0182, 0.9961, 0.9993, 1.0103, 0.9861, 1.0075, 1.0049, 0.9894,\n",
       "                      1.0201, 0.9799, 1.0037, 1.0091, 1.0092, 0.9926, 1.0046, 1.0153, 0.9784,\n",
       "                      0.9902, 1.0211, 1.0070, 1.0187, 0.9960, 1.0121, 1.0016, 1.0139, 1.0086,\n",
       "                      0.9815, 0.9857, 1.0212, 0.9996, 1.0150, 0.9971, 1.0100, 1.0143, 0.9949,\n",
       "                      1.0029, 0.9667, 1.0123, 1.0146, 1.0095, 1.0097, 1.0076, 1.0000, 1.0124,\n",
       "                      1.0232, 1.0080, 1.0209, 1.0020, 1.0133, 1.0117, 0.9632, 0.9634, 1.0135,\n",
       "                      1.0180, 1.0082, 1.0072, 1.0147, 1.0165, 1.0062, 1.0030, 0.9871, 1.0247,\n",
       "                      1.0060, 1.0149, 1.0058, 1.0086, 0.9472, 1.0026, 1.0043, 1.0039, 0.9797,\n",
       "                      1.0040, 1.0122, 1.0172, 0.9942, 1.0152, 1.0118, 1.0217, 1.0199, 1.0073,\n",
       "                      1.0058, 1.0184, 1.0180, 1.0343, 0.9999, 1.0229, 1.0069, 1.0117, 1.0017,\n",
       "                      0.9962, 0.9944, 1.0187, 1.0073, 0.9937, 0.9667, 1.0054, 0.9729, 1.0077,\n",
       "                      1.0131, 0.9973, 1.0199, 1.0151, 1.0070, 1.0171, 1.0025, 1.0119, 1.0004,\n",
       "                      1.0231, 1.0262, 0.9895, 1.0150, 1.0175, 1.0032, 0.9444, 1.0080, 1.0064,\n",
       "                      1.0110, 0.9997, 1.0153, 0.9967, 0.9724, 1.0037, 1.0059, 1.0209, 1.0216,\n",
       "                      1.0107, 1.0082, 1.0133, 1.0064, 0.9789, 0.9920, 1.0141, 1.0006, 1.0187,\n",
       "                      1.0134, 0.9984, 1.0145, 0.9802, 1.0123, 1.0143, 1.0114, 1.0005, 1.0236,\n",
       "                      0.9886, 1.0082, 0.9903, 1.0118, 1.0070, 1.0200, 0.9980, 0.9635, 1.0129,\n",
       "                      1.0248, 0.9969, 1.0309, 1.0081, 0.9972, 1.0090, 0.9664, 1.0006, 0.9902,\n",
       "                      1.0050, 1.0121, 1.0060, 1.0165, 1.0045, 1.0152, 1.0136, 0.9887, 0.9961,\n",
       "                      0.9990, 1.0058, 0.9987, 1.0199, 1.0015, 1.0097, 0.9614, 1.0132, 1.0017,\n",
       "                      0.9544, 1.0132, 1.0020, 1.0189, 1.0142, 1.0018, 1.0158, 1.0129, 0.9971,\n",
       "                      0.9902, 1.0078, 1.0131])),\n",
       "             ('bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       "              tensor([ 3.3470e-03,  8.7432e-03,  1.0357e-02, -1.1259e-03, -6.3320e-03,\n",
       "                      -8.8924e-03,  1.7827e-04, -8.0102e-03,  3.5679e-03, -6.9274e-03,\n",
       "                       5.7017e-03,  1.4418e-02, -3.1200e-03,  2.8536e-03,  5.2725e-03,\n",
       "                      -9.4625e-03,  4.2837e-03,  7.7597e-03, -3.1208e-03, -2.0194e-03,\n",
       "                       1.2450e-02, -7.5057e-06, -9.1730e-03,  6.1141e-03, -6.9424e-03,\n",
       "                      -2.1460e-03,  5.4096e-03,  2.2806e-02, -8.4453e-03,  9.3364e-03,\n",
       "                       6.5319e-03, -1.4394e-03, -4.2554e-04, -1.0034e-02, -6.4582e-03,\n",
       "                       2.4161e-03, -5.7385e-03, -1.2069e-02,  1.7761e-03, -8.2557e-03,\n",
       "                      -8.3389e-04, -4.4606e-03, -2.6024e-03,  8.7844e-03,  2.1962e-05,\n",
       "                      -9.1607e-03,  9.9010e-03, -2.9364e-03,  5.6465e-04, -1.7857e-04,\n",
       "                      -2.2713e-04, -1.1149e-02, -9.2818e-03,  5.8225e-03,  1.5590e-03,\n",
       "                      -1.9325e-03, -8.0934e-03, -3.6543e-04, -5.6856e-04,  3.5816e-03,\n",
       "                      -3.8600e-03,  1.5376e-04,  7.4528e-03, -1.4034e-02,  1.1445e-03,\n",
       "                      -2.6345e-03, -8.5873e-03,  1.6929e-02,  2.2118e-03,  7.6269e-03,\n",
       "                       7.4620e-03,  2.6694e-03,  3.2102e-03,  5.1122e-03, -5.7030e-04,\n",
       "                       9.4499e-03, -1.4957e-03,  5.0707e-03, -1.7439e-03, -2.8967e-03,\n",
       "                       1.7594e-03,  6.2551e-03, -6.1123e-03, -4.3928e-03, -1.9774e-03,\n",
       "                      -1.0520e-02, -2.1833e-03, -6.7301e-03,  1.5922e-03, -1.3986e-03,\n",
       "                      -4.0368e-03,  1.2982e-02,  2.5296e-03,  3.2481e-03, -1.5120e-02,\n",
       "                       3.2547e-02,  1.2172e-03,  3.5483e-03,  3.7016e-04, -7.1136e-04,\n",
       "                      -5.2757e-03,  1.0513e-02, -7.0928e-03,  1.1555e-03,  1.9801e-03,\n",
       "                       8.1164e-03, -2.7707e-03,  6.7227e-03, -3.2816e-03,  4.0597e-03,\n",
       "                      -5.6144e-03,  8.6728e-04,  2.3403e-03, -7.7858e-03, -2.6710e-02,\n",
       "                       1.2797e-02,  8.8954e-03, -6.8426e-03,  1.1115e-02, -1.0575e-02,\n",
       "                      -8.5705e-03, -1.9130e-03,  5.4601e-03,  1.2669e-05,  4.1821e-04,\n",
       "                       2.6337e-04,  9.9521e-03,  3.4343e-03,  1.3418e-02, -4.4120e-03,\n",
       "                       1.4033e-02,  5.1448e-03,  2.8545e-03, -2.8404e-03, -3.0944e-03,\n",
       "                       2.3252e-03, -1.2270e-02,  9.3694e-03,  2.4051e-03,  1.0339e-02,\n",
       "                       1.8828e-04,  4.1811e-04,  2.8392e-03, -4.8649e-04, -5.0101e-03,\n",
       "                      -7.0026e-03,  5.7968e-03, -3.4951e-04,  2.9604e-03, -8.8762e-03,\n",
       "                      -4.4873e-03,  1.1481e-02, -1.0237e-02, -8.3430e-03,  6.6666e-03,\n",
       "                      -2.8753e-03, -1.2181e-02,  1.0412e-02,  2.1484e-03, -1.1858e-02,\n",
       "                      -7.5833e-03,  9.0343e-04, -5.8357e-03, -8.1748e-03,  9.7656e-04,\n",
       "                       5.7373e-03,  3.8330e-03,  2.8913e-03,  5.5731e-03, -5.7278e-03,\n",
       "                      -7.6718e-03, -2.7873e-03,  2.1449e-03, -2.2408e-03,  6.1903e-03,\n",
       "                       8.3520e-03, -7.9247e-03, -8.5721e-03, -2.1895e-04, -3.1138e-04,\n",
       "                       2.9314e-02, -2.2271e-03, -1.0059e-02, -6.1015e-03,  1.2101e-03,\n",
       "                       2.2516e-03, -1.1667e-02, -5.3458e-03,  1.2787e-02,  1.5576e-02,\n",
       "                      -1.4153e-02, -4.5147e-04])),\n",
       "             ('bert.encoder.layer.3.intermediate.dense.weight',\n",
       "              tensor([[ 0.0307, -0.0154,  0.0187,  ..., -0.0167, -0.0135,  0.0392],\n",
       "                      [-0.0056, -0.0058, -0.0319,  ..., -0.0011,  0.0382, -0.0031],\n",
       "                      [ 0.0306, -0.0464, -0.0179,  ..., -0.0292, -0.0416,  0.0323],\n",
       "                      ...,\n",
       "                      [-0.0376, -0.0518,  0.0187,  ..., -0.0340, -0.0084, -0.0187],\n",
       "                      [-0.0232,  0.0157, -0.0162,  ...,  0.0198,  0.0201,  0.0093],\n",
       "                      [-0.0042, -0.0324, -0.0416,  ..., -0.0816,  0.0568,  0.0263]])),\n",
       "             ('bert.encoder.layer.3.intermediate.dense.bias',\n",
       "              tensor([-0.0153,  0.0074, -0.0078,  0.0043,  0.0068,  0.0045, -0.0173,  0.0339,\n",
       "                       0.0094, -0.0191,  0.0196, -0.0146, -0.0177,  0.0306, -0.0045,  0.0177,\n",
       "                       0.0148, -0.0063, -0.0018, -0.0027, -0.0118, -0.0056,  0.0010, -0.0123,\n",
       "                       0.0052, -0.0107,  0.0017,  0.0127, -0.0057, -0.0012,  0.0039, -0.0019,\n",
       "                      -0.0110,  0.0016, -0.0003,  0.0058, -0.0093, -0.0004, -0.0074,  0.0063,\n",
       "                      -0.0108,  0.0130, -0.0175,  0.0127,  0.0051, -0.0062, -0.0062,  0.0014,\n",
       "                       0.0158, -0.0100,  0.0297, -0.0086, -0.0109,  0.0149,  0.0068, -0.0143,\n",
       "                      -0.0119,  0.0039, -0.0070, -0.0041, -0.0030,  0.0108, -0.0072, -0.0154])),\n",
       "             ('bert.encoder.layer.3.output.dense.weight',\n",
       "              tensor([[ 0.0149,  0.0038, -0.0074,  ..., -0.0129,  0.0258,  0.0067],\n",
       "                      [ 0.0196, -0.0107, -0.0252,  ...,  0.0118, -0.0043, -0.0043],\n",
       "                      [ 0.0091,  0.0527,  0.0208,  ...,  0.0079,  0.0010,  0.0418],\n",
       "                      ...,\n",
       "                      [-0.0028,  0.0138,  0.0173,  ..., -0.0079,  0.0088, -0.0465],\n",
       "                      [-0.0199, -0.0304, -0.0198,  ...,  0.0002,  0.0116, -0.0176],\n",
       "                      [ 0.0292, -0.0139,  0.0401,  ..., -0.0213,  0.0081, -0.0018]])),\n",
       "             ('bert.encoder.layer.3.output.dense.bias',\n",
       "              tensor([ 4.9835e-03,  7.2881e-03,  1.1506e-02, -8.1892e-04, -3.0007e-03,\n",
       "                      -1.4193e-03, -3.0464e-03, -1.0402e-02,  1.8708e-03, -1.0652e-03,\n",
       "                       4.3170e-03,  1.1089e-02, -2.1248e-03,  1.5925e-03,  1.6889e-03,\n",
       "                      -8.3542e-03, -2.6220e-03,  7.4979e-03, -6.1322e-03, -6.5390e-04,\n",
       "                       8.7114e-03, -5.6345e-04, -8.2934e-03,  9.2931e-03, -7.8997e-03,\n",
       "                       2.7221e-03,  7.2520e-03,  1.9290e-02, -1.0111e-02,  9.1625e-03,\n",
       "                       2.7073e-03, -3.0945e-03, -1.5786e-03, -7.8880e-03, -4.4350e-03,\n",
       "                       2.6215e-03, -7.4414e-03, -1.0425e-02,  3.5801e-03, -1.3551e-03,\n",
       "                       9.3366e-04,  5.2878e-03, -6.4709e-04,  3.8809e-03, -1.1058e-03,\n",
       "                      -7.4556e-03,  6.0609e-03, -3.7188e-03,  3.5028e-03, -4.7727e-03,\n",
       "                       1.2835e-03, -6.6392e-03, -1.2775e-02,  5.0225e-03,  2.9744e-04,\n",
       "                       2.6966e-03, -8.8764e-03,  2.7157e-04,  2.3311e-03, -1.6778e-03,\n",
       "                      -5.3501e-03,  5.2682e-03,  4.7091e-03, -9.8623e-03, -2.8114e-03,\n",
       "                      -6.6773e-04, -7.3199e-03,  1.3216e-02,  2.9352e-03,  3.2262e-03,\n",
       "                       9.2002e-03,  1.9065e-03,  7.0112e-03,  5.2963e-03, -1.1336e-03,\n",
       "                       7.9584e-03,  4.1874e-03,  3.9376e-03, -4.1786e-04, -1.5705e-03,\n",
       "                       9.3057e-05,  5.8227e-03, -3.2152e-03, -1.8044e-03, -2.5202e-04,\n",
       "                      -7.4499e-03, -5.7266e-03, -6.9505e-03,  3.2510e-03, -1.2838e-03,\n",
       "                      -5.4810e-03,  1.0359e-02, -3.1674e-04,  4.4522e-03, -1.4520e-02,\n",
       "                       2.6164e-02, -1.1249e-03,  1.2193e-03,  2.7375e-04, -3.2129e-03,\n",
       "                      -4.5131e-03,  7.2007e-03, -1.1055e-02,  7.8032e-04,  4.8009e-03,\n",
       "                       7.7729e-03, -1.9186e-03,  5.9902e-03, -2.6298e-03, -4.2032e-04,\n",
       "                      -2.5156e-03,  3.2587e-03, -1.8702e-03, -6.1471e-03, -2.5121e-02,\n",
       "                       1.6023e-02,  7.1456e-03, -7.3909e-03,  9.8957e-03, -9.8778e-03,\n",
       "                      -9.3524e-03, -6.9536e-03,  2.5708e-03, -2.6505e-03,  1.9190e-04,\n",
       "                      -1.6159e-03,  7.3656e-03,  5.0923e-03,  1.0192e-02, -4.7743e-03,\n",
       "                       1.3128e-02,  8.1471e-03,  3.6782e-03, -5.8136e-03, -3.4954e-03,\n",
       "                       7.2451e-04, -8.5994e-03,  5.3072e-03, -2.4021e-03,  1.1104e-02,\n",
       "                      -3.1818e-03,  9.6279e-04,  4.4819e-03,  2.3189e-06, -5.7211e-03,\n",
       "                      -7.2772e-03,  3.3790e-03,  1.9424e-04,  1.4693e-03, -7.0149e-03,\n",
       "                      -5.8346e-03,  9.8964e-03, -8.8527e-03, -5.3234e-03,  3.3078e-03,\n",
       "                      -7.3518e-03, -1.5273e-02,  8.5192e-03,  5.6870e-03, -1.3904e-02,\n",
       "                      -6.1062e-03,  2.1017e-03, -2.7879e-03, -3.5482e-03,  3.3985e-04,\n",
       "                       7.0020e-03,  2.9059e-03,  4.1859e-03,  4.5113e-03, -6.0049e-03,\n",
       "                      -6.4948e-03, -3.8442e-03,  9.0363e-04, -1.1216e-03,  6.8734e-03,\n",
       "                       6.6488e-03, -1.0003e-02, -9.0455e-03,  2.5895e-04, -3.3234e-03,\n",
       "                       2.7787e-02, -6.1749e-03, -6.8369e-03, -5.4653e-03,  6.3525e-04,\n",
       "                       3.0142e-03, -6.9842e-03, -3.0236e-03,  1.0633e-02,  1.0034e-02,\n",
       "                      -1.4650e-02,  1.5861e-03])),\n",
       "             ('bert.encoder.layer.3.output.LayerNorm.weight',\n",
       "              tensor([1.0300, 1.0295, 1.0064, 1.0086, 1.0195, 0.9945, 1.0195, 1.0221, 1.0008,\n",
       "                      1.0257, 0.9865, 1.0181, 1.0222, 1.0210, 1.0103, 1.0112, 1.0233, 0.9936,\n",
       "                      0.9960, 1.0339, 1.0196, 1.0310, 1.0062, 1.0249, 1.0096, 1.0253, 1.0273,\n",
       "                      0.9883, 0.9858, 1.0415, 1.0171, 1.0284, 1.0097, 1.0261, 1.0273, 1.0001,\n",
       "                      1.0165, 0.9664, 1.0263, 1.0251, 1.0231, 1.0254, 1.0245, 1.0154, 1.0286,\n",
       "                      1.0306, 1.0161, 1.0369, 1.0137, 1.0279, 1.0279, 0.9621, 0.9562, 1.0276,\n",
       "                      1.0304, 1.0196, 1.0245, 1.0192, 1.0300, 1.0188, 1.0172, 0.9984, 1.0367,\n",
       "                      1.0112, 1.0266, 1.0213, 1.0204, 0.9371, 1.0168, 1.0105, 1.0156, 0.9935,\n",
       "                      1.0156, 1.0329, 1.0290, 1.0026, 1.0316, 1.0224, 1.0416, 1.0341, 1.0149,\n",
       "                      1.0201, 1.0309, 1.0332, 1.0490, 1.0129, 1.0420, 1.0184, 1.0260, 1.0193,\n",
       "                      1.0120, 0.9996, 1.0338, 1.0190, 1.0041, 0.9619, 1.0160, 0.9744, 1.0227,\n",
       "                      1.0209, 1.0120, 1.0274, 1.0286, 1.0203, 1.0305, 1.0113, 1.0301, 1.0124,\n",
       "                      1.0391, 1.0423, 0.9964, 1.0271, 1.0328, 1.0119, 0.9215, 1.0182, 1.0140,\n",
       "                      1.0181, 1.0120, 1.0349, 1.0013, 0.9778, 1.0138, 1.0166, 1.0337, 1.0348,\n",
       "                      1.0269, 1.0210, 1.0309, 1.0205, 0.9762, 1.0012, 1.0286, 1.0137, 1.0346,\n",
       "                      1.0285, 1.0003, 1.0273, 0.9904, 1.0264, 1.0274, 1.0251, 1.0115, 1.0352,\n",
       "                      0.9951, 1.0155, 0.9978, 1.0242, 1.0198, 1.0365, 1.0095, 0.9599, 1.0269,\n",
       "                      1.0370, 1.0090, 1.0423, 1.0228, 1.0025, 1.0193, 0.9671, 1.0153, 0.9982,\n",
       "                      1.0148, 1.0304, 1.0136, 1.0317, 1.0182, 1.0291, 1.0276, 1.0060, 1.0032,\n",
       "                      1.0184, 1.0229, 1.0171, 1.0316, 1.0139, 1.0253, 0.9647, 1.0237, 1.0160,\n",
       "                      0.9390, 1.0265, 1.0131, 1.0307, 1.0259, 1.0206, 1.0205, 1.0248, 1.0053,\n",
       "                      0.9941, 1.0107, 1.0288])),\n",
       "             ('bert.encoder.layer.3.output.LayerNorm.bias',\n",
       "              tensor([ 2.9746e-03,  7.3791e-03,  1.1108e-02,  1.4529e-03, -3.2950e-03,\n",
       "                      -1.0496e-02, -3.3863e-03, -1.1029e-02,  4.6644e-03, -6.0331e-03,\n",
       "                       1.5177e-02,  1.5403e-02, -8.0486e-03,  7.4304e-03,  9.8133e-03,\n",
       "                      -7.7987e-03,  2.1068e-03,  1.2265e-02, -2.3479e-03, -1.7198e-03,\n",
       "                       9.7381e-03, -1.6018e-03, -9.5928e-03,  4.2882e-03, -4.8762e-03,\n",
       "                      -2.9384e-03,  6.6671e-03,  2.1654e-02, -3.0699e-03,  1.1203e-02,\n",
       "                       1.2119e-02, -1.3057e-03, -4.7858e-03, -1.2220e-02, -7.2741e-03,\n",
       "                       2.2700e-04, -8.1267e-03, -8.2948e-03,  3.7343e-03, -8.4111e-03,\n",
       "                       4.0517e-05, -4.2076e-03, -3.7447e-03,  1.2042e-02,  1.0671e-04,\n",
       "                      -8.8603e-03,  5.6028e-03,  1.1451e-04, -1.1524e-03, -5.9040e-04,\n",
       "                      -3.9606e-03, -7.5602e-03, -6.5143e-03,  2.1868e-03,  2.5230e-03,\n",
       "                      -6.2274e-03, -6.9694e-03, -1.9519e-03, -4.0725e-04,  1.4635e-03,\n",
       "                      -4.9007e-03, -1.2469e-03,  1.1382e-02, -1.0825e-02,  2.9651e-04,\n",
       "                      -2.3631e-03, -8.2809e-03,  1.0664e-02,  1.1434e-03,  3.4934e-03,\n",
       "                       6.6678e-03, -2.3253e-04, -4.8640e-04,  7.6117e-03, -9.3183e-04,\n",
       "                       1.1206e-02, -3.8877e-05,  5.9638e-03, -1.3190e-03, -2.4464e-03,\n",
       "                      -1.0469e-03,  1.0951e-02, -8.1938e-03, -6.4173e-03,  6.4881e-04,\n",
       "                      -1.1564e-02, -6.0510e-03, -6.6571e-03,  7.9177e-03, -2.1038e-03,\n",
       "                      -1.2020e-02,  1.0135e-02,  4.0670e-03,  6.2569e-03, -1.5705e-02,\n",
       "                       2.6876e-02,  9.4951e-04, -3.0818e-03, -7.8688e-05, -5.1751e-04,\n",
       "                      -6.9991e-03,  1.2987e-02, -7.1814e-03,  6.3010e-04,  4.1110e-03,\n",
       "                       7.9189e-03, -4.9900e-03,  9.0434e-03, -6.1101e-03,  5.4959e-03,\n",
       "                      -5.0854e-03,  1.0893e-03,  3.7802e-03, -1.1434e-02, -1.5400e-02,\n",
       "                       1.3203e-02,  6.5431e-03, -6.1083e-03,  1.1322e-02, -1.4410e-02,\n",
       "                      -5.0945e-03, -1.0279e-02,  5.3799e-03, -4.5826e-04, -1.0223e-03,\n",
       "                       8.7181e-05,  1.2091e-02,  2.2675e-03,  1.6108e-02, -6.0815e-03,\n",
       "                       7.6429e-03,  2.9216e-03,  7.3896e-03, -6.1252e-03, -2.8453e-03,\n",
       "                       5.0567e-03, -5.8366e-03,  7.2488e-03,  5.1731e-03,  1.2387e-02,\n",
       "                      -2.0719e-03,  2.7756e-03,  3.9320e-03,  5.0703e-04, -5.1164e-03,\n",
       "                      -7.1026e-03,  6.4285e-03,  1.8047e-03,  3.3027e-03, -8.2864e-03,\n",
       "                      -2.5218e-03,  7.7459e-03, -7.3802e-03, -7.2467e-03,  2.3301e-04,\n",
       "                      -3.1382e-03, -1.4547e-02,  3.7487e-03,  4.9030e-03, -7.3913e-03,\n",
       "                      -7.3692e-03,  2.5840e-03, -3.2443e-03, -5.9890e-03, -1.2175e-03,\n",
       "                       8.9703e-03, -5.0600e-04,  6.1399e-03,  8.6350e-03, -6.8268e-03,\n",
       "                      -6.3163e-03, -1.0590e-02,  3.3124e-03, -5.2123e-03,  1.0556e-02,\n",
       "                       5.2167e-03, -9.1044e-03, -9.9181e-03,  5.3663e-04, -2.8727e-04,\n",
       "                       2.2947e-02, -2.3485e-03, -1.0765e-02, -7.4963e-03,  3.2601e-03,\n",
       "                       5.3947e-03, -1.2269e-02, -2.2665e-03,  1.0524e-02,  1.1736e-02,\n",
       "                      -1.1269e-02,  2.6143e-03])),\n",
       "             ('bert.encoder.layer.4.attention.self.query.weight',\n",
       "              tensor([[ 0.0516, -0.0856, -0.0353,  ..., -0.0191,  0.0354,  0.0214],\n",
       "                      [ 0.0513, -0.0100,  0.0182,  ..., -0.0321,  0.0151,  0.0232],\n",
       "                      [-0.0164,  0.0419,  0.0033,  ...,  0.0242, -0.0467, -0.0713],\n",
       "                      ...,\n",
       "                      [-0.0211,  0.0211, -0.0091,  ...,  0.0236, -0.0404,  0.0056],\n",
       "                      [-0.0100, -0.0056,  0.0275,  ...,  0.0509, -0.0209,  0.0017],\n",
       "                      [-0.0379, -0.0162,  0.0426,  ..., -0.0419,  0.0251, -0.0292]])),\n",
       "             ('bert.encoder.layer.4.attention.self.query.bias',\n",
       "              tensor([-0.0482, -0.0209,  0.0457,  0.0080,  0.0430, -0.0260,  0.0300, -0.0007,\n",
       "                       0.0275, -0.0486,  0.0308, -0.0462,  0.0426,  0.0168,  0.0081, -0.0252,\n",
       "                      -0.0222, -0.0313, -0.0410,  0.0173,  0.0289, -0.0508,  0.0359,  0.0352,\n",
       "                       0.0367,  0.0259, -0.0547,  0.0078,  0.0431,  0.0304, -0.0004,  0.0125,\n",
       "                       0.0028, -0.0031,  0.0142,  0.0090, -0.0327, -0.0292,  0.0057, -0.0078,\n",
       "                      -0.0021,  0.0215,  0.0311,  0.0147,  0.0231,  0.0027, -0.0225, -0.0183,\n",
       "                      -0.0204,  0.0026, -0.0010,  0.0178, -0.0113, -0.0141, -0.0228, -0.0145,\n",
       "                       0.0288, -0.0035, -0.0423, -0.0236,  0.0077, -0.0389,  0.0220,  0.0153,\n",
       "                       0.0459, -0.0324,  0.0230, -0.0338, -0.0479, -0.0203, -0.0211, -0.0355,\n",
       "                      -0.0084,  0.0275,  0.0022,  0.0098,  0.0290, -0.0247, -0.0162, -0.0124,\n",
       "                       0.0228, -0.0420, -0.0086, -0.0296, -0.0209,  0.0231, -0.0309, -0.0354,\n",
       "                      -0.0299,  0.0151,  0.0198, -0.0549,  0.0430,  0.0321, -0.0067,  0.0265,\n",
       "                      -0.0010,  0.0207, -0.0015, -0.0008,  0.0222, -0.0091,  0.0339,  0.0435,\n",
       "                       0.0144,  0.0056, -0.0347, -0.0137, -0.0274,  0.0065,  0.0039,  0.0176,\n",
       "                      -0.0009,  0.0052, -0.0248,  0.0272,  0.0089, -0.0354,  0.0157, -0.0304,\n",
       "                      -0.0152,  0.0057,  0.0179, -0.0035,  0.0117, -0.0463, -0.0365,  0.0239,\n",
       "                      -0.0192, -0.0055,  0.0174,  0.0126,  0.0192, -0.0213,  0.0421, -0.0325,\n",
       "                       0.0078, -0.0012, -0.0300,  0.0089, -0.0365,  0.0318, -0.0111, -0.0306,\n",
       "                      -0.0155,  0.0221, -0.0007, -0.0186,  0.0391,  0.0121,  0.0026, -0.0137,\n",
       "                       0.0043,  0.0171, -0.0110, -0.0038,  0.0145, -0.0087, -0.0293, -0.0088,\n",
       "                       0.0170,  0.0218,  0.0065, -0.0053,  0.0116, -0.0282,  0.0070,  0.0144,\n",
       "                      -0.0154,  0.0042,  0.0178, -0.0092, -0.0156,  0.0145, -0.0023, -0.0163,\n",
       "                       0.0126, -0.0192,  0.0095, -0.0064,  0.0142, -0.0213,  0.0144, -0.0058,\n",
       "                      -0.0004, -0.0354, -0.0055, -0.0110,  0.0038,  0.0188,  0.0127, -0.0102])),\n",
       "             ('bert.encoder.layer.4.attention.self.key.weight',\n",
       "              tensor([[ 0.0258,  0.0249,  0.0010,  ...,  0.0547, -0.0225, -0.0258],\n",
       "                      [ 0.0104,  0.0188, -0.0153,  ..., -0.0122, -0.0673, -0.0033],\n",
       "                      [-0.0275, -0.0438, -0.0383,  ..., -0.0277,  0.0060,  0.0168],\n",
       "                      ...,\n",
       "                      [-0.0208, -0.0373,  0.0068,  ...,  0.0032, -0.0013,  0.0151],\n",
       "                      [-0.0156,  0.0240,  0.0118,  ...,  0.0132,  0.0207,  0.0041],\n",
       "                      [-0.0367,  0.0259,  0.0215,  ..., -0.0246,  0.0201, -0.0305]])),\n",
       "             ('bert.encoder.layer.4.attention.self.key.bias',\n",
       "              tensor([-3.8319e-09,  2.3135e-08,  6.8063e-08,  4.1863e-07,  2.0705e-07,\n",
       "                      -1.2710e-07,  8.1770e-08, -2.4784e-07, -9.6610e-08,  4.8646e-08,\n",
       "                      -1.4163e-07,  3.4240e-07,  2.6940e-08,  1.9371e-07,  2.1843e-07,\n",
       "                      -2.4006e-07,  1.9845e-07,  2.4859e-07, -3.1104e-07,  1.9654e-07,\n",
       "                       3.1564e-07, -1.3672e-07,  2.6908e-07, -2.1939e-07, -1.3327e-07,\n",
       "                       3.0932e-07,  2.2541e-07, -1.3664e-07,  9.2220e-08, -2.8351e-08,\n",
       "                      -3.4307e-08,  2.9468e-07, -5.6605e-08,  2.0288e-07,  2.2589e-07,\n",
       "                       5.9769e-08,  2.5513e-07, -3.6532e-08,  5.2071e-08,  6.1330e-08,\n",
       "                      -4.5426e-08, -3.4027e-08,  1.9398e-07, -6.0256e-08,  4.6264e-08,\n",
       "                       6.2791e-08,  1.5190e-07,  2.6172e-07, -1.8466e-08,  1.2106e-08,\n",
       "                      -1.1329e-07,  7.6225e-09,  2.1932e-07, -4.2773e-08,  4.0610e-08,\n",
       "                       6.5120e-09,  1.1902e-07,  6.0222e-08,  7.3412e-08, -1.2319e-07,\n",
       "                      -1.0137e-07,  4.0407e-08, -1.1833e-07,  5.5755e-08,  1.7864e-07,\n",
       "                      -1.7276e-07,  8.6450e-08, -9.1138e-08, -1.6973e-07, -5.8491e-08,\n",
       "                      -2.2097e-07,  8.1304e-08, -6.3374e-08, -8.9361e-08, -1.7527e-08,\n",
       "                       1.1140e-07, -3.3453e-08, -1.0552e-07, -1.0078e-07, -1.2002e-07,\n",
       "                       2.4879e-07, -2.0988e-07, -4.2151e-08, -2.0493e-07, -1.4348e-07,\n",
       "                       1.0224e-07, -1.3951e-07, -2.2104e-07, -2.2035e-07,  4.6540e-08,\n",
       "                       7.7594e-08, -1.3686e-07,  4.6610e-08,  2.0057e-07, -6.2407e-08,\n",
       "                       1.6584e-07, -1.0166e-07, -1.5833e-07, -1.0582e-08,  9.3531e-08,\n",
       "                      -2.5384e-09,  8.8804e-08, -5.6639e-08,  9.8936e-08, -3.4796e-09,\n",
       "                       1.6851e-07,  1.5252e-07,  1.9696e-07,  1.5406e-07,  4.7870e-08,\n",
       "                      -2.3125e-07,  7.7551e-08,  3.2523e-08,  9.1206e-08,  4.1936e-08,\n",
       "                      -1.1825e-07,  8.2102e-08,  2.4883e-07, -3.7720e-08,  1.3475e-07,\n",
       "                       3.8382e-08,  1.9510e-07, -1.9857e-07, -5.3130e-08,  1.6434e-07,\n",
       "                       2.3550e-07,  1.8073e-07,  4.6566e-08, -2.9683e-07, -1.4862e-07,\n",
       "                       2.5310e-08,  2.5025e-07,  1.2317e-07,  9.2718e-08,  3.3274e-07,\n",
       "                      -3.0929e-07,  7.9568e-08, -1.2182e-07, -1.8828e-07,  9.5735e-08,\n",
       "                      -1.8610e-07,  1.2931e-07,  1.9491e-08, -5.5048e-08, -5.7522e-09,\n",
       "                       1.2242e-07, -5.6975e-08, -3.1341e-08, -9.4964e-08,  1.4936e-07,\n",
       "                       2.6126e-07, -1.2372e-07, -2.0860e-07,  3.6169e-07,  1.2048e-07,\n",
       "                       7.4160e-08,  3.3141e-08,  7.2013e-08, -7.6479e-08, -1.4402e-07,\n",
       "                       9.7715e-08, -3.2770e-08, -4.6257e-08,  3.0744e-08, -8.1828e-10,\n",
       "                       1.8766e-08, -1.3068e-07, -3.5326e-09, -1.1380e-07, -3.3063e-08,\n",
       "                      -1.3027e-08,  9.6960e-08,  2.2013e-08, -2.8704e-08, -2.4641e-08,\n",
       "                      -2.7919e-07,  1.1581e-07, -3.2697e-07,  9.4290e-08, -1.2098e-08,\n",
       "                       1.6775e-07, -1.3871e-07,  2.3011e-07,  5.2738e-09, -8.2020e-08,\n",
       "                       1.9679e-08,  1.2810e-07, -1.9614e-07,  4.7903e-08,  1.5258e-07,\n",
       "                       1.8271e-07, -8.2259e-08])),\n",
       "             ('bert.encoder.layer.4.attention.self.value.weight',\n",
       "              tensor([[ 0.0267,  0.0072,  0.0340,  ...,  0.0101,  0.0072, -0.0074],\n",
       "                      [ 0.0044, -0.0110,  0.0072,  ..., -0.0252, -0.0211,  0.0201],\n",
       "                      [-0.0226,  0.0173, -0.0131,  ...,  0.0422, -0.0495, -0.0050],\n",
       "                      ...,\n",
       "                      [ 0.0138,  0.0039,  0.0107,  ...,  0.0134, -0.0031, -0.0133],\n",
       "                      [ 0.0278, -0.0027,  0.0128,  ...,  0.0046, -0.0187,  0.0291],\n",
       "                      [-0.0391,  0.0213, -0.0201,  ..., -0.0115, -0.0099,  0.0261]])),\n",
       "             ('bert.encoder.layer.4.attention.self.value.bias',\n",
       "              tensor([ 2.5111e-03, -1.2069e-02, -4.4653e-03, -7.7863e-03,  5.4015e-03,\n",
       "                      -1.1996e-02,  5.2072e-03,  2.7205e-03,  3.8880e-03, -2.0801e-03,\n",
       "                       1.2838e-02, -5.3906e-03,  5.6546e-03,  1.0906e-02,  2.9736e-03,\n",
       "                       4.9142e-03,  3.8062e-03,  6.3312e-03, -8.5307e-03, -4.8200e-03,\n",
       "                      -2.8113e-03,  1.3374e-03,  3.2668e-03, -7.1516e-03,  6.2762e-03,\n",
       "                      -2.4234e-03, -3.1491e-03,  8.1209e-03, -5.1596e-03, -3.4173e-03,\n",
       "                       2.7779e-03, -7.8053e-03, -9.5629e-03, -9.0743e-03,  8.4998e-03,\n",
       "                      -2.0343e-03, -8.8367e-04, -2.9255e-03, -1.6827e-03, -4.9122e-03,\n",
       "                       8.9469e-04, -5.5805e-03, -1.8510e-03, -7.7870e-03,  5.0110e-04,\n",
       "                      -9.2432e-03,  3.8383e-03,  2.0737e-03,  2.2126e-03,  4.9521e-03,\n",
       "                      -3.9415e-03,  9.0113e-03, -8.7205e-06, -8.5799e-03, -6.7739e-04,\n",
       "                       2.6513e-03,  8.0082e-03, -1.2187e-03,  5.5814e-03, -7.8356e-03,\n",
       "                      -6.4836e-03,  1.0554e-02,  2.6599e-03,  3.5584e-03,  2.1380e-03,\n",
       "                      -1.0537e-02, -3.3319e-03,  3.1460e-03,  3.9320e-03, -8.7738e-03,\n",
       "                       5.1011e-03,  4.7810e-03, -4.4214e-04,  2.0805e-03, -7.1336e-03,\n",
       "                       3.5691e-03, -8.9899e-03, -4.5444e-03,  5.6281e-03,  8.3690e-03,\n",
       "                       6.7046e-03,  1.1873e-02, -4.4159e-03, -3.1938e-03,  2.1553e-03,\n",
       "                       1.7090e-03,  2.1409e-03,  9.6551e-03, -2.6168e-03, -1.6144e-03,\n",
       "                      -1.2751e-03,  5.0467e-03,  2.2559e-03, -5.1622e-03, -4.6109e-03,\n",
       "                       2.8435e-03, -2.1204e-03, -7.1449e-03, -8.2704e-03, -4.0764e-03,\n",
       "                       8.2325e-03,  5.3607e-03, -6.6233e-03, -4.9976e-04, -6.5172e-03,\n",
       "                       9.1650e-03,  1.7878e-02, -2.5213e-04,  4.2635e-03,  2.5794e-04,\n",
       "                       8.7373e-03, -8.3840e-03,  3.8913e-03,  1.1380e-02,  3.9836e-03,\n",
       "                      -1.7111e-02, -1.2745e-04,  5.6497e-03,  1.9542e-02, -1.8747e-02,\n",
       "                      -1.2201e-02, -1.8948e-02,  1.8524e-03, -5.4408e-03, -1.8047e-02,\n",
       "                      -8.4605e-03, -2.8827e-03, -3.8957e-03, -1.5677e-03,  6.6853e-03,\n",
       "                      -4.4062e-03,  2.8186e-04,  4.7917e-04,  6.1163e-03, -8.2013e-03,\n",
       "                       2.3202e-03,  3.9880e-03, -1.5462e-03, -4.2745e-03, -4.5689e-03,\n",
       "                       3.5596e-03, -1.7012e-04, -8.7395e-03, -3.3788e-03, -2.6078e-04,\n",
       "                      -7.0607e-03,  2.7669e-03, -1.2538e-02, -4.1388e-03, -1.9669e-03,\n",
       "                       5.8108e-03,  2.0291e-03,  4.5703e-03,  4.1038e-03, -7.0698e-03,\n",
       "                       2.6204e-03, -5.9953e-03, -4.3981e-03,  7.8528e-03, -2.6541e-03,\n",
       "                       7.5055e-03, -4.4710e-03, -5.6836e-03,  2.6511e-03, -9.2580e-03,\n",
       "                      -2.8632e-03, -2.5166e-05,  1.3735e-03,  1.9219e-03, -2.6453e-03,\n",
       "                      -7.0955e-04,  1.2119e-02, -3.5768e-03, -6.7374e-03, -1.8497e-03,\n",
       "                       1.6000e-03,  3.6290e-03, -5.0136e-04, -1.3208e-02, -1.2320e-02,\n",
       "                       1.0384e-02, -1.6384e-02, -1.3412e-02, -4.6399e-03, -8.0502e-03,\n",
       "                       7.4695e-04, -7.6880e-03,  9.2014e-03, -9.0373e-03,  1.4921e-03,\n",
       "                       1.0402e-02,  6.4326e-03])),\n",
       "             ('bert.encoder.layer.4.attention.output.dense.weight',\n",
       "              tensor([[ 0.0020,  0.0217,  0.0300,  ...,  0.0090, -0.0050,  0.0091],\n",
       "                      [ 0.0005, -0.0155,  0.0508,  ..., -0.0302,  0.0123, -0.0262],\n",
       "                      [ 0.0131, -0.0114,  0.0433,  ..., -0.0172,  0.0077,  0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0283, -0.0232, -0.0316,  ...,  0.0306,  0.0071, -0.0090],\n",
       "                      [ 0.0354,  0.0237, -0.0373,  ..., -0.0068, -0.0295, -0.0086],\n",
       "                      [ 0.0190, -0.0039,  0.0185,  ...,  0.0301,  0.0247,  0.0284]])),\n",
       "             ('bert.encoder.layer.4.attention.output.dense.bias',\n",
       "              tensor([ 3.6378e-03,  2.6837e-03,  1.7973e-03,  5.9392e-03, -3.1160e-03,\n",
       "                      -1.0664e-02, -8.0167e-03, -4.5140e-03,  5.0472e-03, -4.0974e-03,\n",
       "                       8.2360e-03,  9.6346e-03, -7.7835e-03, -4.9859e-03,  1.2915e-03,\n",
       "                      -3.1205e-03,  6.4817e-04,  5.5849e-03,  1.2630e-03, -7.7262e-04,\n",
       "                       1.7127e-03, -5.4821e-03, -8.0049e-03,  2.3579e-03,  1.1642e-03,\n",
       "                       6.6724e-04,  4.5290e-03,  1.6810e-02, -7.0667e-03,  1.0125e-02,\n",
       "                       1.4879e-03, -1.1055e-03, -4.3769e-03, -1.2231e-02, -9.0146e-03,\n",
       "                      -5.3262e-03, -8.8212e-03, -3.4995e-03,  2.8804e-03,  2.7674e-03,\n",
       "                       1.3470e-03, -3.4833e-03, -2.1464e-03,  4.2674e-03,  6.0540e-03,\n",
       "                      -6.3986e-03, -1.7937e-03,  4.7586e-03,  5.6139e-04, -6.0648e-03,\n",
       "                      -5.0468e-04, -1.7556e-03, -7.8805e-03, -2.2705e-03, -2.4770e-03,\n",
       "                      -3.9534e-03, -3.7368e-03,  4.0542e-03,  4.1726e-03, -3.2489e-04,\n",
       "                      -1.0211e-03, -4.0888e-03,  8.2724e-03,  2.2229e-03, -2.3517e-03,\n",
       "                       7.8832e-04, -7.0384e-03, -2.9526e-03, -2.1995e-03,  1.0316e-03,\n",
       "                       3.4577e-03, -2.8065e-03, -4.7378e-04,  7.3078e-03, -1.7072e-03,\n",
       "                       6.6695e-03,  1.6531e-04,  1.0442e-03,  2.4131e-03, -2.3093e-03,\n",
       "                      -5.0787e-03,  4.7779e-03, -3.9640e-03,  1.6016e-03, -2.1304e-03,\n",
       "                      -4.3253e-03, -2.8539e-03, -4.9838e-03, -1.1810e-03,  3.6043e-04,\n",
       "                      -6.8898e-03,  3.1290e-03,  1.8106e-03,  9.1480e-03, -1.2892e-02,\n",
       "                       1.3842e-02, -5.2034e-03, -1.7175e-03, -2.3784e-03,  1.4612e-03,\n",
       "                      -3.9930e-03,  3.0153e-03, -1.0852e-02,  6.8223e-03, -1.1580e-03,\n",
       "                       1.6352e-03, -8.8297e-04,  1.1057e-03,  1.2134e-03, -1.0433e-03,\n",
       "                       2.8625e-03, -1.1723e-03,  3.0580e-03, -6.1904e-03, -1.8895e-02,\n",
       "                       3.2759e-03,  3.1205e-03, -7.4682e-03,  4.3764e-03, -8.4375e-03,\n",
       "                       4.0213e-03, -3.7660e-03,  3.9786e-03, -6.8215e-03,  4.7744e-03,\n",
       "                       4.1619e-04,  2.6795e-03,  5.9839e-03,  1.2878e-02, -5.6808e-03,\n",
       "                       7.0770e-03,  4.0587e-03,  6.4022e-03, -1.0741e-02,  3.8110e-04,\n",
       "                       5.5087e-03, -3.7249e-03, -1.3351e-03,  5.2380e-03,  3.9559e-03,\n",
       "                       2.8837e-03,  2.3751e-03,  6.1087e-03,  9.5479e-04, -1.7221e-03,\n",
       "                      -3.7360e-03,  5.3290e-04, -6.2289e-03, -8.5067e-04, -3.6764e-03,\n",
       "                      -9.0155e-03,  4.2626e-03,  1.3415e-04, -5.6002e-03, -1.2737e-03,\n",
       "                      -3.5671e-03, -7.5051e-03,  2.4286e-03,  4.2277e-03, -2.3282e-03,\n",
       "                      -1.4520e-03,  8.4786e-04, -2.6348e-04, -6.6002e-03, -1.5171e-04,\n",
       "                       4.9011e-03,  1.8192e-03,  7.3328e-03,  2.4225e-03,  1.1197e-03,\n",
       "                      -2.5742e-04, -2.0181e-03, -3.3433e-06,  1.9661e-03,  5.1957e-03,\n",
       "                       8.3846e-04, -2.8141e-03, -1.7727e-03, -5.8544e-05, -8.1945e-03,\n",
       "                       1.5418e-02, -6.3809e-03, -3.5017e-03, -2.8189e-04,  5.7643e-05,\n",
       "                      -1.1720e-03, -2.8838e-03, -8.3185e-03,  3.2832e-03,  7.5873e-03,\n",
       "                      -3.0553e-03,  1.0303e-03])),\n",
       "             ('bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0235, 1.0158, 0.9953, 1.0029, 1.0110, 0.9817, 1.0140, 1.0061, 0.9879,\n",
       "                      1.0187, 0.9796, 1.0140, 1.0180, 1.0102, 1.0015, 1.0018, 1.0104, 0.9863,\n",
       "                      0.9886, 1.0231, 1.0118, 1.0268, 0.9912, 1.0127, 0.9984, 1.0155, 1.0273,\n",
       "                      0.9882, 0.9752, 1.0294, 1.0086, 1.0137, 1.0039, 1.0173, 1.0181, 0.9954,\n",
       "                      1.0056, 0.9589, 1.0195, 1.0156, 1.0118, 1.0160, 1.0074, 1.0061, 1.0204,\n",
       "                      1.0174, 1.0052, 1.0252, 0.9952, 1.0167, 1.0248, 0.9580, 0.9510, 1.0138,\n",
       "                      1.0154, 1.0080, 1.0122, 1.0147, 1.0223, 1.0107, 1.0044, 0.9893, 1.0269,\n",
       "                      1.0024, 1.0175, 1.0065, 1.0126, 0.9337, 1.0088, 1.0037, 1.0101, 0.9797,\n",
       "                      1.0091, 1.0293, 1.0154, 0.9922, 1.0164, 1.0058, 1.0385, 1.0208, 1.0084,\n",
       "                      1.0052, 1.0179, 1.0271, 1.0331, 1.0031, 1.0316, 1.0084, 1.0102, 1.0106,\n",
       "                      1.0071, 0.9851, 1.0150, 0.9974, 0.9952, 0.9566, 1.0078, 0.9693, 1.0121,\n",
       "                      1.0186, 1.0048, 1.0218, 1.0247, 0.9979, 1.0138, 1.0003, 1.0157, 0.9916,\n",
       "                      1.0243, 1.0293, 0.9872, 1.0218, 1.0209, 0.9950, 0.9281, 1.0159, 1.0030,\n",
       "                      1.0070, 1.0090, 1.0238, 0.9931, 0.9577, 1.0099, 1.0043, 1.0249, 1.0280,\n",
       "                      1.0230, 1.0089, 1.0201, 1.0106, 0.9744, 0.9697, 1.0127, 1.0041, 1.0330,\n",
       "                      1.0033, 0.9908, 1.0130, 0.9769, 1.0166, 1.0112, 1.0110, 0.9984, 1.0233,\n",
       "                      0.9896, 1.0006, 0.9850, 1.0056, 1.0078, 1.0284, 0.9997, 0.9558, 1.0167,\n",
       "                      1.0256, 0.9928, 1.0237, 1.0199, 0.9983, 1.0044, 0.9616, 1.0004, 0.9949,\n",
       "                      1.0088, 1.0140, 0.9926, 1.0147, 1.0056, 1.0135, 1.0114, 0.9956, 0.9999,\n",
       "                      1.0000, 1.0085, 1.0108, 1.0231, 1.0063, 1.0152, 0.9599, 1.0124, 1.0012,\n",
       "                      0.9415, 1.0160, 1.0058, 1.0196, 1.0068, 1.0164, 1.0047, 1.0117, 0.9890,\n",
       "                      0.9866, 1.0004, 1.0232])),\n",
       "             ('bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       "              tensor([ 4.6133e-03,  9.3362e-03,  1.0494e-02,  3.1787e-03, -2.5637e-03,\n",
       "                      -9.4950e-03, -7.7504e-03, -1.3313e-02,  4.4383e-03, -5.1568e-03,\n",
       "                       1.8642e-02,  2.0692e-02, -5.6464e-03,  7.8829e-03,  1.0311e-02,\n",
       "                      -1.2983e-02,  1.0118e-03,  1.4164e-02, -4.7914e-03,  2.0945e-04,\n",
       "                       1.2982e-02, -3.6837e-03, -1.3305e-02,  1.0173e-02, -8.4078e-03,\n",
       "                      -3.8433e-03,  8.5249e-03,  2.9794e-02, -5.4695e-03,  1.6505e-02,\n",
       "                       1.2757e-02, -2.5051e-03, -6.9065e-03, -1.4842e-02, -1.2392e-02,\n",
       "                      -4.5961e-03, -1.2864e-02, -1.2067e-02,  4.5609e-03, -5.5368e-03,\n",
       "                      -1.7999e-03, -7.7919e-03, -4.9427e-03,  1.3081e-02,  2.9191e-03,\n",
       "                      -1.1805e-02,  5.8072e-03, -9.8878e-04,  3.5581e-04, -5.3189e-03,\n",
       "                      -5.5943e-03, -8.5910e-03, -4.9060e-03,  4.6838e-03,  6.0358e-03,\n",
       "                      -6.6393e-03, -6.9913e-03, -9.5986e-04,  1.7586e-03,  4.2425e-03,\n",
       "                      -7.5064e-03, -3.9888e-03,  1.0288e-02, -1.4685e-02,  8.0236e-04,\n",
       "                      -1.8949e-03, -1.0233e-02,  1.3456e-02,  3.0356e-03,  6.8873e-03,\n",
       "                       9.9742e-03, -2.2092e-04,  1.5824e-03,  1.3446e-02, -2.1486e-03,\n",
       "                       1.4351e-02, -1.0930e-03,  2.3500e-03, -7.9622e-04, -1.5366e-03,\n",
       "                       2.0113e-03,  1.2505e-02, -5.3490e-03, -9.3478e-03, -3.0342e-05,\n",
       "                      -1.0131e-02, -4.9898e-03, -7.9646e-03,  1.0099e-02, -1.4939e-03,\n",
       "                      -1.2294e-02,  1.4883e-02,  3.6203e-03,  7.1398e-03, -1.6122e-02,\n",
       "                       3.3743e-02, -1.0047e-04, -1.2511e-03, -3.3187e-03,  2.0722e-03,\n",
       "                      -7.4956e-03,  1.4828e-02, -9.6522e-03,  4.0709e-03,  2.9999e-03,\n",
       "                       1.0414e-02, -8.4374e-03,  7.5867e-03, -9.2333e-03,  6.4617e-03,\n",
       "                      -3.9824e-03, -1.1015e-04,  3.9783e-03, -1.3026e-02, -2.7393e-02,\n",
       "                       1.7281e-02,  8.8441e-03, -9.3252e-03,  1.4154e-02, -1.4111e-02,\n",
       "                      -3.0677e-03, -6.9078e-03,  5.2624e-03, -5.0119e-03,  3.2045e-03,\n",
       "                       1.6492e-03,  1.8061e-02,  3.7507e-03,  2.1712e-02, -4.8211e-03,\n",
       "                       8.0707e-03,  2.7033e-03,  6.6219e-03, -8.1082e-03, -4.2588e-03,\n",
       "                       7.6024e-03, -7.9742e-03,  9.2300e-03,  5.9181e-03,  1.5133e-02,\n",
       "                      -4.5896e-03,  1.2440e-03,  3.2842e-03, -4.2526e-03, -8.3481e-03,\n",
       "                      -9.6930e-03,  4.2573e-03, -8.7411e-06,  3.3900e-03, -1.0632e-02,\n",
       "                      -2.9848e-03,  8.9748e-03, -1.0890e-02, -1.2637e-02,  4.6767e-04,\n",
       "                      -6.0910e-03, -1.6762e-02,  6.4355e-03,  7.9747e-03, -1.1516e-02,\n",
       "                      -9.3376e-03,  4.9917e-03, -5.5415e-03, -1.0514e-02,  1.5482e-03,\n",
       "                       8.3105e-03,  2.2466e-03,  6.9478e-03,  1.1161e-02, -7.0960e-03,\n",
       "                      -7.3751e-03, -8.4563e-03,  4.1216e-03, -5.6408e-03,  9.3079e-03,\n",
       "                       9.1383e-03, -1.3095e-02, -1.1341e-02,  1.3083e-03, -4.5373e-03,\n",
       "                       3.0981e-02, -4.5165e-03, -1.3022e-02, -8.4004e-03,  3.4503e-03,\n",
       "                       4.2809e-03, -1.3913e-02, -5.2080e-03,  9.2101e-03,  1.7230e-02,\n",
       "                      -1.3418e-02, -7.5086e-04])),\n",
       "             ('bert.encoder.layer.4.intermediate.dense.weight',\n",
       "              tensor([[ 6.6158e-03, -4.3227e-02, -2.2611e-02,  ..., -1.9781e-02,\n",
       "                        1.6530e-02, -2.1007e-02],\n",
       "                      [ 2.2926e-06,  1.2516e-02, -1.4085e-02,  ...,  3.8476e-03,\n",
       "                        1.4527e-02,  4.8004e-03],\n",
       "                      [-4.9870e-02,  9.3581e-03, -5.5752e-03,  ..., -8.0646e-04,\n",
       "                        1.2236e-02,  3.0652e-02],\n",
       "                      ...,\n",
       "                      [ 2.9640e-02, -1.8857e-02, -7.9710e-03,  ..., -3.1403e-02,\n",
       "                        4.9274e-02, -2.0044e-03],\n",
       "                      [-1.2566e-02, -2.6421e-03,  4.2273e-02,  ...,  1.7204e-02,\n",
       "                       -1.3719e-03,  1.6898e-02],\n",
       "                      [ 3.3733e-02,  2.4350e-02,  6.7413e-03,  ...,  1.1592e-02,\n",
       "                       -1.8221e-02,  1.2902e-03]])),\n",
       "             ('bert.encoder.layer.4.intermediate.dense.bias',\n",
       "              tensor([-1.4854e-03, -7.3179e-03, -3.8624e-03,  1.7968e-02,  2.3987e-02,\n",
       "                       9.9714e-03,  3.5888e-02,  1.9210e-02,  3.0148e-02, -1.3895e-02,\n",
       "                      -1.4666e-02, -2.0569e-02, -7.5411e-03, -2.3285e-02,  7.9093e-03,\n",
       "                      -3.8591e-03, -3.1959e-04,  2.1876e-02, -2.1515e-02, -2.2328e-02,\n",
       "                       2.9033e-02, -1.3011e-02,  4.1308e-03,  1.6556e-02, -4.5068e-03,\n",
       "                       1.2709e-02, -1.7921e-02, -5.9206e-03,  1.3905e-02, -1.3273e-03,\n",
       "                      -2.7306e-02, -6.4617e-03,  1.2698e-02, -2.2906e-02,  1.8473e-02,\n",
       "                       7.0384e-03, -5.4002e-03,  7.5092e-03, -2.7130e-02,  3.2620e-03,\n",
       "                       2.4345e-02,  2.7145e-02, -8.0069e-04,  1.0143e-02, -9.2409e-03,\n",
       "                      -2.5956e-02, -2.5367e-02, -1.2876e-02, -1.8453e-02, -2.2352e-02,\n",
       "                       4.3102e-03,  1.0917e-02,  8.0063e-05,  1.7208e-02, -3.5360e-02,\n",
       "                      -3.0221e-02,  7.1598e-03,  9.5445e-03, -4.9379e-03, -1.8252e-03,\n",
       "                       1.3975e-02, -2.0973e-02,  3.4039e-02, -5.2556e-03])),\n",
       "             ('bert.encoder.layer.4.output.dense.weight',\n",
       "              tensor([[ 0.0145,  0.0072, -0.0166,  ..., -0.0276,  0.0215,  0.0316],\n",
       "                      [ 0.0178, -0.0167, -0.0156,  ..., -0.0068, -0.0128, -0.0109],\n",
       "                      [ 0.0088, -0.0243, -0.0107,  ..., -0.0238,  0.0292, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0238,  0.0255,  0.0086,  ..., -0.0264,  0.0278,  0.0023],\n",
       "                      [-0.0213, -0.0160,  0.0109,  ...,  0.0314,  0.0219,  0.0103],\n",
       "                      [-0.0284,  0.0091, -0.0182,  ...,  0.0352,  0.0238,  0.0131]])),\n",
       "             ('bert.encoder.layer.4.output.dense.bias',\n",
       "              tensor([ 5.8778e-03,  4.9842e-03,  8.1538e-03, -3.2708e-04, -3.8987e-03,\n",
       "                       2.7655e-04, -6.1985e-03, -1.2274e-02,  2.3118e-03, -2.2273e-03,\n",
       "                       1.1883e-02,  1.6336e-02, -5.0270e-03,  1.1989e-03,  3.0018e-03,\n",
       "                      -8.7210e-03,  4.8739e-03,  1.0749e-02, -4.3413e-03,  2.4792e-03,\n",
       "                       8.3238e-03, -1.0557e-04, -1.1242e-02,  8.5715e-03, -3.0435e-03,\n",
       "                      -2.4667e-03,  3.7525e-03,  2.6333e-02, -3.1502e-03,  9.5665e-03,\n",
       "                       1.2549e-02,  4.3346e-04, -4.9563e-03, -1.4798e-02, -1.3067e-02,\n",
       "                      -3.8608e-03, -9.4838e-03, -1.0907e-02,  4.7604e-03, -3.5318e-03,\n",
       "                       1.2906e-03, -6.9100e-03,  8.0823e-04,  8.7450e-03,  8.6412e-03,\n",
       "                      -5.9683e-03,  6.4318e-03, -6.5855e-03, -1.0019e-03, -4.1238e-03,\n",
       "                      -2.8411e-03, -4.5610e-03, -5.3398e-03,  6.1931e-05,  3.7022e-03,\n",
       "                      -4.6568e-03, -5.0049e-03,  6.0900e-03,  2.0832e-03,  3.2731e-03,\n",
       "                      -1.1777e-03, -1.0065e-04,  7.3038e-03, -8.8985e-03, -3.3664e-03,\n",
       "                      -2.9603e-03, -6.2577e-03,  1.3698e-02,  1.6817e-03, -5.7785e-04,\n",
       "                       8.0122e-03, -2.6936e-03,  4.8180e-03,  1.1228e-02, -1.0002e-03,\n",
       "                       1.0072e-02, -5.0335e-03, -1.1020e-03,  5.4090e-04,  8.3276e-04,\n",
       "                      -3.2166e-03,  7.7679e-03, -4.6070e-03, -3.2532e-03, -2.2275e-03,\n",
       "                      -5.6154e-03, -3.3353e-03, -2.9763e-03,  7.8709e-03, -6.1048e-03,\n",
       "                      -8.6311e-03,  1.3487e-02,  2.1562e-03,  7.6652e-03, -1.8036e-02,\n",
       "                       3.2271e-02,  7.2615e-04, -6.3501e-04,  2.5586e-03,  1.5986e-03,\n",
       "                      -3.4421e-03,  1.0237e-02, -6.0478e-03,  3.3476e-03,  2.0123e-04,\n",
       "                       8.7521e-03, -1.0375e-02,  5.8630e-03, -4.4043e-03,  4.2231e-03,\n",
       "                      -8.7666e-04, -1.5275e-03,  2.6309e-03, -3.8274e-03, -3.1632e-02,\n",
       "                       1.0896e-02,  6.9156e-03, -3.8338e-03,  1.5110e-02, -9.4966e-03,\n",
       "                      -2.7657e-03, -5.9272e-03,  3.8187e-04, -5.1266e-04,  5.6537e-03,\n",
       "                      -2.0968e-03,  1.4736e-02,  1.6476e-03,  1.1563e-02, -3.8725e-03,\n",
       "                       6.8514e-03,  9.7405e-04,  2.3930e-03, -3.8372e-03, -3.4185e-03,\n",
       "                       6.3271e-03, -1.0441e-02,  7.1706e-03, -1.1935e-04,  1.4572e-02,\n",
       "                      -1.2862e-03,  1.9411e-04,  4.4869e-03, -5.1774e-03, -5.2168e-03,\n",
       "                      -8.9763e-03,  2.3889e-03,  2.4460e-03, -1.9811e-03, -4.1849e-03,\n",
       "                      -5.7936e-03,  6.7802e-03, -6.7899e-03, -5.2568e-03, -1.0307e-03,\n",
       "                      -6.0562e-03, -1.2269e-02,  1.2657e-02,  8.2389e-03, -1.1339e-02,\n",
       "                      -4.7606e-03,  1.0474e-03, -1.2779e-03, -8.6472e-03, -2.2952e-03,\n",
       "                       4.5496e-03,  4.2869e-03,  4.2385e-03,  5.6617e-03, -5.2626e-03,\n",
       "                      -4.1856e-03, -2.6916e-03,  5.9389e-03, -7.5047e-03,  1.8078e-03,\n",
       "                       4.0296e-03, -2.1246e-03, -1.2611e-02, -2.0988e-03, -3.6671e-04,\n",
       "                       2.9287e-02, -7.7823e-04, -5.7679e-03, -1.0240e-02,  4.0381e-04,\n",
       "                      -1.4975e-04, -9.7307e-03, -5.4848e-03,  8.3507e-03,  1.2652e-02,\n",
       "                      -4.4695e-03,  1.3786e-03])),\n",
       "             ('bert.encoder.layer.4.output.LayerNorm.weight',\n",
       "              tensor([1.0380, 1.0266, 1.0061, 1.0120, 1.0190, 0.9886, 1.0261, 1.0231, 0.9994,\n",
       "                      1.0259, 0.9796, 1.0265, 1.0308, 1.0228, 1.0173, 1.0100, 1.0202, 1.0056,\n",
       "                      0.9905, 1.0382, 1.0229, 1.0400, 1.0022, 1.0200, 1.0096, 1.0299, 1.0453,\n",
       "                      0.9848, 0.9685, 1.0472, 1.0313, 1.0248, 1.0200, 1.0322, 1.0318, 1.0032,\n",
       "                      1.0174, 0.9487, 1.0327, 1.0260, 1.0242, 1.0337, 1.0257, 1.0243, 1.0345,\n",
       "                      1.0245, 1.0110, 1.0382, 1.0082, 1.0329, 1.0381, 0.9506, 0.9296, 1.0248,\n",
       "                      1.0291, 1.0189, 1.0245, 1.0212, 1.0377, 1.0216, 1.0163, 0.9990, 1.0379,\n",
       "                      1.0067, 1.0276, 1.0206, 1.0274, 0.9103, 1.0230, 1.0088, 1.0180, 0.9919,\n",
       "                      1.0258, 1.0517, 1.0276, 1.0014, 1.0318, 1.0227, 1.0535, 1.0321, 1.0161,\n",
       "                      1.0172, 1.0279, 1.0454, 1.0509, 1.0134, 1.0548, 1.0170, 1.0300, 1.0251,\n",
       "                      1.0211, 0.9874, 1.0280, 1.0105, 1.0013, 0.9370, 1.0187, 0.9672, 1.0220,\n",
       "                      1.0280, 1.0179, 1.0338, 1.0367, 1.0091, 1.0256, 1.0094, 1.0307, 1.0059,\n",
       "                      1.0410, 1.0469, 0.9874, 1.0327, 1.0383, 1.0030, 0.8507, 1.0272, 1.0081,\n",
       "                      1.0144, 1.0204, 1.0449, 0.9928, 0.9548, 1.0197, 1.0155, 1.0346, 1.0430,\n",
       "                      1.0346, 1.0232, 1.0350, 1.0270, 0.9625, 0.9776, 1.0222, 1.0168, 1.0446,\n",
       "                      1.0218, 0.9877, 1.0266, 0.9835, 1.0335, 1.0197, 1.0260, 1.0100, 1.0382,\n",
       "                      0.9942, 1.0032, 0.9861, 1.0248, 1.0211, 1.0454, 1.0179, 0.9447, 1.0267,\n",
       "                      1.0363, 1.0044, 1.0337, 1.0356, 1.0015, 1.0133, 0.9515, 1.0135, 1.0018,\n",
       "                      1.0189, 1.0283, 1.0054, 1.0308, 1.0178, 1.0266, 1.0262, 1.0100, 1.0065,\n",
       "                      1.0212, 1.0280, 1.0298, 1.0350, 1.0156, 1.0312, 0.9586, 1.0236, 1.0143,\n",
       "                      0.9002, 1.0314, 1.0146, 1.0345, 1.0221, 1.0343, 1.0126, 1.0232, 0.9933,\n",
       "                      0.9844, 1.0017, 1.0397])),\n",
       "             ('bert.encoder.layer.4.output.LayerNorm.bias',\n",
       "              tensor([ 4.5536e-04,  8.0099e-03,  1.2062e-02,  9.0986e-03,  1.9523e-03,\n",
       "                      -1.4705e-02, -1.0440e-02, -1.4899e-02,  7.0731e-03, -5.1542e-03,\n",
       "                       3.1410e-02,  1.8845e-02, -1.0840e-02,  1.4134e-02,  1.3801e-02,\n",
       "                      -6.5202e-03,  1.3986e-03,  1.9902e-02, -1.3982e-03, -4.2304e-05,\n",
       "                       6.8979e-03, -6.5548e-03, -5.7511e-03,  4.6976e-03, -2.0765e-03,\n",
       "                      -8.6527e-03,  8.6044e-03,  2.2533e-02,  5.3369e-03,  1.6327e-02,\n",
       "                       2.2287e-02, -1.6334e-03, -1.0199e-02, -1.5038e-02, -1.3233e-02,\n",
       "                      -1.2731e-02, -1.1609e-02, -2.6779e-03,  4.2840e-03, -8.1444e-03,\n",
       "                      -2.1035e-04, -1.1584e-02, -6.2114e-03,  1.7241e-02,  4.5695e-03,\n",
       "                      -9.6746e-03,  2.3145e-03, -4.3053e-04, -3.8796e-03, -2.7635e-03,\n",
       "                      -8.5510e-03,  2.7762e-03,  1.0942e-02,  1.5697e-03,  8.1437e-03,\n",
       "                      -1.2554e-02, -4.6300e-03, -6.2337e-03,  6.2227e-04,  2.2531e-04,\n",
       "                      -6.0169e-03, -8.4368e-03,  1.3771e-02, -9.4711e-03,  5.0295e-05,\n",
       "                       8.3831e-04, -1.0066e-02, -1.6187e-03,  1.6660e-03, -8.2572e-04,\n",
       "                       6.6913e-03, -1.0129e-03, -3.6829e-03,  1.5497e-02, -3.9385e-03,\n",
       "                       1.6119e-02,  9.2634e-04,  6.4315e-03, -2.0004e-03, -3.3265e-04,\n",
       "                      -1.6477e-03,  1.5475e-02, -2.1374e-03, -1.5832e-02,  4.0149e-03,\n",
       "                      -9.2989e-03, -1.0999e-02, -7.7390e-03,  1.7224e-02, -2.3934e-03,\n",
       "                      -1.7875e-02,  9.9147e-03,  3.9528e-03,  6.7472e-03, -1.2488e-02,\n",
       "                       1.6851e-02,  1.1421e-03, -1.1873e-02,  4.0367e-04,  1.4792e-03,\n",
       "                      -9.3170e-03,  1.5628e-02, -6.7758e-03,  1.2135e-03,  6.8755e-04,\n",
       "                       7.8275e-03, -1.2957e-02,  6.3684e-03, -1.1643e-02,  6.4658e-03,\n",
       "                      -1.2589e-03, -4.2964e-04,  7.3696e-03, -1.9632e-02,  2.5559e-02,\n",
       "                       1.9235e-02,  2.4595e-03, -5.3170e-03,  1.5564e-02, -1.6644e-02,\n",
       "                       2.9263e-03, -2.0303e-02,  1.8140e-03, -3.4412e-03,  3.8561e-03,\n",
       "                       2.0918e-03,  1.9055e-02,  3.1774e-03,  2.2321e-02, -5.8323e-03,\n",
       "                      -2.8374e-03,  3.4342e-03,  1.1053e-02, -7.4604e-03, -1.3109e-03,\n",
       "                       1.2954e-02,  5.0923e-03,  9.4718e-03,  7.0264e-03,  2.0137e-02,\n",
       "                      -6.1093e-03,  2.4908e-03,  3.7174e-03, -3.6242e-03, -3.8911e-03,\n",
       "                      -1.0134e-02,  4.7114e-05,  5.2156e-03,  2.3405e-03, -8.6101e-03,\n",
       "                       2.4485e-03, -5.7275e-03, -6.9544e-03, -1.1179e-02, -8.5812e-03,\n",
       "                      -5.9195e-03, -1.9260e-02, -2.2849e-03,  1.2238e-02,  5.9443e-04,\n",
       "                      -9.4146e-03,  7.4059e-03,  1.4085e-04, -6.9448e-03,  2.7550e-03,\n",
       "                       1.1564e-02,  9.6740e-04,  7.8776e-03,  1.5825e-02, -8.5771e-03,\n",
       "                      -3.7508e-03, -1.8586e-02,  1.5795e-03, -6.6262e-03,  1.3135e-02,\n",
       "                       1.6423e-03, -1.1897e-02, -9.3537e-03, -8.0492e-04, -1.0527e-03,\n",
       "                       2.7258e-03, -6.7332e-03, -1.0983e-02, -1.0262e-02,  6.1153e-03,\n",
       "                       9.2110e-03, -1.1673e-02,  6.8945e-05,  3.9481e-03,  6.0037e-03,\n",
       "                      -6.5718e-03, -2.2910e-04])),\n",
       "             ('bert.encoder.layer.5.attention.self.query.weight',\n",
       "              tensor([[ 0.0249, -0.0487, -0.0328,  ..., -0.0283,  0.0579,  0.0275],\n",
       "                      [ 0.0154, -0.0320,  0.0095,  ...,  0.0120, -0.0011,  0.0489],\n",
       "                      [-0.0178, -0.0130, -0.0371,  ..., -0.0099,  0.0173,  0.0499],\n",
       "                      ...,\n",
       "                      [-0.0356,  0.0224, -0.0566,  ...,  0.0179,  0.0024,  0.0171],\n",
       "                      [-0.0150, -0.0227, -0.0525,  ..., -0.0708,  0.0044, -0.0215],\n",
       "                      [-0.0069,  0.0263, -0.0090,  ...,  0.0305, -0.0285, -0.0126]])),\n",
       "             ('bert.encoder.layer.5.attention.self.query.bias',\n",
       "              tensor([-3.3469e-02, -2.2286e-02, -1.5606e-02, -1.8061e-02,  3.3367e-02,\n",
       "                      -1.6786e-02,  8.0374e-03, -2.0422e-02,  3.2736e-02, -4.9067e-03,\n",
       "                       1.8100e-03,  5.2797e-03,  5.4836e-03,  2.7408e-02, -2.3455e-02,\n",
       "                      -1.4033e-02, -8.0355e-03, -9.2293e-03, -5.1406e-04, -7.7880e-03,\n",
       "                      -1.8388e-02,  3.0658e-02, -4.1480e-02,  7.8231e-03,  2.7542e-02,\n",
       "                       2.3130e-02, -4.6169e-02, -1.7389e-02,  3.9791e-02,  5.8022e-03,\n",
       "                      -1.4513e-03, -1.8931e-02,  2.0631e-02,  4.0420e-03, -9.5750e-03,\n",
       "                      -4.0550e-02,  3.2458e-02, -5.5230e-03,  3.5400e-02, -2.0900e-02,\n",
       "                       7.7578e-03, -4.4718e-02,  3.1635e-02,  2.5435e-02, -4.9739e-02,\n",
       "                      -4.3317e-03, -1.1022e-02, -2.8650e-02, -5.5906e-02, -5.6876e-02,\n",
       "                       3.1976e-02, -9.2993e-03,  1.1885e-02,  1.5996e-02,  2.3849e-02,\n",
       "                       3.5762e-02, -4.1887e-02,  5.9313e-03, -2.6727e-02,  2.4278e-02,\n",
       "                      -4.5071e-02, -4.1371e-02, -9.2606e-03, -3.8204e-02,  2.6380e-02,\n",
       "                       2.5180e-02,  1.1014e-02, -1.5633e-02,  1.6812e-02, -7.5001e-03,\n",
       "                      -1.2388e-02, -2.7821e-03,  1.4597e-02,  1.7036e-02, -4.6719e-02,\n",
       "                      -6.8816e-05, -1.6013e-02,  3.7765e-02,  1.1461e-02, -4.5392e-02,\n",
       "                       2.7958e-02,  6.5200e-03, -4.6423e-02,  1.8249e-03, -3.5528e-02,\n",
       "                       5.1705e-03, -2.2993e-02,  1.2708e-02,  5.9935e-02,  4.4134e-03,\n",
       "                       4.1974e-02,  3.6109e-03,  2.3376e-02, -2.2768e-04, -4.0071e-02,\n",
       "                      -7.9357e-03, -2.7076e-02, -4.1150e-02, -1.6044e-02, -4.8144e-02,\n",
       "                       3.1831e-04,  1.6075e-02,  5.1154e-03, -6.4967e-02, -2.6403e-02,\n",
       "                      -2.4792e-02, -1.6802e-02, -2.9846e-02,  2.6487e-02, -1.3610e-02,\n",
       "                       6.5758e-03,  4.5238e-02,  1.7647e-02, -3.5044e-03, -2.9599e-03,\n",
       "                       2.8594e-02,  5.0074e-02, -2.1529e-02, -1.1212e-03,  3.3176e-03,\n",
       "                       1.7497e-02,  4.8686e-02,  2.6062e-02, -2.0962e-02,  3.6025e-02,\n",
       "                       5.8416e-02,  2.0150e-02,  2.5669e-02,  3.1435e-02,  1.9137e-02,\n",
       "                      -2.3841e-03, -2.2776e-02, -2.7429e-02,  3.7943e-02,  1.7159e-02,\n",
       "                      -2.8360e-02, -4.4072e-03, -3.3412e-03, -3.5010e-02, -3.2685e-02,\n",
       "                      -1.0270e-02, -2.5507e-02,  8.7467e-03, -2.5073e-02, -2.6019e-02,\n",
       "                       2.4945e-02, -2.4227e-02, -4.0521e-02,  2.6958e-02,  2.4094e-02,\n",
       "                       1.7532e-02,  2.0983e-02,  1.0474e-02, -1.3317e-02,  2.9241e-03,\n",
       "                      -3.8671e-03, -3.8617e-02, -3.6636e-02, -1.7503e-02,  3.3514e-02,\n",
       "                      -2.0409e-02,  2.4748e-02,  6.1194e-03, -2.1033e-02,  5.9026e-03,\n",
       "                      -8.2350e-03,  1.6618e-02, -8.2303e-03,  2.4233e-03, -1.0251e-02,\n",
       "                       3.4610e-02,  4.7369e-03, -2.7018e-02,  3.1823e-02, -1.5098e-02,\n",
       "                       1.4174e-02, -1.4496e-02,  8.0955e-03, -2.0577e-02, -1.0571e-02,\n",
       "                       1.0256e-02, -8.4979e-03,  1.0801e-02, -2.3056e-02,  2.9382e-02,\n",
       "                      -2.4253e-02,  1.5563e-02, -1.7397e-02,  3.8230e-02, -1.1453e-02,\n",
       "                      -1.6174e-02,  3.1240e-02])),\n",
       "             ('bert.encoder.layer.5.attention.self.key.weight',\n",
       "              tensor([[ 0.0295,  0.0015,  0.0129,  ...,  0.0441, -0.0491, -0.0173],\n",
       "                      [ 0.0142,  0.0472,  0.0079,  ...,  0.0566, -0.0255, -0.0273],\n",
       "                      [ 0.0029,  0.0263, -0.0089,  ...,  0.0018, -0.0504,  0.0001],\n",
       "                      ...,\n",
       "                      [-0.0180,  0.0096,  0.0009,  ...,  0.0120, -0.0112,  0.0011],\n",
       "                      [-0.0215,  0.0497,  0.0044,  ..., -0.0019, -0.0167,  0.0052],\n",
       "                      [-0.0661, -0.0088,  0.0374,  ..., -0.0198, -0.0195,  0.0153]])),\n",
       "             ('bert.encoder.layer.5.attention.self.key.bias',\n",
       "              tensor([ 1.2006e-08,  2.5241e-08, -3.3545e-08,  7.9822e-08, -1.9912e-07,\n",
       "                       4.4925e-08,  7.8977e-09,  6.1361e-08, -3.5873e-08, -7.4048e-08,\n",
       "                       1.2332e-08,  1.5812e-07, -5.6591e-08, -1.5246e-07,  1.1140e-07,\n",
       "                       4.6276e-08, -6.2483e-08,  5.8808e-08, -5.1856e-08,  6.1014e-08,\n",
       "                      -4.5485e-08,  9.1607e-08,  5.5133e-08,  2.1135e-08, -1.4441e-07,\n",
       "                      -2.6967e-08,  2.7906e-08,  7.5913e-08, -3.4241e-08, -1.0583e-07,\n",
       "                       3.3792e-08, -6.8479e-08,  8.7946e-08, -5.5637e-08,  7.4945e-08,\n",
       "                      -1.3219e-07,  3.1163e-07, -8.7147e-08,  1.8036e-07, -1.5045e-07,\n",
       "                       6.7543e-09, -2.4805e-07,  8.0205e-08,  9.4452e-08, -1.5515e-07,\n",
       "                       2.8853e-08,  7.8555e-08, -2.6695e-07, -1.8442e-07, -2.1132e-07,\n",
       "                       7.5734e-09, -6.3870e-08,  1.6945e-07,  8.1975e-08,  1.2684e-07,\n",
       "                       2.5946e-07, -1.6534e-07, -4.5546e-09, -1.6192e-07,  1.6211e-07,\n",
       "                      -1.0048e-07, -2.2443e-07,  2.7358e-08, -3.0156e-07,  9.3673e-08,\n",
       "                      -2.8063e-07, -2.5775e-07, -1.8328e-07, -8.3110e-08,  8.3305e-08,\n",
       "                      -5.0150e-08, -2.4964e-07, -1.4820e-07,  1.1656e-08,  4.2000e-08,\n",
       "                       1.9009e-07, -1.5287e-07,  3.4844e-08,  7.3271e-08,  2.7569e-07,\n",
       "                      -1.3763e-07, -2.0791e-07, -3.4996e-08, -2.2061e-08, -6.5393e-08,\n",
       "                      -1.2811e-07, -5.0495e-08, -2.3926e-07, -8.3380e-08, -8.0721e-08,\n",
       "                      -2.5904e-07, -5.2988e-08,  1.3724e-08,  2.2764e-08,  6.2207e-08,\n",
       "                       1.0874e-07,  1.0240e-07,  4.0840e-08,  1.9044e-08,  3.2037e-08,\n",
       "                       8.9812e-08,  4.5897e-08,  2.2170e-09,  2.0096e-08,  1.4513e-08,\n",
       "                       9.0574e-08, -2.4052e-08,  5.1323e-08,  1.4324e-08, -1.2088e-07,\n",
       "                      -6.8254e-09, -6.3706e-08, -1.0095e-07, -2.8572e-08, -2.7493e-09,\n",
       "                       3.7464e-08, -7.4348e-08,  3.1817e-08, -4.4578e-08, -4.5678e-08,\n",
       "                       6.4301e-08, -1.0122e-07,  1.3191e-08, -6.0478e-09, -1.2291e-08,\n",
       "                      -1.1561e-07, -1.1314e-08,  3.1153e-08, -3.4439e-07, -3.8850e-07,\n",
       "                       3.5044e-07,  4.0649e-07,  5.2445e-07, -3.4138e-07, -4.5156e-08,\n",
       "                       3.4929e-08,  3.0332e-07,  9.2877e-08,  3.2233e-07,  8.7682e-08,\n",
       "                      -1.0626e-07,  2.2512e-07,  3.8664e-07,  4.1359e-07,  1.4209e-07,\n",
       "                      -3.1805e-07,  3.4395e-07,  1.1119e-07, -1.2649e-07, -2.1477e-07,\n",
       "                       3.2412e-08, -7.4194e-08, -4.0819e-07,  4.9064e-07, -9.3954e-08,\n",
       "                      -1.3383e-07,  9.8388e-08,  4.0967e-07,  2.5517e-07, -2.4923e-08,\n",
       "                      -1.3553e-07, -4.1146e-08, -1.1789e-07, -1.7162e-07,  4.9726e-08,\n",
       "                      -1.8985e-07, -8.8065e-08, -5.3889e-08,  1.5322e-07, -3.2165e-08,\n",
       "                       1.2691e-07,  7.9238e-09, -9.1979e-08,  2.4858e-07, -1.3370e-07,\n",
       "                       6.1797e-08, -1.6239e-07,  3.0581e-08, -1.1414e-07, -5.7340e-08,\n",
       "                      -8.6193e-08,  5.5417e-08, -7.0124e-08,  4.6386e-08,  2.4687e-07,\n",
       "                       1.3516e-09,  1.4093e-07, -1.5883e-07,  1.9151e-07,  6.3447e-08,\n",
       "                      -8.6038e-08,  1.6833e-07])),\n",
       "             ('bert.encoder.layer.5.attention.self.value.weight',\n",
       "              tensor([[ 0.0011,  0.0171, -0.0211,  ..., -0.0204, -0.0139,  0.0154],\n",
       "                      [-0.0162, -0.0194,  0.0173,  ...,  0.0367,  0.0206,  0.0060],\n",
       "                      [ 0.0194,  0.0163, -0.0315,  ...,  0.0019, -0.0287,  0.0048],\n",
       "                      ...,\n",
       "                      [-0.0149,  0.0203, -0.0274,  ...,  0.0011,  0.0138,  0.0446],\n",
       "                      [-0.0054,  0.0029, -0.0198,  ...,  0.0206, -0.0287, -0.0098],\n",
       "                      [-0.0352,  0.0075,  0.0179,  ..., -0.0097,  0.0031,  0.0406]])),\n",
       "             ('bert.encoder.layer.5.attention.self.value.bias',\n",
       "              tensor([-0.0020,  0.0091, -0.0028, -0.0049,  0.0015, -0.0059,  0.0167,  0.0094,\n",
       "                      -0.0092,  0.0012, -0.0051,  0.0023, -0.0134, -0.0016,  0.0074, -0.0039,\n",
       "                      -0.0119,  0.0070,  0.0065,  0.0083, -0.0118, -0.0134, -0.0260, -0.0024,\n",
       "                       0.0069, -0.0159,  0.0024,  0.0033,  0.0195, -0.0091, -0.0056, -0.0010,\n",
       "                       0.0031, -0.0107, -0.0051, -0.0026,  0.0034, -0.0132, -0.0063,  0.0041,\n",
       "                      -0.0067, -0.0103,  0.0056, -0.0027, -0.0032,  0.0112, -0.0104,  0.0032,\n",
       "                       0.0015,  0.0075,  0.0015, -0.0008,  0.0035,  0.0090,  0.0089,  0.0072,\n",
       "                       0.0014,  0.0031, -0.0027, -0.0009, -0.0125,  0.0107, -0.0089,  0.0046,\n",
       "                      -0.0036, -0.0077,  0.0011,  0.0171,  0.0029, -0.0064, -0.0044,  0.0037,\n",
       "                      -0.0014,  0.0069, -0.0066,  0.0042, -0.0133, -0.0019,  0.0082,  0.0018,\n",
       "                      -0.0005, -0.0036,  0.0045,  0.0060,  0.0084,  0.0012,  0.0170, -0.0126,\n",
       "                      -0.0013, -0.0085,  0.0004, -0.0002,  0.0005, -0.0167, -0.0070,  0.0115,\n",
       "                       0.0023, -0.0049, -0.0031,  0.0037,  0.0038,  0.0024, -0.0039, -0.0044,\n",
       "                      -0.0008, -0.0013,  0.0004,  0.0004,  0.0066,  0.0026,  0.0064, -0.0048,\n",
       "                       0.0010,  0.0108, -0.0047,  0.0053, -0.0009, -0.0008,  0.0036,  0.0034,\n",
       "                      -0.0047,  0.0048, -0.0074,  0.0009, -0.0040, -0.0070,  0.0097, -0.0029,\n",
       "                      -0.0057,  0.0044, -0.0049,  0.0015,  0.0047, -0.0026,  0.0002,  0.0086,\n",
       "                      -0.0012,  0.0044, -0.0005,  0.0031, -0.0003, -0.0033,  0.0057, -0.0029,\n",
       "                      -0.0007,  0.0044, -0.0035,  0.0014,  0.0016, -0.0032,  0.0024,  0.0043,\n",
       "                       0.0016, -0.0064,  0.0047, -0.0046, -0.0019,  0.0015,  0.0056, -0.0021,\n",
       "                      -0.0039,  0.0007,  0.0160, -0.0029, -0.0018,  0.0064,  0.0035,  0.0099,\n",
       "                      -0.0017, -0.0036,  0.0065, -0.0028, -0.0035,  0.0018,  0.0041,  0.0028,\n",
       "                      -0.0014,  0.0008,  0.0063,  0.0088, -0.0015,  0.0016,  0.0012,  0.0075,\n",
       "                       0.0064,  0.0058,  0.0011,  0.0037,  0.0028, -0.0053,  0.0089,  0.0012])),\n",
       "             ('bert.encoder.layer.5.attention.output.dense.weight',\n",
       "              tensor([[-0.0150,  0.0164,  0.0387,  ..., -0.0204, -0.0086, -0.0323],\n",
       "                      [-0.0323,  0.0266, -0.0034,  ...,  0.0132,  0.0281, -0.0106],\n",
       "                      [ 0.0053,  0.0077, -0.0464,  ..., -0.0216,  0.0003, -0.0014],\n",
       "                      ...,\n",
       "                      [-0.0167,  0.0505, -0.0211,  ..., -0.0052,  0.0486, -0.0388],\n",
       "                      [ 0.0003, -0.0005, -0.0230,  ...,  0.0261,  0.0111, -0.0186],\n",
       "                      [ 0.0070, -0.0367, -0.0059,  ..., -0.0090, -0.0141, -0.0140]])),\n",
       "             ('bert.encoder.layer.5.attention.output.dense.bias',\n",
       "              tensor([ 2.3620e-03, -9.2013e-04,  7.8316e-03,  7.0336e-03,  3.4468e-03,\n",
       "                      -1.1076e-02, -5.6248e-03, -9.3464e-03,  1.1208e-03, -5.0799e-03,\n",
       "                       1.5829e-02,  8.4008e-03, -7.8968e-03,  2.9275e-03,  9.3285e-03,\n",
       "                      -2.7620e-03,  4.7509e-03,  1.4375e-02, -1.7387e-03, -5.0032e-03,\n",
       "                      -9.4200e-04, -5.1988e-03,  4.5425e-03,  1.9308e-03, -2.5991e-03,\n",
       "                      -8.1004e-03,  3.5463e-03,  1.5324e-02,  1.6454e-03,  1.2003e-02,\n",
       "                       1.4920e-02, -1.6807e-03, -6.2587e-03, -4.4869e-03, -1.5168e-02,\n",
       "                      -7.2178e-03, -6.7120e-03, -5.8903e-04, -1.0992e-03, -5.8231e-03,\n",
       "                       2.7375e-03, -7.1063e-03, -6.8010e-03,  9.9190e-03, -8.6203e-04,\n",
       "                      -2.8679e-03,  3.1816e-03,  1.5748e-03, -1.5275e-03, -7.2547e-03,\n",
       "                      -2.9346e-03,  6.4598e-03,  1.1555e-02, -1.4463e-03,  3.2239e-03,\n",
       "                      -8.2558e-03, -7.4158e-03, -2.2782e-03, -3.1370e-03, -7.0749e-03,\n",
       "                      -1.1013e-03, -7.3459e-03,  7.1397e-03,  2.9453e-03, -5.6982e-03,\n",
       "                       3.9639e-03, -5.0677e-03, -4.2680e-03, -4.4581e-03, -6.1207e-04,\n",
       "                       3.0330e-03,  2.4523e-03, -1.1345e-03,  9.1641e-03, -2.3919e-04,\n",
       "                       7.4715e-03,  5.7747e-06,  3.6630e-03, -2.9401e-03, -2.8878e-03,\n",
       "                      -6.7858e-03,  3.7066e-03, -1.0983e-03, -6.7379e-04, -2.0746e-03,\n",
       "                      -1.9947e-03, -8.6795e-03, -3.4143e-03,  7.5320e-03,  6.9089e-03,\n",
       "                      -4.7674e-03,  4.4176e-05, -1.3187e-03,  6.0885e-03, -3.5309e-03,\n",
       "                       5.7967e-03, -3.0752e-03, -8.6630e-03, -2.9652e-03,  5.6349e-03,\n",
       "                      -6.2062e-03,  8.1930e-03, -7.3563e-03,  1.4029e-03, -5.4550e-03,\n",
       "                       2.2393e-03, -1.0299e-02, -2.9689e-03, -3.3644e-04,  2.6160e-03,\n",
       "                       2.7252e-03, -5.1921e-03,  2.2080e-03, -9.1181e-03,  3.3372e-02,\n",
       "                       7.1705e-03, -2.0669e-03, -2.0250e-03,  8.9720e-03, -1.5368e-02,\n",
       "                       4.1776e-03, -1.1914e-02,  3.5285e-03, -2.1411e-03,  9.8515e-03,\n",
       "                       3.3755e-03,  1.0283e-02, -4.9750e-03,  1.2095e-02, -4.2389e-04,\n",
       "                      -3.4900e-04, -9.6948e-04,  7.0151e-03, -7.4600e-03, -2.5372e-04,\n",
       "                       1.2096e-02,  4.3517e-03,  5.0238e-03,  3.1284e-03,  1.2150e-02,\n",
       "                      -5.0814e-03,  3.3819e-03,  3.2718e-03, -1.8585e-03,  5.5931e-04,\n",
       "                      -3.2477e-03, -2.2306e-03,  2.1107e-03, -7.4933e-03, -4.4842e-03,\n",
       "                      -1.2794e-03, -1.2867e-02,  1.4839e-03, -5.5361e-03, -1.0307e-02,\n",
       "                      -6.9929e-03, -1.6685e-02, -2.0000e-03,  5.1857e-03,  5.4580e-03,\n",
       "                      -2.6606e-03,  1.4681e-02,  3.7126e-03, -1.3453e-03, -2.2767e-05,\n",
       "                       3.5518e-03, -8.4240e-04,  5.9214e-03,  7.1336e-03, -1.8039e-03,\n",
       "                       7.2890e-03, -8.2689e-03, -4.3390e-03, -8.3141e-03,  4.2701e-03,\n",
       "                       6.2903e-03, -3.8902e-03, -8.0431e-03, -4.1857e-03, -4.3616e-03,\n",
       "                      -5.7164e-03, -3.4901e-03, -5.5835e-03, -1.3018e-03,  4.4708e-03,\n",
       "                      -1.8924e-03, -3.4705e-03, -4.1363e-03,  2.0563e-03, -1.7320e-03,\n",
       "                       7.3386e-04,  2.4302e-03])),\n",
       "             ('bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0307, 1.0193, 0.9933, 1.0127, 1.0104, 0.9842, 1.0170, 1.0182, 0.9930,\n",
       "                      1.0168, 0.9684, 1.0274, 1.0215, 1.0146, 1.0103, 1.0066, 1.0169, 1.0050,\n",
       "                      0.9894, 1.0338, 1.0165, 1.0367, 0.9910, 1.0126, 0.9943, 1.0224, 1.0358,\n",
       "                      0.9792, 0.9593, 1.0382, 1.0290, 1.0150, 1.0150, 1.0216, 1.0260, 0.9966,\n",
       "                      1.0031, 0.9451, 1.0153, 1.0229, 1.0110, 1.0349, 1.0160, 1.0178, 1.0290,\n",
       "                      1.0108, 1.0075, 1.0323, 1.0077, 1.0233, 1.0311, 0.9456, 0.9265, 1.0166,\n",
       "                      1.0282, 1.0141, 1.0122, 1.0193, 1.0284, 1.0044, 1.0074, 0.9963, 1.0240,\n",
       "                      0.9976, 1.0224, 1.0073, 1.0237, 0.9015, 1.0197, 1.0020, 1.0164, 0.9848,\n",
       "                      1.0221, 1.0482, 1.0085, 0.9926, 1.0234, 1.0164, 1.0485, 1.0140, 1.0079,\n",
       "                      1.0090, 1.0163, 1.0337, 1.0466, 1.0117, 1.0485, 1.0135, 1.0257, 1.0193,\n",
       "                      1.0096, 0.9762, 1.0131, 1.0042, 0.9902, 0.9312, 1.0142, 0.9601, 1.0092,\n",
       "                      1.0188, 1.0106, 1.0273, 1.0174, 0.9986, 1.0203, 1.0007, 1.0238, 0.9924,\n",
       "                      1.0328, 1.0247, 0.9805, 1.0192, 1.0326, 0.9882, 0.8483, 1.0231, 0.9958,\n",
       "                      1.0033, 1.0189, 1.0352, 0.9867, 0.9440, 1.0134, 1.0017, 1.0254, 1.0335,\n",
       "                      1.0296, 1.0158, 1.0312, 1.0219, 0.9581, 0.9668, 1.0078, 1.0035, 1.0379,\n",
       "                      1.0101, 0.9774, 1.0193, 0.9792, 1.0214, 1.0104, 1.0168, 1.0065, 1.0240,\n",
       "                      0.9863, 1.0008, 0.9788, 1.0157, 1.0134, 1.0374, 1.0070, 0.9346, 1.0181,\n",
       "                      1.0237, 0.9921, 1.0275, 1.0325, 0.9957, 1.0065, 0.9425, 1.0070, 0.9995,\n",
       "                      1.0024, 1.0224, 0.9932, 1.0214, 1.0173, 1.0237, 1.0194, 1.0078, 0.9998,\n",
       "                      1.0230, 1.0117, 1.0262, 1.0240, 1.0095, 1.0159, 0.9590, 1.0181, 1.0093,\n",
       "                      0.8959, 1.0279, 1.0104, 1.0297, 1.0152, 1.0287, 1.0096, 1.0155, 0.9860,\n",
       "                      0.9806, 0.9944, 1.0261])),\n",
       "             ('bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       "              tensor([-9.0228e-04,  9.3638e-03,  1.3245e-02,  1.5972e-02,  4.0093e-03,\n",
       "                      -1.3660e-02, -1.2218e-02, -1.7052e-02,  6.8358e-03, -5.3018e-03,\n",
       "                       3.2003e-02,  1.9273e-02, -1.2022e-02,  1.7256e-02,  1.5918e-02,\n",
       "                      -4.8231e-03,  1.3234e-03,  2.5483e-02,  4.0200e-04,  5.9878e-06,\n",
       "                       1.0173e-02, -8.2907e-03, -3.9678e-03,  6.3389e-03, -4.8388e-03,\n",
       "                      -1.1864e-02,  1.0226e-02,  2.3821e-02,  6.1897e-03,  1.6933e-02,\n",
       "                       2.5713e-02,  1.3161e-03, -1.1628e-02, -1.5094e-02, -1.5479e-02,\n",
       "                      -1.4948e-02, -1.5301e-02, -5.1038e-03,  1.5259e-03, -7.1522e-03,\n",
       "                      -1.6284e-03, -1.4919e-02, -1.0318e-02,  1.9679e-02,  4.5474e-03,\n",
       "                      -7.9936e-03,  3.1927e-03, -6.8059e-04, -3.8435e-03, -1.1587e-03,\n",
       "                      -1.1498e-02,  4.7050e-03,  1.0818e-02,  3.6911e-03,  1.0602e-02,\n",
       "                      -1.6380e-02, -4.9227e-03, -7.0821e-03,  1.1681e-03,  1.2363e-03,\n",
       "                      -8.0847e-03, -1.1275e-02,  1.4247e-02, -9.5656e-03,  4.0412e-04,\n",
       "                       7.8784e-04, -1.3085e-02, -4.4901e-04,  2.5097e-03, -1.1318e-03,\n",
       "                       9.7463e-03,  8.0328e-05, -6.0496e-03,  2.1048e-02, -1.5262e-03,\n",
       "                       1.7927e-02,  9.4427e-04,  7.8530e-03, -4.2749e-04, -4.8071e-04,\n",
       "                      -2.7974e-03,  1.6565e-02, -1.1711e-03, -1.9477e-02,  5.7297e-03,\n",
       "                      -9.4112e-03, -1.5475e-02, -1.1852e-02,  1.8831e-02,  1.5273e-04,\n",
       "                      -1.7901e-02,  1.3122e-02,  2.1498e-03,  5.9095e-03, -8.7104e-03,\n",
       "                       1.8113e-02,  2.3131e-03, -1.5851e-02,  2.4990e-03, -1.8839e-03,\n",
       "                      -9.6352e-03,  1.5802e-02, -6.6235e-03, -1.1438e-03, -2.1586e-03,\n",
       "                       6.5870e-03, -1.6921e-02,  4.7070e-03, -1.3463e-02,  8.0564e-03,\n",
       "                      -1.5985e-03, -9.7002e-04,  8.7599e-03, -2.2147e-02,  2.0231e-02,\n",
       "                       2.1737e-02,  3.2116e-04, -5.6263e-03,  1.5121e-02, -1.7845e-02,\n",
       "                       3.2609e-03, -1.9141e-02,  1.7695e-03, -2.1824e-03,  4.6848e-03,\n",
       "                       1.9155e-03,  2.2262e-02,  1.7538e-03,  2.6938e-02, -5.7051e-03,\n",
       "                      -2.4868e-03,  2.6619e-03,  1.0326e-02, -8.6372e-03, -1.6737e-03,\n",
       "                       1.6859e-02,  7.5914e-03,  1.1053e-02,  2.8560e-03,  2.4831e-02,\n",
       "                      -9.1229e-03,  4.1906e-03,  2.7934e-03, -2.4375e-03, -4.6360e-03,\n",
       "                      -1.3287e-02,  6.8240e-04,  8.7911e-03,  3.4819e-04, -9.3258e-03,\n",
       "                       4.6703e-03, -6.3410e-03, -9.3196e-03, -8.3338e-03, -1.1915e-02,\n",
       "                      -6.0797e-03, -2.3925e-02, -4.0534e-03,  1.1650e-02,  8.6707e-04,\n",
       "                      -1.1415e-02,  1.0940e-02,  9.1870e-04, -8.8499e-03,  4.8776e-03,\n",
       "                       1.2519e-02, -9.4445e-04,  9.8107e-03,  1.8447e-02, -9.6272e-03,\n",
       "                      -4.1099e-03, -2.1281e-02, -2.8119e-03, -6.8706e-03,  1.2127e-02,\n",
       "                       4.3606e-04, -1.3600e-02, -1.5218e-02, -1.4313e-03, -1.1615e-03,\n",
       "                       5.4910e-03, -1.0482e-02, -1.4420e-02, -1.2498e-02,  6.5442e-03,\n",
       "                       9.5666e-03, -1.1860e-02,  1.1821e-03,  4.2762e-03,  4.8806e-03,\n",
       "                      -7.7769e-03, -2.2695e-03])),\n",
       "             ('bert.encoder.layer.5.intermediate.dense.weight',\n",
       "              tensor([[ 0.0428, -0.0614,  0.0382,  ..., -0.0239,  0.0258, -0.0246],\n",
       "                      [-0.0100, -0.0206,  0.0076,  ...,  0.0151, -0.0060,  0.0272],\n",
       "                      [-0.0429, -0.0279,  0.0305,  ...,  0.0528,  0.0243, -0.0103],\n",
       "                      ...,\n",
       "                      [-0.0164, -0.0280, -0.0183,  ...,  0.0092,  0.0148,  0.0234],\n",
       "                      [ 0.0031, -0.0074,  0.0456,  ..., -0.0269, -0.0302,  0.0232],\n",
       "                      [-0.0450,  0.0091, -0.0363,  ..., -0.0294,  0.0324, -0.0399]])),\n",
       "             ('bert.encoder.layer.5.intermediate.dense.bias',\n",
       "              tensor([-4.6155e-03,  3.4288e-02,  1.4150e-02, -3.0023e-03, -1.3184e-03,\n",
       "                      -1.4228e-02, -1.2283e-02, -1.6272e-02, -6.0292e-03,  3.2425e-02,\n",
       "                       1.5867e-02, -1.3036e-02,  3.1609e-02, -1.3658e-02,  1.5289e-02,\n",
       "                      -2.7055e-02, -1.1539e-02, -1.3435e-02, -1.8962e-02,  1.1888e-02,\n",
       "                       3.0102e-02,  1.8275e-02, -9.5526e-03, -9.3743e-03, -2.7886e-03,\n",
       "                       1.4261e-02, -2.4983e-02, -2.3259e-02,  1.5399e-02, -7.3491e-03,\n",
       "                      -3.3216e-02,  2.2487e-02,  2.2815e-02, -1.4929e-02, -2.1504e-02,\n",
       "                      -1.7841e-02, -1.6864e-02,  1.6730e-03, -8.5055e-03,  1.1526e-02,\n",
       "                       7.2859e-03, -1.8329e-02,  2.9620e-02,  1.5131e-02, -9.2016e-03,\n",
       "                       2.0629e-02,  2.9782e-02, -6.1845e-03,  1.6909e-02,  1.9501e-02,\n",
       "                       1.8890e-02, -1.6884e-02, -2.1053e-02, -1.1261e-02, -2.6031e-02,\n",
       "                      -8.0745e-03,  2.4845e-02,  1.6497e-02,  1.9134e-02, -2.6175e-03,\n",
       "                       1.9376e-02,  1.6449e-02, -8.5098e-05, -2.6176e-02])),\n",
       "             ('bert.encoder.layer.5.output.dense.weight',\n",
       "              tensor([[ 1.5655e-02,  2.3280e-03, -4.5653e-02,  ..., -2.6772e-03,\n",
       "                       -4.6612e-03, -2.9817e-02],\n",
       "                      [ 5.2960e-02, -4.5857e-03,  2.3095e-02,  ..., -3.3756e-02,\n",
       "                        1.5616e-02, -2.8728e-02],\n",
       "                      [ 2.2073e-02, -8.7330e-03, -2.3445e-02,  ...,  4.3414e-02,\n",
       "                        1.8809e-02, -2.3539e-02],\n",
       "                      ...,\n",
       "                      [-2.2391e-03,  4.6266e-02,  3.0158e-02,  ...,  5.0692e-05,\n",
       "                       -6.8371e-03,  8.5381e-03],\n",
       "                      [-5.4946e-02,  1.2480e-02,  2.3095e-02,  ..., -3.0342e-02,\n",
       "                       -1.2586e-02,  1.1635e-02],\n",
       "                      [ 3.1851e-03, -1.9745e-02,  2.5970e-02,  ..., -3.4060e-02,\n",
       "                       -2.5241e-02, -2.9841e-02]])),\n",
       "             ('bert.encoder.layer.5.output.dense.bias',\n",
       "              tensor([-0.0024,  0.0081,  0.0127,  0.0137, -0.0008, -0.0025, -0.0134, -0.0114,\n",
       "                       0.0002, -0.0014,  0.0206,  0.0137, -0.0104,  0.0099,  0.0121, -0.0052,\n",
       "                      -0.0042,  0.0197, -0.0045,  0.0055,  0.0117, -0.0051, -0.0041,  0.0092,\n",
       "                       0.0011, -0.0070,  0.0085,  0.0261, -0.0048,  0.0087,  0.0152,  0.0019,\n",
       "                      -0.0113, -0.0084, -0.0125, -0.0082, -0.0114, -0.0070,  0.0038, -0.0018,\n",
       "                      -0.0021, -0.0056, -0.0002,  0.0152,  0.0049, -0.0082,  0.0045,  0.0006,\n",
       "                      -0.0013,  0.0008, -0.0105, -0.0037,  0.0048,  0.0029,  0.0068, -0.0060,\n",
       "                       0.0013, -0.0045,  0.0029, -0.0040, -0.0069, -0.0099,  0.0089, -0.0068,\n",
       "                      -0.0027,  0.0031, -0.0062,  0.0061,  0.0006,  0.0064,  0.0018, -0.0068,\n",
       "                       0.0008,  0.0174,  0.0037,  0.0165, -0.0004,  0.0052,  0.0029, -0.0021,\n",
       "                      -0.0024,  0.0126, -0.0007, -0.0129, -0.0013, -0.0009, -0.0113, -0.0076,\n",
       "                       0.0147,  0.0020, -0.0149,  0.0118, -0.0016,  0.0009, -0.0110,  0.0135,\n",
       "                      -0.0056, -0.0092,  0.0025, -0.0029, -0.0022,  0.0125, -0.0088,  0.0036,\n",
       "                       0.0007,  0.0055, -0.0139,  0.0094, -0.0089,  0.0078, -0.0044, -0.0001,\n",
       "                       0.0022, -0.0084,  0.0101,  0.0139, -0.0046, -0.0050,  0.0110, -0.0145,\n",
       "                       0.0050, -0.0165,  0.0017, -0.0068,  0.0036, -0.0057,  0.0118,  0.0078,\n",
       "                       0.0197, -0.0026,  0.0020,  0.0023,  0.0048, -0.0055,  0.0014,  0.0130,\n",
       "                      -0.0029,  0.0069,  0.0013,  0.0224, -0.0048,  0.0007,  0.0044, -0.0068,\n",
       "                      -0.0103, -0.0051,  0.0048,  0.0076,  0.0013, -0.0018, -0.0006,  0.0006,\n",
       "                      -0.0075, -0.0029, -0.0120, -0.0119, -0.0207, -0.0015,  0.0134, -0.0023,\n",
       "                      -0.0089,  0.0024,  0.0045, -0.0008, -0.0046,  0.0087, -0.0019,  0.0058,\n",
       "                       0.0103, -0.0048, -0.0082, -0.0108,  0.0012, -0.0069,  0.0082,  0.0012,\n",
       "                      -0.0072, -0.0206, -0.0040,  0.0062,  0.0042, -0.0061, -0.0093, -0.0111,\n",
       "                       0.0059,  0.0052, -0.0055,  0.0005,  0.0158,  0.0057, -0.0087,  0.0023])),\n",
       "             ('bert.encoder.layer.5.output.LayerNorm.weight',\n",
       "              tensor([1.0148, 1.0134, 0.9881, 1.0019, 1.0056, 0.9847, 1.0090, 1.0046, 0.9924,\n",
       "                      1.0079, 0.9599, 1.0272, 1.0181, 1.0046, 1.0020, 0.9991, 1.0090, 1.0037,\n",
       "                      0.9845, 1.0341, 1.0015, 1.0322, 0.9893, 1.0035, 0.9897, 1.0199, 1.0339,\n",
       "                      0.9725, 0.9525, 1.0228, 1.0234, 1.0067, 1.0069, 1.0216, 1.0189, 0.9883,\n",
       "                      0.9887, 0.9403, 1.0120, 1.0136, 1.0068, 1.0268, 1.0030, 1.0063, 1.0201,\n",
       "                      1.0021, 1.0015, 1.0191, 0.9994, 1.0175, 1.0150, 0.9408, 0.9219, 1.0094,\n",
       "                      1.0176, 1.0092, 1.0114, 1.0135, 1.0218, 0.9951, 1.0013, 0.9825, 1.0086,\n",
       "                      0.9900, 1.0140, 0.9968, 1.0208, 0.8932, 1.0120, 1.0000, 1.0028, 0.9810,\n",
       "                      1.0156, 1.0284, 0.9999, 0.9869, 1.0132, 1.0180, 1.0411, 1.0089, 1.0040,\n",
       "                      0.9931, 1.0037, 1.0190, 1.0326, 1.0077, 1.0411, 0.9955, 1.0172, 1.0161,\n",
       "                      0.9983, 0.9650, 1.0059, 0.9937, 0.9871, 0.9205, 1.0064, 0.9586, 0.9989,\n",
       "                      1.0160, 1.0038, 1.0195, 1.0152, 0.9903, 1.0131, 0.9953, 1.0180, 0.9906,\n",
       "                      1.0177, 1.0152, 0.9769, 1.0142, 1.0283, 0.9793, 0.8389, 1.0085, 0.9909,\n",
       "                      1.0100, 1.0094, 1.0253, 0.9765, 0.9380, 1.0067, 0.9990, 1.0191, 1.0282,\n",
       "                      1.0230, 1.0104, 1.0148, 1.0146, 0.9508, 0.9622, 0.9952, 0.9918, 1.0278,\n",
       "                      1.0075, 0.9752, 1.0058, 0.9777, 1.0122, 0.9986, 1.0146, 0.9990, 1.0174,\n",
       "                      0.9771, 0.9926, 0.9719, 1.0070, 1.0074, 1.0372, 1.0093, 0.9280, 0.9980,\n",
       "                      1.0188, 0.9911, 1.0169, 1.0174, 0.9938, 0.9996, 0.9340, 1.0020, 0.9969,\n",
       "                      1.0008, 1.0133, 0.9803, 1.0209, 1.0109, 1.0131, 1.0063, 1.0028, 0.9914,\n",
       "                      1.0176, 1.0043, 1.0227, 1.0147, 1.0116, 1.0059, 0.9419, 1.0064, 1.0035,\n",
       "                      0.8873, 1.0197, 0.9980, 1.0235, 1.0067, 1.0230, 0.9987, 1.0031, 0.9811,\n",
       "                      0.9709, 0.9868, 1.0102])),\n",
       "             ('bert.encoder.layer.5.output.LayerNorm.bias',\n",
       "              tensor([-3.5064e-03,  5.0593e-03,  1.0105e-02,  1.1171e-02,  5.7431e-03,\n",
       "                      -1.2978e-02, -1.4126e-02, -8.6369e-03,  7.5646e-03, -2.2991e-03,\n",
       "                       2.7482e-02,  1.4865e-02, -1.3521e-02,  1.7406e-02,  1.3019e-02,\n",
       "                      -6.0578e-04,  5.0781e-04,  2.2746e-02,  3.6414e-03,  8.1362e-04,\n",
       "                      -5.0253e-04, -4.2271e-03, -7.3829e-04,  1.1324e-03, -2.0299e-03,\n",
       "                      -1.1486e-02,  6.0449e-03,  1.6548e-02,  3.1432e-03,  1.3834e-02,\n",
       "                       2.4349e-02,  4.7871e-03, -9.2217e-03, -8.1972e-03, -1.3881e-02,\n",
       "                      -1.0566e-02, -9.3756e-03, -1.6019e-03, -3.6244e-03, -5.3568e-03,\n",
       "                       1.6140e-03, -1.4090e-02, -4.9957e-03,  1.5628e-02,  3.2869e-03,\n",
       "                      -3.9877e-03,  8.9488e-04,  3.0841e-03, -2.5541e-03, -7.9578e-04,\n",
       "                      -9.9349e-03,  8.0108e-03,  8.1441e-03, -6.7980e-04,  7.7107e-03,\n",
       "                      -1.4959e-02, -2.1993e-03, -7.5221e-03,  3.4918e-04, -3.1042e-03,\n",
       "                      -5.1831e-03, -7.2843e-03,  9.6634e-03, -5.7721e-03, -1.7573e-03,\n",
       "                      -2.7893e-04, -1.0349e-02, -2.6972e-03, -1.0776e-03, -4.8977e-03,\n",
       "                       5.3968e-03, -2.4185e-03, -9.6378e-03,  1.1767e-02, -4.4777e-04,\n",
       "                       1.2461e-02, -1.7693e-03,  8.8339e-03, -8.3559e-04, -1.8928e-03,\n",
       "                      -5.4076e-03,  1.4903e-02,  4.5380e-03, -1.3025e-02,  1.0760e-03,\n",
       "                      -6.8456e-03, -1.6779e-02, -3.2884e-03,  1.5463e-02,  4.4410e-04,\n",
       "                      -1.3256e-02,  3.4407e-03, -2.2221e-03,  3.5848e-03, -5.5888e-03,\n",
       "                       6.1890e-03, -1.7945e-03, -1.2664e-02,  2.4032e-03, -2.5602e-03,\n",
       "                      -6.7482e-03,  1.0534e-02, -2.4020e-03, -4.0260e-03, -3.5717e-03,\n",
       "                      -4.0169e-04, -1.5532e-02,  5.4057e-03, -1.2332e-02,  4.5198e-03,\n",
       "                      -2.3196e-03, -3.7155e-03,  5.2492e-03, -1.8219e-02,  2.8066e-02,\n",
       "                       1.4648e-02, -6.8002e-03, -2.5978e-03,  8.4137e-03, -1.6622e-02,\n",
       "                       8.5141e-03, -2.0638e-02, -4.3309e-03,  2.0213e-04,  2.6921e-03,\n",
       "                       2.1690e-03,  1.5065e-02, -3.4207e-03,  1.6688e-02, -5.4012e-03,\n",
       "                      -7.1793e-03,  4.1142e-03,  1.1403e-02, -8.5739e-03,  8.8576e-04,\n",
       "                       1.5333e-02,  4.8612e-03,  3.1955e-03, -8.9311e-04,  1.8853e-02,\n",
       "                      -5.0239e-03,  6.1536e-03,  4.7762e-03, -1.3490e-03,  3.0281e-04,\n",
       "                      -9.6360e-03,  1.0896e-03,  9.4090e-03,  1.2997e-03, -4.9884e-03,\n",
       "                       7.3778e-03, -1.2523e-02, -4.7256e-04, -2.0688e-03, -1.5546e-02,\n",
       "                      -7.7338e-03, -1.5389e-02, -9.7882e-03,  1.0007e-02,  4.7575e-03,\n",
       "                      -9.2471e-03,  9.2051e-03,  3.1379e-03, -5.6569e-03,  3.9771e-03,\n",
       "                       1.1279e-02, -2.4232e-03,  6.4256e-03,  1.0378e-02, -1.0884e-02,\n",
       "                       6.3331e-05, -1.9899e-02, -1.1023e-03, -3.7551e-03,  1.2739e-02,\n",
       "                      -4.8869e-03, -7.5361e-03, -8.3758e-03, -4.1507e-03,  5.0670e-05,\n",
       "                      -7.9912e-03, -7.7760e-03, -1.0840e-02, -9.0178e-03,  6.5914e-03,\n",
       "                       7.0370e-03, -6.4722e-03, -2.6799e-03,  3.2961e-03, -3.4687e-03,\n",
       "                      -2.8383e-03, -3.6958e-04])),\n",
       "             ('bert.pooler.dense.weight',\n",
       "              tensor([[ 0.0196,  0.0208, -0.0182,  ...,  0.0041, -0.0018,  0.0113],\n",
       "                      [-0.0268, -0.0331,  0.0017,  ...,  0.0227,  0.0427,  0.0503],\n",
       "                      [ 0.0172,  0.0089, -0.0352,  ..., -0.0110,  0.0135,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0061, -0.0231, -0.0179,  ..., -0.0439,  0.0483, -0.0164],\n",
       "                      [-0.0101, -0.0165, -0.0027,  ..., -0.0047,  0.0070,  0.0298],\n",
       "                      [ 0.0044,  0.0035, -0.0087,  ...,  0.0291, -0.0252, -0.0067]])),\n",
       "             ('bert.pooler.dense.bias',\n",
       "              tensor([ 5.8375e-03,  2.8746e-03, -6.2476e-03, -1.1664e-02, -2.5100e-03,\n",
       "                       1.2292e-03,  2.2210e-03,  7.0760e-03,  2.9990e-03,  8.8105e-03,\n",
       "                       4.1650e-03, -9.8019e-03, -8.4401e-03,  2.3563e-03, -2.7696e-04,\n",
       "                      -1.1673e-03,  8.4968e-03,  6.9579e-03,  7.0998e-03, -4.3943e-03,\n",
       "                       1.0980e-02, -1.7808e-03, -4.5994e-03,  6.7036e-03, -1.4195e-02,\n",
       "                       1.7770e-05, -4.9811e-03, -4.5800e-03,  1.1116e-02, -1.0104e-02,\n",
       "                      -5.0014e-04, -4.9827e-03, -1.9769e-03,  9.9974e-04, -1.3160e-03,\n",
       "                       2.0984e-03,  7.2314e-05,  2.2490e-03,  9.2767e-04, -3.2817e-03,\n",
       "                      -9.1148e-05,  3.8108e-04,  1.1911e-02,  7.4571e-03,  1.0682e-02,\n",
       "                      -1.2052e-02,  5.2199e-03, -1.0537e-02,  3.5229e-03,  1.4544e-03,\n",
       "                       5.5221e-03, -1.9194e-03,  7.5294e-03, -1.5210e-03, -5.8734e-03,\n",
       "                       5.2985e-03,  2.1591e-04, -3.2165e-03,  8.3366e-03, -1.0070e-02,\n",
       "                      -2.3711e-03, -1.9816e-03, -9.9825e-04,  2.1027e-03,  6.5407e-03,\n",
       "                       9.1098e-04, -9.2319e-03,  8.0912e-03,  8.1528e-03,  8.8372e-03,\n",
       "                       1.5232e-03,  1.0593e-02,  1.1115e-02,  8.6742e-03, -8.7393e-03,\n",
       "                      -8.1157e-03, -6.9797e-03, -3.9819e-04, -3.8310e-04,  1.1698e-02,\n",
       "                      -8.8463e-03,  7.8386e-03,  3.5181e-03,  8.3776e-03,  1.0638e-02,\n",
       "                       4.0390e-03,  8.3744e-03,  7.0782e-03, -1.6967e-03, -9.4236e-03,\n",
       "                      -3.5241e-03, -6.6552e-03, -5.7229e-03, -8.5699e-03, -2.1767e-03,\n",
       "                       7.3044e-03, -4.4493e-04, -1.0548e-02, -1.0382e-02,  7.1521e-03,\n",
       "                       1.1375e-02,  1.8898e-03,  6.2280e-03,  1.0896e-02, -2.7497e-03,\n",
       "                       6.9059e-03, -8.1004e-03,  3.7372e-03, -6.6883e-04,  7.6206e-03,\n",
       "                       1.0687e-02, -8.2146e-03,  4.2654e-03, -6.8405e-03,  9.1744e-03,\n",
       "                      -3.2075e-04,  1.0795e-02,  1.7196e-04,  6.1672e-03, -5.0441e-03,\n",
       "                      -5.0114e-03, -7.8530e-03, -7.3984e-03,  4.7339e-03, -9.7846e-03,\n",
       "                       7.1884e-03, -7.5079e-03,  5.1296e-04, -2.8740e-03,  1.0814e-02,\n",
       "                       2.8907e-03, -9.2711e-03, -3.2895e-03,  7.8665e-03,  4.4011e-03,\n",
       "                      -1.3838e-03,  1.2436e-03,  6.0072e-03,  7.7435e-03,  7.1162e-03,\n",
       "                       1.9681e-04, -2.7787e-03, -9.6359e-03, -5.1274e-03,  7.2051e-03,\n",
       "                       6.3816e-03, -6.9447e-03,  3.9199e-03,  8.7098e-03, -3.3579e-03,\n",
       "                       6.2362e-03,  4.8698e-03,  1.1400e-02,  1.0584e-02, -1.9225e-03,\n",
       "                      -9.1574e-03,  1.2531e-03,  1.3271e-03,  3.6703e-03, -9.5306e-04,\n",
       "                       8.3118e-03,  1.9835e-03, -8.3204e-03,  8.0669e-03, -1.7875e-03,\n",
       "                      -6.3975e-03, -1.3106e-03, -8.4176e-03,  6.5042e-03, -5.1665e-03,\n",
       "                      -7.2621e-04, -2.6879e-03,  1.5541e-03, -1.6089e-03,  7.2232e-03,\n",
       "                       3.4469e-04,  5.3864e-03,  1.7834e-03, -1.7098e-03, -3.2839e-03,\n",
       "                      -1.0943e-02,  4.0915e-03,  1.1770e-02,  1.0007e-03, -5.2735e-03,\n",
       "                       8.9759e-03, -6.4132e-03, -4.3968e-04, -1.2724e-02, -8.3822e-03,\n",
       "                       2.7959e-03,  9.6675e-03])),\n",
       "             ('cls.predictions.bias',\n",
       "              tensor([-1.1020, -0.0146, -0.0058,  ..., -0.0652, -0.0662, -0.0667])),\n",
       "             ('cls.predictions.transform.dense.weight',\n",
       "              tensor([[ 0.1659, -0.0080,  0.0047,  ...,  0.0076,  0.0010,  0.0132],\n",
       "                      [-0.0218,  0.1622,  0.0305,  ...,  0.0037,  0.0138,  0.0075],\n",
       "                      [-0.0227,  0.0110,  0.1169,  ...,  0.0106,  0.0115, -0.0269],\n",
       "                      ...,\n",
       "                      [-0.0234,  0.0055, -0.0010,  ...,  0.1126,  0.0110, -0.0065],\n",
       "                      [-0.0007, -0.0121,  0.0055,  ...,  0.0088,  0.1352,  0.0111],\n",
       "                      [ 0.0042,  0.0121,  0.0131,  ...,  0.0124,  0.0134,  0.1618]])),\n",
       "             ('cls.predictions.transform.dense.bias',\n",
       "              tensor([-2.3597e-03,  2.5548e-03,  1.1080e-03, -4.1874e-03, -1.0007e-03,\n",
       "                      -2.5120e-01,  2.8545e-03,  1.1980e-02,  7.4477e-03, -1.0117e-02,\n",
       "                      -6.6092e-03,  1.0262e-02, -5.3299e-03, -5.1444e-03, -8.1676e-03,\n",
       "                       7.9640e-04, -2.8799e-03, -2.0463e-01, -4.4928e-03,  2.2583e-03,\n",
       "                       7.4804e-04,  8.7750e-04, -5.3549e-03,  6.6997e-04,  4.4504e-03,\n",
       "                      -1.8706e-03, -3.1981e-03,  2.7873e-03, -2.0106e-03,  3.0274e-03,\n",
       "                      -5.8671e-03,  4.8301e-03,  4.3967e-03,  5.0626e-03,  3.4346e-04,\n",
       "                       4.1138e-04,  4.2554e-03,  6.5650e-03,  7.1145e-03, -2.9518e-03,\n",
       "                       2.3052e-03, -3.4155e-03,  2.6310e-03, -4.0001e-03, -6.2535e-03,\n",
       "                      -1.8001e-03,  3.5040e-03,  5.7080e-03,  5.9055e-03,  9.2052e-03,\n",
       "                       5.2957e-03,  1.1873e-02,  5.8112e-04,  4.4796e-03, -5.5418e-04,\n",
       "                       6.8351e-03, -7.3152e-03,  8.4599e-03, -1.4906e-03,  9.0089e-03,\n",
       "                      -6.2133e-04, -2.9987e-01, -2.6273e-03,  1.4948e-03, -2.7520e-03,\n",
       "                      -1.0760e-03, -9.1560e-03, -2.1621e-03,  8.1720e-04,  1.5701e-03,\n",
       "                       6.0318e-03,  9.2689e-03, -3.5561e-03, -4.9927e-03,  1.5151e-02,\n",
       "                       4.7063e-03, -5.7794e-03, -2.7101e-03,  3.1200e-03, -6.2623e-03,\n",
       "                       1.1747e-02, -6.7686e-03,  9.7233e-03,  4.6460e-03, -1.3930e-02,\n",
       "                       1.1980e-02,  1.7351e-03,  3.3273e-03, -1.2968e-02,  5.6194e-03,\n",
       "                       6.9499e-03,  3.5890e-03,  7.6850e-03,  4.3844e-03, -3.6810e-03,\n",
       "                      -7.0615e-03,  3.2039e-04, -8.4185e-03,  1.3863e-02, -1.8792e-03,\n",
       "                       7.3185e-03, -1.3371e-03,  3.1015e-03,  7.8295e-03,  9.2269e-03,\n",
       "                       3.5080e-03, -8.9045e-03, -2.7438e-01,  2.9849e-03, -3.4118e-03,\n",
       "                      -9.8282e-04, -1.7030e-03,  2.2888e-03,  1.0530e-02,  4.9347e-03,\n",
       "                       1.9850e-03, -2.7019e-03,  2.0229e-03, -8.2919e-03,  7.3864e-04,\n",
       "                       3.1317e-03, -1.6501e-03,  3.4725e-03, -9.0969e-03,  4.2031e-03,\n",
       "                      -9.3691e-03, -8.7240e-04, -5.6532e-03, -1.0939e-02, -1.5773e-03,\n",
       "                       9.8177e-03, -2.7540e-01, -6.1178e-03,  1.0379e-03,  3.2650e-03,\n",
       "                      -4.8662e-03,  7.2252e-03,  4.0480e-03, -5.2957e-04, -3.6481e-03,\n",
       "                       8.0589e-04,  1.8066e-03,  4.7732e-03, -1.4309e-03,  5.6458e-04,\n",
       "                       1.0759e-02, -5.5612e-03, -4.6232e-03,  2.7491e-03, -3.0676e-03,\n",
       "                      -1.2690e-02,  5.0495e-03,  4.6715e-03,  4.3054e-03,  2.1617e-04,\n",
       "                       2.8296e-03, -2.0601e-03, -3.7593e-03,  5.6215e-03,  8.7982e-03,\n",
       "                      -8.1243e-03,  7.9428e-03,  6.5062e-03, -1.7608e-03, -2.6728e-03,\n",
       "                      -1.8346e-03, -3.4372e-03, -3.1712e-03, -8.7783e-03, -4.2215e-03,\n",
       "                       6.3449e-03,  7.0591e-04,  1.0102e-02,  9.0939e-03,  1.2059e-02,\n",
       "                       9.8759e-03, -2.4047e-01, -7.5672e-03, -3.1050e-04, -1.5847e-03,\n",
       "                      -5.8528e-03,  8.8967e-03,  8.9736e-03, -5.5419e-03,  8.4191e-03,\n",
       "                      -6.2700e-03,  7.8061e-03,  4.9567e-03,  5.4176e-03, -7.0094e-03,\n",
       "                       1.9300e-03,  1.1080e-02])),\n",
       "             ('cls.predictions.transform.LayerNorm.weight',\n",
       "              tensor([2.2364, 2.2396, 2.3895, 2.2455, 2.2448, 1.7680, 2.1327, 2.0943, 2.1679,\n",
       "                      2.1416, 2.2598, 2.1600, 2.2408, 2.2136, 2.1156, 2.1609, 2.2368, 2.0375,\n",
       "                      2.2447, 2.2336, 2.4170, 2.2003, 2.2273, 2.1985, 2.1813, 2.2497, 2.2293,\n",
       "                      2.1647, 2.2122, 2.2412, 2.3017, 2.2055, 2.3201, 2.4163, 2.2197, 2.1954,\n",
       "                      2.1936, 2.0884, 2.2663, 2.1195, 2.1808, 2.2373, 2.2268, 2.1627, 2.3522,\n",
       "                      2.1973, 2.3677, 2.2258, 2.2371, 2.2537, 2.1589, 2.2778, 2.1474, 2.2985,\n",
       "                      2.4082, 2.2393, 2.1525, 2.2966, 2.2557, 2.2316, 2.2721, 1.6671, 2.1674,\n",
       "                      2.1035, 2.2319, 2.2645, 2.1452, 2.2252, 2.1502, 2.2035, 2.3087, 2.3332,\n",
       "                      2.2440, 2.2563, 2.1502, 2.1108, 2.1774, 2.2227, 2.1958, 2.2489, 2.2124,\n",
       "                      2.3126, 2.2097, 2.1993, 2.2974, 2.3019, 2.3206, 2.3179, 2.2457, 2.3763,\n",
       "                      2.1408, 2.1164, 2.2348, 2.2401, 2.3303, 2.2050, 2.2158, 2.2512, 2.1474,\n",
       "                      2.2332, 2.1733, 2.1803, 2.2039, 2.1725, 2.2425, 2.2711, 2.2517, 1.7274,\n",
       "                      2.2190, 2.2263, 2.3278, 2.1624, 2.1661, 2.2503, 2.1631, 2.2166, 2.2765,\n",
       "                      2.2231, 2.3315, 2.3006, 2.1846, 2.2287, 2.2330, 2.1363, 2.2963, 2.3556,\n",
       "                      2.2356, 2.2376, 2.2594, 2.2560, 2.1261, 1.7233, 2.2129, 2.4201, 2.2384,\n",
       "                      2.1205, 2.1454, 2.1524, 2.2674, 2.1912, 2.1332, 2.3933, 2.2213, 2.1958,\n",
       "                      2.2435, 2.3501, 2.1062, 2.1299, 2.1685, 2.2586, 2.2298, 2.2181, 2.2053,\n",
       "                      2.1999, 2.2939, 2.2561, 2.2357, 2.2276, 2.1592, 2.0914, 2.3138, 2.2633,\n",
       "                      2.1957, 2.2681, 2.2344, 2.0727, 2.2443, 2.2106, 2.1971, 2.3398, 2.1540,\n",
       "                      2.2781, 2.1645, 2.3768, 2.2250, 2.2092, 1.8959, 2.3756, 2.2195, 2.2348,\n",
       "                      2.3232, 2.1739, 2.3014, 2.1635, 2.1752, 2.1536, 2.1840, 2.1802, 2.1200,\n",
       "                      2.2986, 2.1610, 2.2371])),\n",
       "             ('cls.predictions.transform.LayerNorm.bias',\n",
       "              tensor([ 0.2777,  0.3413,  0.4607,  0.3043, -0.3174, -0.7037, -0.3682, -0.3437,\n",
       "                      -0.4141,  0.0560,  0.4981,  0.4859,  0.6137, -0.6453, -0.5515, -0.4018,\n",
       "                      -0.2512, -0.8815,  0.3157, -0.3554,  0.6655, -0.4051, -0.3244,  0.3451,\n",
       "                      -0.4930,  0.3510,  0.3891,  0.3974, -0.0635,  0.4074, -0.4640, -0.1869,\n",
       "                       0.5278, -0.0533,  0.1254, -0.4548, -0.4813,  0.3169, -0.2440,  0.3271,\n",
       "                      -0.4383, -0.3205, -0.5309, -0.4977,  0.3604, -0.2821,  0.5128, -0.3576,\n",
       "                      -0.3542, -0.2340,  0.2940, -0.3613,  0.3576,  0.4510, -0.0386, -0.3851,\n",
       "                      -0.3554, -0.2191,  0.4234,  0.4482,  0.5326, -0.6224, -0.3103, -0.2432,\n",
       "                       0.2878,  0.5074, -0.3563, -0.3498,  0.2701, -0.3384,  0.3962,  0.5350,\n",
       "                       0.4137,  0.2923, -0.3928, -0.4092, -0.3652, -0.3307,  0.2294, -0.3940,\n",
       "                       0.4648,  0.6121,  0.3596, -0.3528,  0.3177, -0.0997,  0.5156, -0.3311,\n",
       "                      -0.3522, -0.2116,  0.3378, -0.2857, -0.3324,  0.4769, -0.0265,  0.4419,\n",
       "                       0.4557, -0.3582, -0.4003,  0.2343,  0.3224, -0.4262, -0.4089, -0.3728,\n",
       "                      -0.4220,  0.4000,  0.2992, -0.6602, -0.3928,  0.2434,  0.6131, -0.3013,\n",
       "                      -0.3856, -0.3487,  0.4008, -0.3153,  0.2812, -0.0034,  0.5025,  0.4834,\n",
       "                      -0.3470,  0.3111,  0.3706, -0.4505,  0.2318,  0.1822, -0.0502,  0.3647,\n",
       "                       0.4295,  0.3915, -0.2601, -0.6456, -0.3616,  0.0572, -0.3276, -0.3348,\n",
       "                      -0.5326,  0.3531,  0.4756, -0.3951,  0.3187, -0.1048, -0.3983, -0.4244,\n",
       "                       0.4553,  0.4399, -0.3553, -0.4262, -0.2746, -0.2886, -0.3855, -0.4144,\n",
       "                      -0.3621, -0.4965,  0.4916, -0.2616, -0.4182,  0.4543, -0.4505, -0.3760,\n",
       "                      -0.2602,  0.5526, -0.3551, -0.4042,  0.4035, -0.4078,  0.4287, -0.3763,\n",
       "                       0.2891,  0.5225, -0.3185,  0.4410, -0.5797,  0.4321, -0.3314,  0.4135,\n",
       "                      -0.7614,  0.5840, -0.3686, -0.3687,  0.0742, -0.3392, -0.2083,  0.3650,\n",
       "                      -0.4166, -0.3880, -0.3832, -0.4243, -0.4219,  0.5533, -0.3635, -0.3504])),\n",
       "             ('cls.predictions.decoder.weight',\n",
       "              tensor([[-0.0011,  0.0004, -0.0045,  ...,  0.0159, -0.0101,  0.0027],\n",
       "                      [-0.0006,  0.0035,  0.0629,  ..., -0.0453,  0.0473, -0.0018],\n",
       "                      [ 0.0705,  0.0396,  0.0785,  ...,  0.0846,  0.0274, -0.0472],\n",
       "                      ...,\n",
       "                      [-0.0316, -0.0742, -0.0694,  ..., -0.0674,  0.0521,  0.0679],\n",
       "                      [-0.0442, -0.0457, -0.0513,  ..., -0.0837,  0.0522,  0.0241],\n",
       "                      [-0.0279, -0.0400, -0.0354,  ..., -0.0921,  0.0450,  0.0153]])),\n",
       "             ('cls.predictions.decoder.bias',\n",
       "              tensor([-1.1020, -0.0146, -0.0058,  ..., -0.0652, -0.0662, -0.0667])),\n",
       "             ('cls.seq_relationship.weight',\n",
       "              tensor([[ 0.0400,  0.0592, -0.0292, -0.0228,  0.0451,  0.0307,  0.0370,  0.0471,\n",
       "                        0.0530,  0.0191,  0.0493, -0.0544, -0.0181, -0.0080,  0.0280, -0.0176,\n",
       "                        0.0294,  0.0364,  0.0380, -0.0621,  0.0664, -0.0447, -0.0239,  0.0289,\n",
       "                       -0.0383, -0.0105, -0.0416, -0.0153,  0.0457, -0.0523,  0.0554, -0.0134,\n",
       "                        0.0142,  0.0292,  0.0032,  0.0211, -0.0583,  0.0283,  0.0453, -0.0649,\n",
       "                       -0.0709, -0.0647,  0.0204,  0.0356,  0.0208, -0.0366,  0.0266, -0.0267,\n",
       "                        0.0382,  0.0399,  0.0206, -0.0287,  0.0410,  0.0570, -0.0278,  0.0040,\n",
       "                        0.0624, -0.0267,  0.0083, -0.0277, -0.0276, -0.0013, -0.0539,  0.0099,\n",
       "                        0.0294, -0.0364, -0.0411,  0.0188,  0.0384,  0.0404,  0.0406,  0.0330,\n",
       "                        0.0319,  0.0652, -0.0373, -0.0181, -0.0156, -0.0285,  0.0163,  0.0409,\n",
       "                       -0.0491,  0.0409,  0.0431,  0.0390,  0.0452,  0.0333,  0.0161,  0.0412,\n",
       "                       -0.0189, -0.0453, -0.0026, -0.0518, -0.0485, -0.0599, -0.0283,  0.0411,\n",
       "                       -0.0426, -0.0317, -0.0285,  0.0373,  0.0172,  0.0369,  0.0658,  0.0121,\n",
       "                       -0.0054,  0.0104, -0.0184,  0.0039, -0.0320,  0.0274,  0.0269, -0.0428,\n",
       "                        0.0328, -0.0261,  0.0382, -0.0292,  0.0387, -0.0209,  0.0249, -0.0166,\n",
       "                       -0.0280, -0.0632, -0.0204,  0.0180, -0.0134,  0.0255, -0.0119, -0.0458,\n",
       "                       -0.0115,  0.0319,  0.0329, -0.0207, -0.0418,  0.0395,  0.0204, -0.0478,\n",
       "                       -0.0532,  0.0290,  0.0185, -0.0154, -0.0376, -0.0515, -0.0337,  0.0376,\n",
       "                        0.0465,  0.0413, -0.0335,  0.0280,  0.0261, -0.0655,  0.0286,  0.0522,\n",
       "                        0.0337,  0.0396, -0.0471, -0.0238,  0.0484, -0.0276,  0.0163,  0.0090,\n",
       "                        0.0213,  0.0192, -0.0297,  0.0463, -0.0367, -0.0278, -0.0122, -0.0131,\n",
       "                        0.0567, -0.0357, -0.0398, -0.0100,  0.0287, -0.0023,  0.0515, -0.0480,\n",
       "                        0.0361,  0.0610, -0.0098, -0.0176, -0.0035,  0.0318,  0.0428, -0.0315,\n",
       "                       -0.0785,  0.0364, -0.0423, -0.0377, -0.0473, -0.0350,  0.0347,  0.0320],\n",
       "                      [-0.0275, -0.0255,  0.0143,  0.0253, -0.0203, -0.0343, -0.0410, -0.0320,\n",
       "                       -0.0447, -0.0615,  0.0027,  0.0294,  0.0257, -0.0464, -0.0421,  0.0438,\n",
       "                       -0.0381, -0.0566, -0.0309,  0.0432, -0.0400,  0.0655,  0.0430, -0.0313,\n",
       "                        0.0286, -0.0524,  0.0200,  0.0549, -0.0570,  0.0282, -0.0594,  0.0324,\n",
       "                       -0.0440, -0.0550,  0.0416, -0.0323,  0.0305, -0.0364, -0.0223,  0.0473,\n",
       "                        0.0507,  0.0519, -0.0619, -0.0187, -0.0458,  0.0242, -0.0438,  0.0407,\n",
       "                       -0.0637, -0.0415, -0.0330,  0.0539, -0.0399, -0.0589,  0.0203, -0.0202,\n",
       "                       -0.0673,  0.0086, -0.0389,  0.0207,  0.0519, -0.0074,  0.0153, -0.0437,\n",
       "                       -0.0226,  0.0179,  0.0144, -0.0319, -0.0353, -0.0481, -0.0292, -0.0219,\n",
       "                       -0.0269, -0.0576,  0.0432,  0.0261,  0.0090,  0.0391, -0.0221, -0.0363,\n",
       "                        0.0385, -0.0677, -0.0443, -0.0414, -0.0436, -0.0062, -0.0261, -0.0180,\n",
       "                        0.0559,  0.0521,  0.0360,  0.0570,  0.0150,  0.0503,  0.0214,  0.0101,\n",
       "                        0.0340,  0.0321,  0.0233, -0.0247, -0.0484, -0.0365, -0.0130, -0.0388,\n",
       "                        0.0522, -0.0366,  0.0349, -0.0117,  0.0473, -0.0033, -0.0315,  0.0457,\n",
       "                       -0.0278,  0.0352, -0.0367,  0.0428, -0.0637,  0.0437, -0.0469,  0.0564,\n",
       "                        0.0287,  0.0095,  0.0559, -0.0347,  0.0182, -0.0317,  0.0294,  0.0436,\n",
       "                        0.0454, -0.0418, -0.0211,  0.0267,  0.0074, -0.0377, -0.0091,  0.0404,\n",
       "                        0.0335, -0.0303, -0.0521, -0.0493,  0.0289,  0.0251,  0.0205, -0.0161,\n",
       "                       -0.0419, -0.0111,  0.0509, -0.0138, -0.0170,  0.0175, -0.0356, -0.0616,\n",
       "                       -0.0496, -0.0204,  0.0398,  0.0446, -0.0275,  0.0531, -0.0373,  0.0667,\n",
       "                       -0.0126, -0.0408,  0.0240, -0.0322,  0.0448,  0.0394,  0.0558,  0.0279,\n",
       "                       -0.0130,  0.0428,  0.0476, -0.0007, -0.0525,  0.0605, -0.0094,  0.0558,\n",
       "                       -0.0087, -0.0252,  0.0115,  0.0591,  0.0411, -0.0545, -0.0542,  0.0269,\n",
       "                        0.0784, -0.0318,  0.0160,  0.0601,  0.0709,  0.0036, -0.0497, -0.0339]])),\n",
       "             ('cls.seq_relationship.bias', tensor([ 0.0022, -0.0022]))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(root_path + \"MedBERT Pretraining/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Packages\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertForSequenceClassification\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import transformers\n",
    "import time\n",
    "import tqdm\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from termcolor import colored\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle as pkl\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "%matplotlib inline\n",
    "use_cuda = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = pkl.load(\n",
    "    open(\n",
    "        root_path + \"data/finetuning_data_updated/finetuning_gentacimin_2.bencs.train\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    encoding=\"bytes\",\n",
    ")\n",
    "valid_f = pkl.load(\n",
    "    open(\n",
    "        root_path + \"data/finetuning_data_updated/finetuning_gentacimin_2.bencs.valid\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    encoding=\"bytes\",\n",
    ")\n",
    "test_f = pkl.load(\n",
    "    open(\n",
    "        root_path + \"data/finetuning_data_updated/finetuning_gentacimin_2.bencs.test\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    encoding=\"bytes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 12038\n",
      "length of validation dataset 1719\n",
      "length of testing dataset 3439\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of training dataset:\", len(train_f))\n",
    "print(\"length of validation dataset\", len(valid_f))\n",
    "print(\"length of testing dataset\", len(test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11046564,\n",
       " 1,\n",
       " [8931, 261, 1031, 192, 4026, 205, 11799, 397, 2391, 3107],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The version in this file is updated to dump pt_id for better visualization and results analysis\n",
    "\n",
    "# Below are key functions for  Data prepartion ,formating input data into features, and model defintion\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    Based on original BERT code: We use this class instead of `None` because treating `None` as padding\n",
    "    batches could cause silent errors.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_ids, input_mask, segment_ids, label_id, pt_id, is_real_example=True\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example\n",
    "        self.pt_id = pt_id\n",
    "\n",
    "\n",
    "def convert_EHRexamples_to_features(examples, max_seq_length):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        feature = convert_singleEHR_example(ex_index, example, max_seq_length)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "# This is the EHR version\n",
    "def convert_singleEHR_example(ex_index, example, max_seq_length):\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_id=0,\n",
    "            is_real_example=False,\n",
    "        )\n",
    "\n",
    "    input_ids = example[2]\n",
    "    segment_ids = example[3]\n",
    "    label_id = example[1]\n",
    "    pt_id = example[0]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # LR 5/13 Left Truncate longer sequence\n",
    "    while len(input_ids) > max_seq_length:\n",
    "        input_ids = input_ids[-max_seq_length:]\n",
    "        input_mask = input_mask[-max_seq_length:]\n",
    "        segment_ids = segment_ids[-max_seq_length:]\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    # TODO remove label_id\n",
    "    # feature = [input_ids, input_mask, segment_ids, label_id, pt_id, True]\n",
    "    # feature = [input_ids, input_mask, segment_ids, pt_id, True]\n",
    "    feature = {\"input_ids\": input_ids, \"input_mask\": input_mask, \"segment_ids\":\n",
    "    segment_ids, \"pt_id\": pt_id, \"a\": True}\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTdataEHR(Dataset):\n",
    "    # TODO add labels\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.features.items()}\n",
    "        item = {key: torch.tensor(val) for key, val in self.features[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 75\n",
    "\n",
    "train_labels = []\n",
    "for (ex_index, example) in enumerate(train_f):\n",
    "    label = example[1]\n",
    "    train_labels.append(label)\n",
    "test_labels = []\n",
    "for (ex_index, example) in enumerate(test_f):\n",
    "    label = example[1]\n",
    "    test_labels.append(label)\n",
    "valid_labels = []\n",
    "for (ex_index, example) in enumerate(valid_f):\n",
    "    label = example[1]\n",
    "    valid_labels.append(label)\n",
    "\n",
    "# Data Preparation\n",
    "train_features = convert_EHRexamples_to_features(train_f, MAX_SEQ_LENGTH)\n",
    "test_features = convert_EHRexamples_to_features(test_f, MAX_SEQ_LENGTH)\n",
    "valid_features = convert_EHRexamples_to_features(valid_f, MAX_SEQ_LENGTH)\n",
    "train_dataset = BERTdataEHR(train_features, train_labels)\n",
    "test_dataset = BERTdataEHR(test_features, test_labels)\n",
    "valid_dataset = BERTdataEHR(valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BERTdataEHR at 0x7feb7a18cdf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2022-12-15 23:13:33,930]\u001b[0m A new study created in memory with name: no-name-11f65776-8b36-4706-8d8c-c19ff042cf5d\u001b[0m\n",
      "Trial: {'learning_rate': 0.0019708459339504566, 'num_train_epochs': 26, 'seed': 38, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 26\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 78260\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78260' max='78260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78260/78260 11:58, Epoch 26/26]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.290593</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.353320</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.310189</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.314019</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>0.329224</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.262593</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.336501</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.351497</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.274063</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.292766</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.346278</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.261805</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.326103</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.309120</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.314074</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.275458</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.312855</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.272710</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.335118</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.298125</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.363411</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>0.380974</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.325736</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.269862</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.353814</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.302004</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.329471</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.362656</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.339700</td>\n",
       "      <td>0.316309</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>0.287068</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.317657</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.380262</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.328414</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.273963</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.274731</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.288601</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.302213</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.363743</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>0.339364</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.312101</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.343060</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>0.350581</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.275178</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.301830</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.301038</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.314888</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.265972</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.297517</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>0.301333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.301383</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.312566</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.316275</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.305236</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.286034</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.355564</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.319109</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.287312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.335305</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.311923</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.287465</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.278576</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.296566</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.307406</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>0.284890</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.294451</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.290712</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.297832</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.307691</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.300307</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.293673</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.304757</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.297094</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.310334</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.297413</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-36000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-37000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-37000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-37500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-37500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-37000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-38000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-38000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-38500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-38500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-38000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-39000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-39000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-39500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-39500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-39500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-39000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-40000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-40000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-40500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-40500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-40500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-40000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-41000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-41000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-41500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-41500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-41500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-41000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-42000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-42000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-42500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-42500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-42500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-42000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-43000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-43000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-43500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-43500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-43500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-43000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-44000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-44000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-44500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-44500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-44500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-44000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-45000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-45000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-45500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-45500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-45500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-45000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-46000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-46000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-46500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-46500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-46500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-46000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-47000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-47000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-47500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-47500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-47500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-47000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-48000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-48000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-48500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-48500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-48500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-48000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-49000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-49000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-49500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-49500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-49500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-49000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-50000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-50000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-50500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-50500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-50500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-50000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-51000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-51000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-51500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-51500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-51500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-51000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-52000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-52000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-51500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-52500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-52500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-52500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-52000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-53000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-53000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-53500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-53500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-53500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-53000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-54000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-54000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-54500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-54500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-54500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-54000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-55000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-55000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-55500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-55500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-55500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-55000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-56000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-56000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-56500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-56500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-56500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-56000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-57000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-57000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-57500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-57500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-57500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-57000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-58000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-58000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-58500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-58500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-58500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-58000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-59000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-59000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-59500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-59500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-59500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-59000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-60000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-60000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-60500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-60500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-60500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-60000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-61000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-61000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-60500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-61500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-61500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-61500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-61000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-62000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-62000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-62500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-62500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-62500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-62000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-63000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-63000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-63500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-63500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-63500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-63000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-64000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-64000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-64500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-64500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-64500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-64000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-65000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-65000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-65500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-65500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-65500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-65000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-66000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-66000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-66500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-66500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-66500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-66000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-67000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-67000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-67500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-67500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-67500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-67000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-68000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-68000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-67500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-68500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-68500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-68500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-68000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-69000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-69000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-69000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-69500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-69500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-69500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-69000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-70000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-70000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-70500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-70500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-70500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-71000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-71000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-71000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-71500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-71500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-71500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-71000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-72000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-72000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-72000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-72500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-72500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-72500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-72000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-73000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-73000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-73000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-73500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-73500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-73500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-73000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-74000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-74000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-74000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-74500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-74500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-74500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-74000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-75000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-75000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-75000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-74500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-75500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-75500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-75500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-75000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-76000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-76000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-76000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-76500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-76500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-76500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-76000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-77000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-77000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-77000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-77500\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-77500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-77500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-77000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-0/checkpoint-78000\n",
      "Configuration saved in Gentacimin_finetuning/run-0/checkpoint-78000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-0/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-0/checkpoint-77500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-15 23:25:33,288]\u001b[0m Trial 0 finished with value: 0.29741349816322327 and parameters: {'learning_rate': 0.0019708459339504566, 'num_train_epochs': 26, 'seed': 38, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 0.29741349816322327.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0012283677661165016, 'num_train_epochs': 9, 'seed': 37, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13545\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13545' max='13545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13545/13545 02:06, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.268349</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.270557</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.283066</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.267615</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.262584</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>0.261916</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.263832</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.276920</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.276631</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.262623</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.269747</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.269922</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.264821</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-1/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-1/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-1/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-1/checkpoint-13000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-15 23:27:40,238]\u001b[0m Trial 1 finished with value: 0.26482075452804565 and parameters: {'learning_rate': 0.0012283677661165016, 'num_train_epochs': 9, 'seed': 37, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 0.26482075452804565.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001174682417758682, 'num_train_epochs': 86, 'seed': 22, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 86\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 129430\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129430' max='129430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129430/129430 20:02, Epoch 86/86]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.264894</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.274792</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.297901</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261962</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.321054</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.275364</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.283212</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>0.274514</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.270975</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.308230</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.270062</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.279822</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.285378</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.269460</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.267871</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.270257</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.261138</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.261758</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.261626</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.286759</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>0.264066</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.268859</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.314700</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.282081</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.291696</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.261094</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.285003</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.261020</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.307269</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.262986</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.260794</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.262235</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.279386</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.262673</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.262052</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.269181</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.262837</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.267090</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.278025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.283440</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.278192</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.295035</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.261903</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.262037</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.274830</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.262227</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.274920</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.264405</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.286500</td>\n",
       "      <td>0.281361</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.286809</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.297646</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.260752</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.268994</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.261676</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.267463</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>0.271625</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.268680</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.284835</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.270720</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.272579</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.275909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.278600</td>\n",
       "      <td>0.271045</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.269052</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.269337</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>0.262138</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.263356</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.262394</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.275742</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.295268</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.280967</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.295466</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.261505</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.279799</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.261527</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.283411</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.266732</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.267511</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.286124</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.261038</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.264006</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.260735</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.263943</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.280483</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.282342</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.264487</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.265552</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.256300</td>\n",
       "      <td>0.282314</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.278959</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.273343</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.261792</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.275765</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.272034</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.260536</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.265886</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.278736</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.272801</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.264878</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.274869</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.265022</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.271541</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.267032</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.260537</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.280667</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.263879</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.275426</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.275009</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.273341</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.260824</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.263191</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.266888</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.278937</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.286400</td>\n",
       "      <td>0.267681</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.262722</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.262488</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.266870</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.268685</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.269015</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.271062</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.265421</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.267131</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.267535</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.267220</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.267831</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-36000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-37000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-37000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-37500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-37500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-37000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-38000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-38000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-38500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-38500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-38000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-39000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-39000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-39500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-39500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-39500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-39000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-40000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-40000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-40500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-40500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-40500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-40000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-41000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-41000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-41500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-41500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-41500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-41000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-42000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-42000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-42500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-42500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-42500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-42000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-43000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-43000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-43500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-43500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-43500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-43000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-44000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-44000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-44500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-44500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-44500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-44000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-45000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-45000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-45500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-45500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-45500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-45000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-46000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-46000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-46500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-46500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-46500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-46000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-47000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-47000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-47500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-47500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-47500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-47000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-48000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-48000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-48500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-48500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-48500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-48000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-49000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-49000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-49500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-49500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-49500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-49000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-50000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-50000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-50500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-50500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-50500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-50000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-51000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-51000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-51500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-51500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-51500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-51000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-52000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-52000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-51500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-52500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-52500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-52500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-52000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-53000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-53000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-53500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-53500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-53500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-53000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-54000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-54000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-54500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-54500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-54500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-54000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-55000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-55000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-55500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-55500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-55500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-55000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-56000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-56000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-56500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-56500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-56500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-56000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-57000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-57000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-57500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-57500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-57500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-57000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-58000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-58000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-58500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-58500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-58500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-58000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-59000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-59000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-59500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-59500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-59500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-59000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-60000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-60000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-60500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-60500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-60500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-60000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-61000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-61000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-60500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-61500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-61500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-61500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-61000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-62000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-62000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-62500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-62500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-62500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-62000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-63000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-63000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-63500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-63500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-63500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-63000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-64000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-64000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-64500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-64500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-64500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-64000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-65000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-65000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-65500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-65500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-65500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-65000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-66000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-66000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-66500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-66500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-66500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-66000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-67000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-67000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-67500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-67500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-67500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-67000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-68000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-68000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-67500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-68500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-68500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-68500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-68000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-69000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-69000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-69000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-69500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-69500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-69500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-69000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-70000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-70000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-70500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-70500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-70500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-71000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-71000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-71000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-71500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-71500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-71500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-71000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-72000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-72000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-72000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-72500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-72500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-72500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-72000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-73000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-73000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-73000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-73500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-73500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-73500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-73000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-74000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-74000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-74000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-74500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-74500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-74500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-74000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-75000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-75000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-75000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-74500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-75500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-75500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-75500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-75000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-76000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-76000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-76000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-76500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-76500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-76500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-76000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-77000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-77000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-77000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-77500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-77500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-77500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-77000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-78000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-78000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-78500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-78500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-78500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-78000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-79000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-79000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-79000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-79500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-79500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-79500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-79000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-80000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-80000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-80500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-80500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-80500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-80000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-81000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-81000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-81000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-81500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-81500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-81500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-81000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-82000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-82000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-82000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-81500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-82500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-82500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-82500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-82000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-83000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-83000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-83000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-83500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-83500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-83500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-83000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-84000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-84000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-84000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-83500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-84500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-84500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-84500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-84000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-85000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-85000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-85000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-84500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-85500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-85500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-85500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-85000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-86000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-86000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-86000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-86500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-86500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-86500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-86000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-87000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-87000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-87000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-86500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-87500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-87500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-87500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-87000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-88000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-88000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-88000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-87500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-88500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-88500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-88500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-88000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-89000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-89000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-89000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-88500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-89500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-89500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-89500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-89000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-90000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-90000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-89500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-90500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-90500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-90500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-90000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-91000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-91000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-91000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-90500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-91500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-91500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-91500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-91000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-92000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-92000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-92000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-91500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-92500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-92500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-92500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-92000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-93000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-93000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-93000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-92500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-93500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-93500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-93500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-93000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-94000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-94000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-94000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-93500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-94500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-94500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-94500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-94000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-95000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-95000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-95000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-94500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-95500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-95500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-95500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-95000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-96000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-96000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-96000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-95500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-96500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-96500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-96500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-96000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-97000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-97000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-97000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-96500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-97500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-97500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-97500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-97000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-98000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-98000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-98000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-97500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-98500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-98500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-98500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-98000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-99000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-99000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-99000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-98500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-99500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-99500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-99500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-99000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-100000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-100000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-100000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-99500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-100500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-100500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-100500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-100000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-101000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-101000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-101000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-100500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-101500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-101500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-101500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-101000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-102000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-102000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-102000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-101500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-102500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-102500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-102500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-102000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-103000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-103000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-103000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-102500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-103500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-103500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-103500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-103000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-104000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-104000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-104000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-103500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-104500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-104500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-104500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-104000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-105000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-105000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-105000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-104500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-105500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-105500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-105500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-105000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-106000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-106000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-106000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-105500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-106500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-106500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-106500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-106000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-107000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-107000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-107000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-106500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-107500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-107500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-107500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-107000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-108000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-108000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-108000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-107500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-108500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-108500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-108500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-108000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-109000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-109000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-109000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-108500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-109500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-109500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-109500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-109000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-110000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-110000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-109500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-110500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-110500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-110500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-110000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-111000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-111000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-111000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-110500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-111500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-111500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-111500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-111000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-112000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-112000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-112000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-111500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-112500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-112500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-112500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-112000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-113000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-113000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-113000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-112500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-113500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-113500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-113500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-113000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-114000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-114000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-114000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-113500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-114500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-114500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-114500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-114000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-115000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-115000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-115000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-114500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-115500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-115500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-115500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-115000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-116000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-116000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-116000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-115500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-116500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-116500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-116500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-116000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-117000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-117000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-117000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-116500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-117500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-117500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-117500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-117000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-118000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-118000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-118000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-117500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-118500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-118500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-118500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-118000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-119000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-119000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-119000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-118500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-119500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-119500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-119500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-119000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-120000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-120000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-120000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-119500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-120500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-120500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-120500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-120000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-121000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-121000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-121000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-120500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-121500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-121500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-121500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-121000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-122000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-122000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-122000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-121500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-122500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-122500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-122500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-122000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-123000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-123000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-123000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-122500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-123500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-123500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-123500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-123000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-124000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-124000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-124000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-123500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-124500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-124500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-124500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-124000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-125000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-125000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-125000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-124500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-125500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-125500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-125500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-125000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-126000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-126000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-126000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-125500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-126500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-126500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-126500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-126000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-127000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-127000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-127000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-126500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-127500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-127500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-127500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-127000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-128000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-128000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-128000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-127500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-128500\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-128500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-128500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-128000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-2/checkpoint-129000\n",
      "Configuration saved in Gentacimin_finetuning/run-2/checkpoint-129000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-2/checkpoint-129000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-2/checkpoint-128500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-15 23:47:42,977]\u001b[0m Trial 2 finished with value: 0.26783108711242676 and parameters: {'learning_rate': 0.001174682417758682, 'num_train_epochs': 86, 'seed': 22, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 0.26482075452804565.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0012767907243067115, 'num_train_epochs': 13, 'seed': 31, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 13\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39130\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39130' max='39130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39130/39130 06:17, Epoch 13/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.288222</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.329369</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.322888</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.313574</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.365458</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.336500</td>\n",
       "      <td>0.264869</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.316971</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.310308</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.310100</td>\n",
       "      <td>0.314173</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.310308</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.312025</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.264561</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.291867</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.274513</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.293164</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.284027</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.307719</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.321191</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.285206</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.322336</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.275618</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.318942</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.329363</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.280415</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.290575</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.319689</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.325492</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.290994</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.316521</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.296165</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.291355</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.296331</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.301335</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.304083</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.303794</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>0.294893</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.298817</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.301364</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-36000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-37000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-37000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-37500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-37500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-37000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-38000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-38000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-38500\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-38500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-38000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-3/checkpoint-39000\n",
      "Configuration saved in Gentacimin_finetuning/run-3/checkpoint-39000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-3/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-3/checkpoint-38500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-15 23:54:01,045]\u001b[0m Trial 3 finished with value: 0.30136382579803467 and parameters: {'learning_rate': 0.0012767907243067115, 'num_train_epochs': 13, 'seed': 31, 'per_device_train_batch_size': 4}. Best is trial 1 with value: 0.26482075452804565.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001349960761604314, 'num_train_epochs': 72, 'seed': 32, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 72\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 216720\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216720' max='216720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216720/216720 32:59, Epoch 72/72]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.275982</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.326728</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.270114</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.293768</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.352700</td>\n",
       "      <td>0.298673</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.344633</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.332303</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.295357</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.298045</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.377039</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.381689</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.323767</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.281707</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.310957</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.300809</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.273836</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.317312</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.262319</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.325323</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.298185</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.274537</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.294060</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.327541</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.326174</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.265886</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.348096</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.299135</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.317703</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.321920</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.302621</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.364043</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.367902</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.326300</td>\n",
       "      <td>0.348241</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.345833</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.284635</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.292741</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.329578</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.295887</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.316635</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.305876</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.288124</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.382294</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.304345</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.330555</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.265294</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.273776</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.266166</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>0.297539</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.338079</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.327827</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.290431</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.270170</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.294129</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>0.311545</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.301323</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.264172</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.267880</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.294212</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.316400</td>\n",
       "      <td>0.319057</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.293155</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.343916</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.303400</td>\n",
       "      <td>0.311723</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.279129</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.294723</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.266431</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.324866</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.319657</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.355892</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.322471</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.359847</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.321163</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.323452</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.274850</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.309580</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.268801</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.358512</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.280449</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.308311</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.281697</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.265310</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.330416</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.317148</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.325506</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.293450</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.309084</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.279406</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.294299</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.315538</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>0.332322</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.303046</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.295430</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.340277</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.268553</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.282881</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.324025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.287165</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.279753</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.357825</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.283129</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.311135</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.321222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.312024</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.279670</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.342500</td>\n",
       "      <td>0.275150</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.301267</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.292185</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.295557</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.306611</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.298489</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.291755</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.289180</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.270913</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.336236</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>0.317253</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.307738</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.351426</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>0.310198</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.287071</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.303964</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.307117</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.284714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.275658</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.304602</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.304060</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>0.336242</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.310120</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.315133</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.281730</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.298021</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.314178</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.328500</td>\n",
       "      <td>0.276577</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.317631</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.283408</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.345142</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.289478</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.317345</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.309069</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.321969</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.318590</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.289948</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.334596</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.311468</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.298111</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.307208</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>0.292136</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.314076</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.296412</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.296064</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.303419</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.321887</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.316851</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.311494</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.312434</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.274041</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.279198</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.304288</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.319054</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.330031</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.301825</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.340175</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.309749</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.309816</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.308014</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.325825</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.314602</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.281670</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.297222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.321956</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.297399</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181000</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.326364</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.314051</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183000</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.290197</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184000</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.302204</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.313432</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186000</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.325947</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187000</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.328582</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188000</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.292556</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189000</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.284855</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.285678</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191000</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.303668</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193000</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.285177</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194000</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.285227</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196000</td>\n",
       "      <td>0.287100</td>\n",
       "      <td>0.307563</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197000</td>\n",
       "      <td>0.322900</td>\n",
       "      <td>0.301493</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198000</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.296074</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199000</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.282792</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>0.306132</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201000</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.301602</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202000</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.296523</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203000</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.309736</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204000</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.303722</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>0.286500</td>\n",
       "      <td>0.308105</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.296379</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207000</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>0.287478</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208000</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.293908</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.309606</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.301403</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211000</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.293592</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212000</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.307910</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213000</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>0.295838</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214000</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.296648</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.302255</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.298620</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-36000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-37000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-37000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-37500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-37500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-37000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-38000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-38000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-38500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-38500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-38000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-39000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-39000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-39500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-39500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-39500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-39000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-40000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-40000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-40500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-40500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-40500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-40000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-41000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-41000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-41500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-41500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-41500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-41000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-42000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-42000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-42500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-42500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-42500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-42000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-43000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-43000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-43500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-43500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-43500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-43000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-44000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-44000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-44500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-44500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-44500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-44000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-45000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-45000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-45500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-45500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-45500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-45000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-46000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-46000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-46500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-46500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-46500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-46000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-47000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-47000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-47500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-47500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-47500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-47000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-48000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-48000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-48500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-48500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-48500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-48000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-49000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-49000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-49500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-49500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-49500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-49000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-50000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-50000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-50500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-50500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-50500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-50000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-51000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-51000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-51500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-51500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-51500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-51000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-52000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-52000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-51500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-52500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-52500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-52500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-52000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-53000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-53000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-53500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-53500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-53500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-53000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-54000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-54000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-54500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-54500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-54500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-54000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-55000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-55000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-55500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-55500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-55500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-55000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-56000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-56000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-56500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-56500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-56500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-56000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-57000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-57000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-57500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-57500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-57500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-57000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-58000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-58000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-58500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-58500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-58500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-58000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-59000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-59000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-59500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-59500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-59500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-59000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-60000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-60000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-60500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-60500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-60500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-60000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-61000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-61000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-60500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-61500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-61500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-61500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-61000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-62000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-62000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-62500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-62500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-62500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-62000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-63000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-63000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-63500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-63500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-63500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-63000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-64000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-64000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-64500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-64500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-64500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-64000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-65000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-65000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-65500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-65500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-65500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-65000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-66000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-66000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-66500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-66500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-66500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-66000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-67000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-67000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-67500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-67500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-67500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-67000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-68000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-68000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-67500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-68500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-68500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-68500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-68000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-69000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-69000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-69000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-69500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-69500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-69500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-69000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-70000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-70000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-70500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-70500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-70500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-71000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-71000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-71000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-71500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-71500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-71500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-71000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-72000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-72000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-72000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-72500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-72500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-72500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-72000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-73000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-73000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-73000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-73500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-73500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-73500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-73000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-74000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-74000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-74000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-74500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-74500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-74500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-74000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-75000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-75000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-75000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-74500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-75500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-75500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-75500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-75000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-76000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-76000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-76000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-76500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-76500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-76500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-76000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-77000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-77000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-77000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-77500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-77500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-77500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-77000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-78000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-78000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-78500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-78500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-78500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-78000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-79000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-79000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-79000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-79500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-79500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-79500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-79000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-80000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-80000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-80500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-80500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-80500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-80000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-81000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-81000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-81000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-81500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-81500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-81500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-81000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-82000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-82000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-82000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-81500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-82500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-82500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-82500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-82000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-83000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-83000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-83000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-83500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-83500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-83500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-83000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-84000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-84000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-84000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-83500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-84500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-84500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-84500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-84000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-85000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-85000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-85000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-84500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-85500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-85500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-85500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-85000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-86000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-86000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-86000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-86500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-86500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-86500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-86000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-87000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-87000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-87000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-86500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-87500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-87500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-87500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-87000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-88000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-88000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-88000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-87500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-88500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-88500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-88500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-88000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-89000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-89000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-89000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-88500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-89500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-89500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-89500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-89000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-90000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-90000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-89500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-90500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-90500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-90500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-90000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-91000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-91000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-91000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-90500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-91500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-91500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-91500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-91000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-92000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-92000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-92000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-91500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-92500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-92500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-92500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-92000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-93000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-93000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-93000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-92500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-93500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-93500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-93500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-93000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-94000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-94000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-94000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-93500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-94500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-94500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-94500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-94000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-95000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-95000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-95000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-94500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-95500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-95500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-95500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-95000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-96000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-96000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-96000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-95500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-96500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-96500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-96500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-96000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-97000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-97000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-97000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-96500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-97500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-97500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-97500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-97000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-98000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-98000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-98000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-97500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-98500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-98500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-98500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-98000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-99000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-99000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-99000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-98500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-99500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-99500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-99500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-99000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-100000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-100000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-100000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-99500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-100500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-100500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-100500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-100000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-101000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-101000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-101000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-100500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-101500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-101500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-101500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-101000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-102000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-102000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-102000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-101500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-102500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-102500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-102500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-102000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-103000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-103000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-103000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-102500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-103500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-103500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-103500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-103000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-104000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-104000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-104000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-103500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-104500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-104500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-104500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-104000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-105000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-105000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-105000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-104500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-105500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-105500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-105500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-105000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-106000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-106000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-106000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-105500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-106500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-106500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-106500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-106000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-107000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-107000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-107000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-106500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-107500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-107500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-107500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-107000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-108000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-108000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-108000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-107500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-108500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-108500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-108500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-108000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-109000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-109000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-109000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-108500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-109500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-109500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-109500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-109000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-110000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-110000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-109500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-110500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-110500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-110500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-110000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-111000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-111000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-111000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-110500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-111500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-111500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-111500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-111000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-112000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-112000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-112000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-111500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-112500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-112500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-112500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-112000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-113000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-113000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-113000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-112500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-113500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-113500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-113500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-113000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-114000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-114000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-114000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-113500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-114500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-114500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-114500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-114000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-115000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-115000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-115000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-114500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-115500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-115500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-115500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-115000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-116000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-116000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-116000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-115500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-116500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-116500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-116500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-116000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-117000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-117000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-117000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-116500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-117500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-117500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-117500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-117000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-118000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-118000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-118000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-117500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-118500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-118500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-118500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-118000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-119000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-119000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-119000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-118500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-119500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-119500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-119500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-119000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-120000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-120000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-120000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-119500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-120500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-120500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-120500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-120000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-121000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-121000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-121000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-120500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-121500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-121500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-121500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-121000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-122000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-122000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-122000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-121500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-122500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-122500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-122500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-122000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-123000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-123000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-123000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-122500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-123500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-123500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-123500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-123000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-124000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-124000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-124000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-123500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-124500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-124500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-124500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-124000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-125000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-125000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-125000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-124500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-125500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-125500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-125500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-125000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-126000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-126000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-126000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-125500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-126500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-126500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-126500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-126000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-127000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-127000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-127000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-126500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-127500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-127500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-127500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-127000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-128000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-128000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-128000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-127500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-128500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-128500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-128500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-128000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-129000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-129000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-129000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-128500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-129500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-129500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-129500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-129000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-130000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-130000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-130000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-129500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-130500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-130500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-130500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-130000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-131000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-131000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-131000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-130500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-131500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-131500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-131500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-131000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-132000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-132000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-132000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-131500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-132500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-132500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-132500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-132000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-133000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-133000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-133000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-132500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-133500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-133500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-133500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-133000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-134000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-134000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-134000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-133500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-134500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-134500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-134500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-134000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-135000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-135000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-135000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-134500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-135500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-135500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-135500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-135000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-136000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-136000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-136000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-135500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-136500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-136500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-136500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-136000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-137000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-137000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-137000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-136500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-137500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-137500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-137500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-137000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-138000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-138000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-138000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-137500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-138500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-138500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-138500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-138000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-139000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-139000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-139000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-138500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-139500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-139500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-139500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-139000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-140000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-140000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-140000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-139500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-140500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-140500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-140500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-140000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-141000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-141000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-141000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-140500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-141500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-141500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-141500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-141000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-142000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-142000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-142000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-141500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-142500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-142500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-142500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-142000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-143000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-143000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-143000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-142500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-143500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-143500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-143500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-143000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-144000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-144000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-144000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-143500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-144500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-144500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-144500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-144000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-145000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-145000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-145000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-144500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-145500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-145500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-145500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-145000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-146000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-146000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-146000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-145500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-146500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-146500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-146500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-146000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-147000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-147000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-147000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-146500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-147500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-147500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-147500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-147000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-148000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-148000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-148000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-147500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-148500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-148500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-148500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-148000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-149000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-149000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-149000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-148500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-149500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-149500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-149500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-149000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-150000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-150000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-150000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-149500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-150500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-150500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-150500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-150000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-151000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-151000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-151000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-150500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-151500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-151500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-151500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-151000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-152000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-152000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-152000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-151500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-152500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-152500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-152500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-152000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-153000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-153000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-153000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-152500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-153500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-153500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-153500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-153000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-154000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-154000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-154000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-153500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-154500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-154500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-154500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-154000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-155000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-155000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-155000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-154500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-155500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-155500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-155500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-155000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-156000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-156000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-156000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-155500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-156500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-156500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-156500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-156000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-157000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-157000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-157000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-156500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-157500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-157500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-157500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-157000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-158000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-158000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-158000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-157500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-158500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-158500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-158500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-158000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-159000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-159000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-159000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-158500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-159500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-159500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-159500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-159000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-160000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-160000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-160000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-159500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-160500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-160500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-160500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-160000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-161000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-161000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-161000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-160500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-161500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-161500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-161500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-161000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-162000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-162000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-162000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-161500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-162500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-162500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-162500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-162000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-163000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-163000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-163000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-162500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-163500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-163500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-163500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-163000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-164000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-164000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-164000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-163500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-164500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-164500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-164500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-164000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-165000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-165000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-165000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-164500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-165500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-165500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-165500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-165000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-166000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-166000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-166000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-165500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-166500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-166500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-166500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-166000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-167000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-167000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-167000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-166500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-167500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-167500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-167500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-167000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-168000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-168000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-168000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-167500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-168500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-168500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-168500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-168000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-169000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-169000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-169000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-168500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-169500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-169500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-169500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-169000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-170000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-170000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-170000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-169500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-170500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-170500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-170500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-170000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-171000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-171000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-171000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-170500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-171500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-171500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-171500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-171000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-172000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-172000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-172000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-171500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-172500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-172500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-172500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-172000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-173000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-173000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-173000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-172500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-173500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-173500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-173500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-173000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-174000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-174000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-174000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-173500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-174500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-174500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-174500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-174000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-175000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-175000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-175000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-174500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-175500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-175500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-175500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-175000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-176000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-176000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-176000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-175500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-176500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-176500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-176500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-176000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-177000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-177000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-177000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-176500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-177500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-177500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-177500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-177000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-178000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-178000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-178000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-177500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-178500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-178500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-178500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-178000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-179000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-179000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-179000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-178500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-179500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-179500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-179500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-179000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-180000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-180000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-180000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-179500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-180500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-180500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-180500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-180000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-181000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-181000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-181000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-180500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-181500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-181500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-181500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-181000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-182000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-182000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-182000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-181500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-182500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-182500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-182500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-182000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-183000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-183000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-183000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-182500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-183500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-183500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-183500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-183000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-184000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-184000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-184000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-183500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-184500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-184500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-184500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-184000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-185000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-185000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-185000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-184500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-185500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-185500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-185500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-185000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-186000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-186000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-186000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-185500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-186500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-186500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-186500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-186000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-187000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-187000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-187000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-186500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-187500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-187500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-187500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-187000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-188000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-188000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-188000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-187500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-188500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-188500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-188500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-188000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-189000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-189000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-189000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-188500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-189500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-189500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-189500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-189000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-190000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-190000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-190000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-189500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-190500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-190500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-190500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-190000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-191000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-191000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-191000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-190500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-191500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-191500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-191500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-191000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-192000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-192000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-192000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-191500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-192500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-192500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-192500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-192000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-193000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-193000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-193000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-192500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-193500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-193500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-193500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-193000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-194000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-194000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-194000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-193500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-194500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-194500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-194500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-194000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-195000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-195000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-195000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-194500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-195500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-195500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-195500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-195000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-196000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-196000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-196000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-195500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-196500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-196500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-196500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-196000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-197000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-197000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-197000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-196500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-197500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-197500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-197500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-197000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-198000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-198000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-198000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-197500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-198500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-198500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-198500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-198000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-199000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-199000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-199000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-198500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-199500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-199500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-199500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-199000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-200000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-200000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-200000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-199500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-200500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-200500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-200500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-200000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-201000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-201000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-201000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-200500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-201500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-201500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-201500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-201000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-202000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-202000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-202000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-201500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-202500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-202500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-202500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-202000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-203000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-203000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-203000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-202500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-203500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-203500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-203500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-203000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-204000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-204000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-204000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-203500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-204500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-204500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-204500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-204000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-205000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-205000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-205000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-204500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-205500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-205500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-205500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-205000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-206000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-206000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-206000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-205500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-206500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-206500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-206500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-206000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-207000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-207000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-207000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-206500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-207500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-207500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-207500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-207000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-208000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-208000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-208000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-207500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-208500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-208500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-208500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-208000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-209000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-209000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-209000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-208500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-209500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-209500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-209500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-209000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-210000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-210000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-210000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-209500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-210500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-210500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-210500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-210000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-211000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-211000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-211000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-210500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-211500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-211500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-211500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-211000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-212000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-212000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-212000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-211500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-212500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-212500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-212500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-212000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-213000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-213000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-213000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-212500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-213500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-213500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-213500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-213000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-214000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-214000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-214000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-213500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-214500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-214500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-214500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-214000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-215000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-215000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-215000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-214500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-215500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-215500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-215500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-215000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-216000\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-216000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-216000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-215500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-4/checkpoint-216500\n",
      "Configuration saved in Gentacimin_finetuning/run-4/checkpoint-216500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-4/checkpoint-216500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-4/checkpoint-216000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:27:01,244]\u001b[0m Trial 4 finished with value: 0.2986200451850891 and parameters: {'learning_rate': 0.001349960761604314, 'num_train_epochs': 72, 'seed': 32, 'per_device_train_batch_size': 4}. Best is trial 1 with value: 0.26482075452804565.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0015220291397067902, 'num_train_epochs': 40, 'seed': 35, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 120400\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='120400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/120400 00:08 < 17:53, 111.24 it/s, Epoch 0/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.285897</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-5/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-5/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-5/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 00:27:10,481]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014083570641582478, 'num_train_epochs': 85, 'seed': 22, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 85\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32045\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32045' max='32045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32045/32045 05:30, Epoch 85/85]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.270993</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.262481</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.266374</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.276710</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.262859</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.261657</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.262107</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.262096</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.268566</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261742</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.266479</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.261784</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.261487</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.261144</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.263630</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261390</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.261169</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.268641</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.261133</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.265317</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.262654</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.267230</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.261234</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.262722</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.262220</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.261980</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.261099</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261101</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-6/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-6/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-6/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-6/checkpoint-31500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:32:41,186]\u001b[0m Trial 6 finished with value: 0.2611006498336792 and parameters: {'learning_rate': 0.0014083570641582478, 'num_train_epochs': 85, 'seed': 22, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0018725496772830227, 'num_train_epochs': 59, 'seed': 26, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 59\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11151\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11151' max='11151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11151/11151 02:04, Epoch 59/59]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.272578</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.279448</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.262042</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.264570</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261608</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261297</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.264327</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261419</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.262462</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.261672</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.261312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-7/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-7/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-7/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-7/checkpoint-10500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:34:46,357]\u001b[0m Trial 7 finished with value: 0.2613120973110199 and parameters: {'learning_rate': 0.0018725496772830227, 'num_train_epochs': 59, 'seed': 26, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0014759114648143577, 'num_train_epochs': 13, 'seed': 14, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 13\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4901\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4901' max='4901' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4901/4901 00:49, Epoch 13/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.265203</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.262651</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.262757</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.263149</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-8/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-8/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-8/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-8/checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:35:36,487]\u001b[0m Trial 8 finished with value: 0.26314908266067505 and parameters: {'learning_rate': 0.0014759114648143577, 'num_train_epochs': 13, 'seed': 14, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0016136617445746176, 'num_train_epochs': 38, 'seed': 36, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 38\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 114380\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='114380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/114380 00:08 < 16:53, 111.88 it/s, Epoch 0/38]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-9/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-9/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-9/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 00:35:45,670]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0010206434125187764, 'num_train_epochs': 98, 'seed': 1, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 98\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36946\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36946' max='36946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36946/36946 06:19, Epoch 98/98]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.265202</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.262568</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.262713</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.265241</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.262166</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.268325</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.261823</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.263092</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.268525</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.265231</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.262642</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.263124</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.265606</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.266041</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261751</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.262095</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.263399</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.264469</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.261938</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.261848</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.266124</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.266215</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.265662</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.261301</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.262015</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.264467</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.262173</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.261198</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.256300</td>\n",
       "      <td>0.264119</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.263264</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.261579</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.262431</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.261281</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.261120</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-10/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-10/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-10/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-10/checkpoint-36000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:42:05,582]\u001b[0m Trial 10 finished with value: 0.2611195147037506 and parameters: {'learning_rate': 0.0010206434125187764, 'num_train_epochs': 98, 'seed': 1, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0010338436313292994, 'num_train_epochs': 100, 'seed': 3, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 37700\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37700' max='37700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37700/37700 06:27, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.263785</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.262439</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.269219</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.268094</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.265769</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.261769</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261630</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.266539</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.263508</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.264626</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.261898</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.263221</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.263122</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.261378</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261742</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.264523</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.267365</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261509</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.264036</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.263079</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.263187</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.262319</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.262228</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261159</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.262752</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263254</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.261297</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261232</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261611</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.261182</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.261161</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.261249</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261898</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.261231</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.261338</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-36000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-37000\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-37000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-11/checkpoint-37500\n",
      "Configuration saved in Gentacimin_finetuning/run-11/checkpoint-37500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-11/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-11/checkpoint-37000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:48:32,918]\u001b[0m Trial 11 finished with value: 0.26133832335472107 and parameters: {'learning_rate': 0.0010338436313292994, 'num_train_epochs': 100, 'seed': 3, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001009025872505908, 'num_train_epochs': 98, 'seed': 13, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 98\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36946\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36946' max='36946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36946/36946 06:18, Epoch 98/98]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.266431</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.263480</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.269221</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.263205</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262942</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.262521</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.264090</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.262059</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.271291</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.261742</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.264362</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.262877</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.263737</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.261621</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.263565</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.261865</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.265681</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.272567</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.263219</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261296</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.261465</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.260939</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.263593</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.261657</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.261266</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261176</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.261482</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.261037</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.263082</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.261711</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>0.261230</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.261277</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261305</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-12/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-12/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-12/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-12/checkpoint-36000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:54:51,571]\u001b[0m Trial 12 finished with value: 0.2613045275211334 and parameters: {'learning_rate': 0.001009025872505908, 'num_train_epochs': 98, 'seed': 13, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001701408929836327, 'num_train_epochs': 77, 'seed': 1, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 77\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 57981\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='57981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/57981 00:09 < 08:53, 106.85 it/s, Epoch 1/77]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-13/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-13/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-13/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 00:55:01,197]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0011328012976426502, 'num_train_epochs': 61, 'seed': 12, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 61\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22997\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22997' max='22997' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22997/22997 03:55, Epoch 61/61]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.273300</td>\n",
       "      <td>0.265185</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.270368</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.271099</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.262924</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.265345</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.267236</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.276154</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.262222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261709</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.265332</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.262303</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.266118</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.262138</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.263724</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261330</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.269970</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.261261</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.263481</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.265087</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.263691</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.262003</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.261414</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-14/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-14/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-14/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-14/checkpoint-22000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 00:58:57,335]\u001b[0m Trial 14 finished with value: 0.26141396164894104 and parameters: {'learning_rate': 0.0011328012976426502, 'num_train_epochs': 61, 'seed': 12, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.2611006498336792.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013922117061854215, 'num_train_epochs': 89, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 89\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33553\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33553' max='33553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33553/33553 05:44, Epoch 89/89]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263248</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.266849</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.270499</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.277867</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.265675</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.261569</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.262857</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.261506</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.269008</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261338</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.267118</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261245</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.261214</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262163</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.261712</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.263839</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261707</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263764</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261840</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.262040</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.261147</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261031</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261356</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.261561</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261360</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.262029</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261007</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.261534</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261149</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.260997</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-15/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-15/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-15/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-15/checkpoint-33000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:04:42,346]\u001b[0m Trial 15 finished with value: 0.2609974443912506 and parameters: {'learning_rate': 0.0013922117061854215, 'num_train_epochs': 89, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 15 with value: 0.2609974443912506.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013591818524478169, 'num_train_epochs': 82, 'seed': 8, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 82\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15498\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15498' max='15498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15498/15498 02:53, Epoch 82/82]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.262631</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.262271</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261724</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.261723</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261505</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.261659</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.263506</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.264070</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.262362</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.265385</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.261653</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.263389</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.262102</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.261256</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.261252</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-16/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-16/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-16/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-16/checkpoint-14500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:07:36,133]\u001b[0m Trial 16 finished with value: 0.26125210523605347 and parameters: {'learning_rate': 0.0013591818524478169, 'num_train_epochs': 82, 'seed': 8, 'per_device_train_batch_size': 64}. Best is trial 15 with value: 0.2609974443912506.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0016713777877649589, 'num_train_epochs': 67, 'seed': 20, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 67\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50451\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='50451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6000/50451 00:56 < 07:02, 105.27 it/s, Epoch 7/67]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.264925</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.269622</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.270104</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.265661</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.265963</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.267472</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-17/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-17/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-17/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-17/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:08:33,385]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014359342898413057, 'num_train_epochs': 85, 'seed': 19, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 85\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32045\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='32045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/32045 00:09 < 05:05, 101.53 it/s, Epoch 2/85]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.266315</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-18/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-18/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-18/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:08:43,493]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0015425907584410332, 'num_train_epochs': 46, 'seed': 27, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 46\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17342\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='17342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/17342 00:09 < 02:40, 101.59 it/s, Epoch 2/46]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.277125</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-19/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-19/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-19/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:08:53,585]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0013418991338319146, 'num_train_epochs': 89, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 89\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33553\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33553 00:10 < 05:26, 99.62 it/s, Epoch 2/89]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.267448</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-20/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-20/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-20/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:09:03,875]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001122172384565272, 'num_train_epochs': 93, 'seed': 8, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 93\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35061\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35061' max='35061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35061/35061 06:00, Epoch 93/93]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.265362</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.266310</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.273794</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.262655</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.263420</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261635</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.265078</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.262974</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.265268</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.266544</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.261374</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.262023</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261660</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.265009</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261167</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.261855</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.261556</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.262828</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.261337</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261178</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.263055</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261635</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261177</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.261316</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.265103</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261324</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.261991</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.261296</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261407</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261294</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.261426</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.261260</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-21/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-21/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-21/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-21/checkpoint-34500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:15:04,191]\u001b[0m Trial 21 finished with value: 0.2612603008747101 and parameters: {'learning_rate': 0.001122172384565272, 'num_train_epochs': 93, 'seed': 8, 'per_device_train_batch_size': 32}. Best is trial 15 with value: 0.2609974443912506.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0010668229415761283, 'num_train_epochs': 73, 'seed': 16, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 73\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27521\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27521' max='27521' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27521/27521 04:43, Epoch 73/73]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.263138</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.263815</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.262467</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.264253</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.275270</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.261812</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.262399</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261480</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.261920</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.267073</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.264285</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.261576</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261652</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.265439</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.261332</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.262206</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261571</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.261313</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.266628</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.261139</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.262597</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.261251</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.265479</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.262257</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.261805</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.261234</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-22/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-22/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-22/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-22/checkpoint-27000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:19:48,099]\u001b[0m Trial 22 finished with value: 0.26123419404029846 and parameters: {'learning_rate': 0.0010668229415761283, 'num_train_epochs': 73, 'seed': 16, 'per_device_train_batch_size': 32}. Best is trial 15 with value: 0.2609974443912506.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001267463742297948, 'num_train_epochs': 93, 'seed': 4, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 93\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35061\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='35061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/35061 00:10 < 05:42, 99.42 it/s, Epoch 2/93]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.291346</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-23/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-23/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-23/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:19:58,410]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0011909042061761263, 'num_train_epochs': 80, 'seed': 10, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 80\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30160\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='30160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/30160 00:10 < 04:52, 99.69 it/s, Epoch 2/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.269504</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-24/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-24/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-24/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:20:08,689]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0017631499823868929, 'num_train_epochs': 57, 'seed': 23, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 57\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 42921\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='42921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/42921 00:09 < 06:30, 107.41 it/s, Epoch 1/57]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>0.276966</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-25/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-25/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-25/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:20:18,252]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014000864604902458, 'num_train_epochs': 100, 'seed': 1, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18900\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='18900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/18900 00:10 < 03:16, 91.01 it/s, Epoch 5/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.265725</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-26/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-26/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-26/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:20:29,490]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0010849159422437085, 'num_train_epochs': 68, 'seed': 17, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 68\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 102340\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7000' max='102340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7000/102340 01:05 < 14:47, 107.39 it/s, Epoch 4/68]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>0.264436</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.267711</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.277003</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.263888</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-27/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-27/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-27/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-27/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:21:34,955]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0013034311293211296, 'num_train_epochs': 91, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 91\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34307\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34307' max='34307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34307/34307 05:52, Epoch 91/91]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.263313</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.275853</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.262117</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.276004</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.268956</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.261824</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.273868</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.262267</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.275280</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.262469</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.267273</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.261275</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.261239</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261270</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.261244</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.265039</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261695</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.264131</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.261952</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261660</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261705</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261890</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.262635</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261018</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261531</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261035</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.260988</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261489</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.261133</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.260983</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-28/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-28/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-28/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-28/checkpoint-33500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:27:28,221]\u001b[0m Trial 28 finished with value: 0.26098304986953735 and parameters: {'learning_rate': 0.0013034311293211296, 'num_train_epochs': 91, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013122639178166377, 'num_train_epochs': 90, 'seed': 25, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 90\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33930\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33930 00:10 < 05:32, 99.13 it/s, Epoch 2/90]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-29/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-29/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-29/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:27:38,576]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001425152325701292, 'num_train_epochs': 24, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 24\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9048\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='9048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/9048 00:10 < 01:20, 99.42 it/s, Epoch 2/24]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.268174</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-30/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-30/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-30/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:27:48,882]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0015387678439850038, 'num_train_epochs': 95, 'seed': 4, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 95\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35815\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='35815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/35815 00:10 < 05:49, 99.48 it/s, Epoch 2/95]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.302925</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-31/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-31/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-31/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:27:59,186]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001215950508209724, 'num_train_epochs': 78, 'seed': 8, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 78\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 29406\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29406' max='29406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29406/29406 05:02, Epoch 78/78]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.264850</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.265983</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.274160</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.262728</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.263085</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261603</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.265004</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.263117</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.265207</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261578</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.266549</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.261341</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.261957</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.261411</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.264844</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.261214</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261698</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261430</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.263022</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.263651</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.261386</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.261314</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.264221</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.262107</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261239</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.261310</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.261603</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.261251</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-32/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-32/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-32/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-32/checkpoint-28500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:33:02,124]\u001b[0m Trial 32 finished with value: 0.261251300573349 and parameters: {'learning_rate': 0.001215950508209724, 'num_train_epochs': 78, 'seed': 8, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0019695830894371873, 'num_train_epochs': 87, 'seed': 11, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 87\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 130935\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='130935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/130935 00:09 < 19:48, 109.30 it/s, Epoch 0/87]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.297451</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-33/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-33/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-33/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:33:11,527]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001267585558108384, 'num_train_epochs': 84, 'seed': 2, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 84\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31668\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31668' max='31668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31668/31668 05:25, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.263288</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.266042</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.262071</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.266982</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.264632</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.264588</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.262435</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.263902</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.265717</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.267225</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.266244</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.264475</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261324</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.261211</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.261759</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.265695</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.261532</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.260917</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.264763</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261681</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.264141</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.265984</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.263278</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.261321</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.261776</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261220</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.261191</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.261239</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-34/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-34/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-34/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-34/checkpoint-31000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:38:37,078]\u001b[0m Trial 34 finished with value: 0.2612389624118805 and parameters: {'learning_rate': 0.001267585558108384, 'num_train_epochs': 84, 'seed': 2, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001461620282323842, 'num_train_epochs': 92, 'seed': 29, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 92\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 138460\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='138460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/138460 00:09 < 20:50, 109.93 it/s, Epoch 0/92]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.270553</td>\n",
       "      <td>0.499059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-35/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-35/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-35/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:38:46,443]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0012991878812159234, 'num_train_epochs': 74, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 74\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27898\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='27898' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/27898 00:10 < 04:30, 99.30 it/s, Epoch 2/74]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.268475</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-36/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-36/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-36/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:38:56,768]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001169268529806587, 'num_train_epochs': 66, 'seed': 23, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 66\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198660\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='198660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/198660 00:08 < 29:17, 112.47 it/s, Epoch 0/66]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.332680</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-37/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-37/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-37/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:39:05,912]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0013696356583436432, 'num_train_epochs': 52, 'seed': 15, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 52\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9828\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9828' max='9828' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9828/9828 01:50, Epoch 52/52]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.262739</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.263347</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.262256</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261961</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.266236</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.264744</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.270067</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261284</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261370</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-38/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-38/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-38/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-38/checkpoint-9000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:40:56,482]\u001b[0m Trial 38 finished with value: 0.2613704800605774 and parameters: {'learning_rate': 0.0013696356583436432, 'num_train_epochs': 52, 'seed': 15, 'per_device_train_batch_size': 64}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0015938946428743451, 'num_train_epochs': 96, 'seed': 18, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 96\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 72288\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='72288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/72288 00:09 < 11:15, 105.52 it/s, Epoch 1/96]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>0.266377</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-39/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-39/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-39/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:41:06,215]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014883975901852273, 'num_train_epochs': 88, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 88\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33176\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33176' max='33176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33176/33176 05:42, Epoch 88/88]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273300</td>\n",
       "      <td>0.276424</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.262039</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.280216</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.271508</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.262110</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.276218</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.262316</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.279767</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.262864</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.265312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.261467</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.261389</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.261395</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261645</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.264780</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.276844</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.261636</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.260990</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.263938</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.262131</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261645</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261824</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261967</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261832</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.262587</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.260970</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261473</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261351</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261004</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.260941</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261147</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.261078</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-40/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-40/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-40/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-40/checkpoint-32500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:46:49,195]\u001b[0m Trial 40 finished with value: 0.2610780894756317 and parameters: {'learning_rate': 0.0014883975901852273, 'num_train_epochs': 88, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013997783396537998, 'num_train_epochs': 88, 'seed': 4, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 88\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33176\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33176 00:10 < 05:24, 99.12 it/s, Epoch 2/88]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.296893</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-41/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-41/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-41/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:46:59,536]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014905488389054785, 'num_train_epochs': 82, 'seed': 10, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 82\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30914\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='30914' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/30914 00:10 < 05:01, 99.13 it/s, Epoch 2/82]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.274981</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-42/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-42/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-42/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:47:09,897]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0016103441986284643, 'num_train_epochs': 3, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1131\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1131' max='1131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1131/1131 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.264386</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-43/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-43/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-43/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-43/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-43/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-43/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-43/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:47:21,692]\u001b[0m Trial 43 finished with value: 0.26438596844673157 and parameters: {'learning_rate': 0.0016103441986284643, 'num_train_epochs': 3, 'seed': 6, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0015007815180630452, 'num_train_epochs': 100, 'seed': 3, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 301000\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='301000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/301000 00:09 < 45:31, 109.82 it/s, Epoch 0/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>0.301881</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-44/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-44/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-44/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 01:47:31,053]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001571294212081556, 'num_train_epochs': 77, 'seed': 40, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 77\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 29029\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29029' max='29029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29029/29029 05:00, Epoch 77/77]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.263464</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.286575</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.264540</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.263712</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.265792</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.268654</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261104</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.262920</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.264137</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.263887</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.264164</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.264022</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.261442</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.263721</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.264877</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.263610</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.261184</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261246</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261827</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261327</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.266155</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.262941</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.261679</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.264141</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.263585</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.261125</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.261142</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.261114</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.261093</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-45/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-45/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-45/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-45/checkpoint-28500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:52:31,581]\u001b[0m Trial 45 finished with value: 0.26109257340431213 and parameters: {'learning_rate': 0.001571294212081556, 'num_train_epochs': 77, 'seed': 40, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.00155684335563832, 'num_train_epochs': 75, 'seed': 37, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 75\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28275\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28275' max='28275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28275/28275 04:51, Epoch 75/75]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.263783</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.264718</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.272690</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.263770</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261568</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.278063</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.267561</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.262650</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.262144</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.268495</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.263730</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.264217</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261881</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.261642</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.264575</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261878</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.263162</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261288</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.262543</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.262437</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.263252</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.261102</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.261219</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.261270</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.261129</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.261157</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-46/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-46/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-46/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-46/checkpoint-27500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 01:57:23,495]\u001b[0m Trial 46 finished with value: 0.2611565887928009 and parameters: {'learning_rate': 0.00155684335563832, 'num_train_epochs': 75, 'seed': 37, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0017400545997247289, 'num_train_epochs': 69, 'seed': 35, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 69\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 26013\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26013' max='26013' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26013/26013 04:28, Epoch 69/69]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.264026</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.262043</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.278226</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.262616</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.261571</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.284308</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.267661</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.265196</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.261607</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.282134</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.262711</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.261691</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.265638</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.265088</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.262523</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.264158</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.261086</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.271569</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261282</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261166</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261981</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.262075</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.261169</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261015</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261061</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261167</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-47/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-47/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-47/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-47/checkpoint-25500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:01:52,211]\u001b[0m Trial 47 finished with value: 0.26116687059402466 and parameters: {'learning_rate': 0.0017400545997247289, 'num_train_epochs': 69, 'seed': 35, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0014580856942423967, 'num_train_epochs': 80, 'seed': 29, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 80\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15120\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15120' max='15120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15120/15120 02:49, Epoch 80/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.262803</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.266226</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.267094</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.262888</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.272076</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.272955</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.261622</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.263995</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.263020</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.262683</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.261467</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261592</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.261371</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.261460</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-48/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-48/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-48/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-48/checkpoint-14500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:04:42,083]\u001b[0m Trial 48 finished with value: 0.26145994663238525 and parameters: {'learning_rate': 0.0014580856942423967, 'num_train_epochs': 80, 'seed': 29, 'per_device_train_batch_size': 64}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013148983507319815, 'num_train_epochs': 86, 'seed': 32, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 86\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32422\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='32422' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/32422 00:10 < 05:15, 99.44 it/s, Epoch 2/86]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.265606</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-49/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-49/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-49/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:04:52,396]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0016662172444800937, 'num_train_epochs': 32, 'seed': 39, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 32\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96320\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='96320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/96320 00:08 < 14:13, 111.69 it/s, Epoch 0/32]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.354500</td>\n",
       "      <td>0.324440</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-50/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-50/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-50/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:05:01,594]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0012424569840573316, 'num_train_epochs': 96, 'seed': 1, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 96\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36192\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='36192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/36192 00:10 < 05:58, 98.12 it/s, Epoch 2/96]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.265310</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-51/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-51/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-51/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:05:12,039]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001003234110902347, 'num_train_epochs': 90, 'seed': 40, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 90\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33930\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='33930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/33930 02:34 < 03:15, 97.06 it/s, Epoch 39/90]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.263505</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.271627</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.264182</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.262985</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.269436</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.271044</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.262116</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.261971</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.262377</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.264705</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261710</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.262897</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.261760</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.266370</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.265234</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-52/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-52/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-52/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-52/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:07:46,839]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001575041134650805, 'num_train_epochs': 85, 'seed': 13, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 85\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32045\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='32045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/32045 00:10 < 05:13, 99.05 it/s, Epoch 2/85]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.275759</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-53/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-53/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-53/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:07:57,212]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001371404882781924, 'num_train_epochs': 92, 'seed': 8, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 92\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34684\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34684' max='34684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34684/34684 05:57, Epoch 92/92]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.264168</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.264455</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.275200</td>\n",
       "      <td>0.274458</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.263296</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.262221</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.261545</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.264992</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.265329</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.265254</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261727</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.265951</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.261383</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.261980</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.263792</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.264877</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.261297</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.261104</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.261985</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.261689</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.263037</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.262269</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.261275</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.261089</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261315</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261092</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261220</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.265627</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261296</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.262172</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.261227</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261387</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261227</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261337</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-54/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-54/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-54/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-54/checkpoint-34000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:13:55,414]\u001b[0m Trial 54 finished with value: 0.26133662462234497 and parameters: {'learning_rate': 0.001371404882781924, 'num_train_epochs': 92, 'seed': 8, 'per_device_train_batch_size': 32}. Best is trial 28 with value: 0.26098304986953735.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0018057370791132899, 'num_train_epochs': 77, 'seed': 3, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 77\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 57981\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='57981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4000/57981 00:38 < 08:34, 105.00 it/s, Epoch 5/77]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.264156</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.264015</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.274069</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.264369</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-55/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-55/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-55/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-55/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-55/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-55/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-55/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-55/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-55/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:14:33,767]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0016484428525330938, 'num_train_epochs': 96, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 96\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36192\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36192' max='36192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36192/36192 06:14, Epoch 96/96]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.263157</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.274930</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.261955</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.280089</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.274420</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.262294</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.278004</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.261911</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.284713</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.263213</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.263195</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.261422</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261931</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.264290</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.275432</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.261459</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.260943</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.263096</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.262919</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261783</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.262052</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.261964</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.262393</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261696</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.260711</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.261711</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.260955</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.260866</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.260797</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.261877</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.260842</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261342</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261007</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.260906</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-56/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-56/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-56/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-56/checkpoint-35500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:20:48,557]\u001b[0m Trial 56 finished with value: 0.2609061896800995 and parameters: {'learning_rate': 0.0016484428525330938, 'num_train_epochs': 96, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0016328587246333117, 'num_train_epochs': 83, 'seed': 10, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 83\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31291\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='31291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/31291 00:10 < 05:09, 97.86 it/s, Epoch 2/83]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.277966</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-57/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-57/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-57/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:20:59,028]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0015173405589388638, 'num_train_epochs': 63, 'seed': 5, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 63\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 94815\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='94815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/94815 00:09 < 14:26, 108.29 it/s, Epoch 0/63]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.265163</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-58/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-58/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-58/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:21:08,517]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014358847715204483, 'num_train_epochs': 95, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 95\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35815\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35815' max='35815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35815/35815 06:08, Epoch 95/95]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263209</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.266270</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.279055</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.270494</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.278486</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.266556</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.261647</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.263191</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.261361</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.268091</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.261331</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.268335</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.261045</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261191</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.261942</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.261770</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.264634</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261418</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.262853</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.262059</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.261417</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261009</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261141</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.260932</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.261279</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261297</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.263153</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261028</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261303</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.260979</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.261162</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261027</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-59/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-59/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-59/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-59/checkpoint-35000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:27:17,734]\u001b[0m Trial 59 finished with value: 0.2610265612602234 and parameters: {'learning_rate': 0.0014358847715204483, 'num_train_epochs': 95, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0014419295395152207, 'num_train_epochs': 97, 'seed': 9, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 97\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36569\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='36569' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/36569 00:10 < 05:59, 99.07 it/s, Epoch 2/97]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.270941</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-60/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-60/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-60/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:27:28,080]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014056168768295993, 'num_train_epochs': 89, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 89\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33553\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33553' max='33553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33553/33553 05:44, Epoch 89/89]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263235</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.266688</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.278978</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.270497</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.278011</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.265851</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.261581</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.262919</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.261472</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.268856</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.267294</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261229</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.261189</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261542</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.261713</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.263906</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261679</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263799</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261892</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.261155</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261027</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261342</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261016</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.261547</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261357</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.262049</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261004</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261146</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.260994</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-61/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-61/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-61/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-61/checkpoint-33000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:33:12,424]\u001b[0m Trial 61 finished with value: 0.26099419593811035 and parameters: {'learning_rate': 0.0014056168768295993, 'num_train_epochs': 89, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013400693598484463, 'num_train_epochs': 93, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 93\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35061\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35061' max='35061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35061/35061 05:59, Epoch 93/93]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263302</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.267455</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273300</td>\n",
       "      <td>0.278784</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.270511</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.277371</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.265119</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.261544</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.261601</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.269312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261354</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.266856</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261274</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.261244</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261631</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262170</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.261721</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.263932</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261674</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263877</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.262043</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.262083</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.261224</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261037</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261278</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.260987</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261426</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.262580</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261023</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261401</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.261080</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.261043</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.261155</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261050</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-62/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-62/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-62/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-62/checkpoint-34500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:39:12,337]\u001b[0m Trial 62 finished with value: 0.26104989647865295 and parameters: {'learning_rate': 0.0013400693598484463, 'num_train_epochs': 93, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013315863515819032, 'num_train_epochs': 92, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 92\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34684\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34684' max='34684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34684/34684 05:56, Epoch 92/92]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263311</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.267577</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.278743</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.270515</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.264950</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.261536</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.262642</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.261644</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.269456</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.261360</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.266639</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.261299</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261279</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261720</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262198</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.261717</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.263802</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261728</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263795</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261904</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.261189</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261041</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261316</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261004</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261476</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261374</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.262406</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261023</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261444</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.261122</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.261120</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-63/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-63/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-63/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-63/checkpoint-34000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:45:08,610]\u001b[0m Trial 63 finished with value: 0.2611199915409088 and parameters: {'learning_rate': 0.0013315863515819032, 'num_train_epochs': 92, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001392352833216097, 'num_train_epochs': 94, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 94\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35438\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35438' max='35438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35438/35438 06:05, Epoch 94/94]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.263230</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.276274</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.262080</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.278899</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.270321</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.261969</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.275293</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.262331</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.278085</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.262750</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.265937</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.261430</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.261345</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.261381</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261550</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.264818</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.276814</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.261647</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.260999</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.262279</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261666</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.261972</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.262165</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.262097</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.262322</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.260863</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261610</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261030</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.260969</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.260923</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.260936</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.261048</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261083</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-64/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-64/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-64/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-64/checkpoint-34500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:51:14,242]\u001b[0m Trial 64 finished with value: 0.26108285784721375 and parameters: {'learning_rate': 0.001392352833216097, 'num_train_epochs': 94, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013500326764555944, 'num_train_epochs': 89, 'seed': 12, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 89\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33553\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33553 00:10 < 05:27, 99.54 it/s, Epoch 2/89]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.267063</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-65/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-65/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-65/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:51:24,546]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001298226372205758, 'num_train_epochs': 98, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 98\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36946\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36946' max='36946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36946/36946 06:20, Epoch 98/98]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263343</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.267937</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.278626</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.270529</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.276881</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.264718</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.261533</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.262602</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.261668</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.269465</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261366</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.266760</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261285</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.261248</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261604</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.262157</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.261732</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261612</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.264018</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.262342</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.262106</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.261341</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261041</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261198</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.260966</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.261314</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261330</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.263260</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.261076</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.261315</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261017</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261311</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261159</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.261055</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.261055</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-66/checkpoint-36500\n",
      "Configuration saved in Gentacimin_finetuning/run-66/checkpoint-36500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-66/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-66/checkpoint-36000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 02:57:44,986]\u001b[0m Trial 66 finished with value: 0.261055052280426 and parameters: {'learning_rate': 0.001298226372205758, 'num_train_epochs': 98, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0012998451230031565, 'num_train_epochs': 99, 'seed': 11, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 99\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 37323\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='37323' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/37323 00:10 < 06:08, 98.51 it/s, Epoch 2/99]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.265892</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-67/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-67/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-67/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 02:57:55,402]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0012321678303768547, 'num_train_epochs': 96, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 96\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36192\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36192' max='36192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36192/36192 06:14, Epoch 96/96]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263396</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.268837</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.278243</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.270571</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.275510</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.263808</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.261509</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.262264</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.270219</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.265678</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.261430</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261429</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.262049</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262257</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>0.261730</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.263587</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261830</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263735</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.261843</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.262083</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.261216</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261065</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261310</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.261011</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.261432</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261386</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.262775</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261059</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261375</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261040</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261152</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261215</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.261063</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.261121</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-35500\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-35500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-68/checkpoint-36000\n",
      "Configuration saved in Gentacimin_finetuning/run-68/checkpoint-36000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-68/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-68/checkpoint-35500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:04:10,251]\u001b[0m Trial 68 finished with value: 0.26112115383148193 and parameters: {'learning_rate': 0.0012321678303768547, 'num_train_epochs': 96, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0012814799504301448, 'num_train_epochs': 19, 'seed': 9, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 19\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3591\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='3591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/3591 00:11 < 00:28, 90.40 it/s, Epoch 5/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.264892</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-69/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-69/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-69/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:04:21,586]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0011972578447767212, 'num_train_epochs': 91, 'seed': 2, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 91\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 68523\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='68523' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/68523 00:09 < 10:32, 106.68 it/s, Epoch 1/91]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.281428</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-70/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-70/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-70/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:04:31,211]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014209120651846752, 'num_train_epochs': 87, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 87\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32799\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32799' max='32799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32799/32799 05:37, Epoch 87/87]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.263214</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.276339</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.262069</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.270456</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.261973</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.275289</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.262323</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.277796</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.266221</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.261383</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.261289</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.261330</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261337</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.264920</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.276488</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.264098</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.261950</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261644</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.261662</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.261790</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261665</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.262692</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.261054</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261431</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261636</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.260966</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261043</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-71/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-71/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-71/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-71/checkpoint-32000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:10:09,178]\u001b[0m Trial 71 finished with value: 0.2610427737236023 and parameters: {'learning_rate': 0.0014209120651846752, 'num_train_epochs': 87, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001372409894243254, 'num_train_epochs': 94, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 94\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35438\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35438' max='35438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35438/35438 06:03, Epoch 94/94]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263267</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.267035</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.278906</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.270502</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.277793</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.265628</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.261573</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.262876</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.261493</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.268879</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261339</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261223</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.261164</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261445</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.262096</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.261731</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.264192</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261572</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.264006</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.262308</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.261290</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261027</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261220</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.260963</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.261360</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.261339</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.262816</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261022</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261353</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.261028</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.261065</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.261171</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-72/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-72/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-72/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-72/checkpoint-34500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:16:12,949]\u001b[0m Trial 72 finished with value: 0.26105833053588867 and parameters: {'learning_rate': 0.001372409894243254, 'num_train_epochs': 94, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0014213721074708312, 'num_train_epochs': 100, 'seed': 9, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 37700\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='37700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/37700 00:10 < 06:08, 99.50 it/s, Epoch 2/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.270802</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-73/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-73/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-73/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:16:23,254]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.00133234730034128, 'num_train_epochs': 80, 'seed': 4, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 80\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30160\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='30160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/30160 00:10 < 04:53, 99.35 it/s, Epoch 2/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.293836</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-74/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-74/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-74/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:16:33,572]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001402482409744018, 'num_train_epochs': 87, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 87\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32799\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32799' max='32799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32799/32799 05:36, Epoch 87/87]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263238</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.266750</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.278961</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.270497</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.277916</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.265716</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.261569</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.262856</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.261508</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.269048</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.261338</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.267016</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.261255</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.261234</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261666</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.262185</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>0.261706</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.263692</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261768</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.263643</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.261678</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261996</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.261105</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261032</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.261424</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261071</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.261336</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.261716</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261011</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.261533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.261047</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-75/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-75/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-75/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-75/checkpoint-32000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:22:10,147]\u001b[0m Trial 75 finished with value: 0.2610473036766052 and parameters: {'learning_rate': 0.001402482409744018, 'num_train_epochs': 87, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0013950565113047984, 'num_train_epochs': 82, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 82\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30914\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30914' max='30914' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30914/30914 05:17, Epoch 82/82]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.262083</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.278306</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.269867</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.261894</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.274631</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.262290</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.276254</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.262531</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.267058</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.261273</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.261219</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261239</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.261218</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.265083</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.275352</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.261665</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.261113</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.264266</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261658</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.261618</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.261319</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.261368</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261303</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.262542</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261182</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.261207</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261837</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.261017</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-76/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-76/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-76/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-76/checkpoint-30000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:27:28,223]\u001b[0m Trial 76 finished with value: 0.26101669669151306 and parameters: {'learning_rate': 0.0013950565113047984, 'num_train_epochs': 82, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 56 with value: 0.2609061896800995.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0014374178487291346, 'num_train_epochs': 86, 'seed': 5, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 86\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 258860\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='258860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/258860 00:08 < 36:58, 116.22 it/s, Epoch 0/86]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.306104</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-77/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-77/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-77/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:27:37,088]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0013960643696404621, 'num_train_epochs': 83, 'seed': 2, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 83\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31291\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='31291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/31291 00:09 < 04:54, 102.86 it/s, Epoch 2/83]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.263828</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-78/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-78/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-78/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:27:47,058]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014740237200694135, 'num_train_epochs': 88, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 88\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33176\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33176 00:09 < 05:16, 101.71 it/s, Epoch 2/88]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.264993</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-79/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-79/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-79/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:27:57,141]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001424738569566816, 'num_train_epochs': 45, 'seed': 3, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 67725\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='67725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/67725 00:09 < 10:12, 108.91 it/s, Epoch 0/45]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.288600</td>\n",
       "      <td>0.267741</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-80/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-80/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-80/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:28:06,576]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0012507764377334026, 'num_train_epochs': 90, 'seed': 8, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 90\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33930\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33930 00:10 < 05:32, 98.90 it/s, Epoch 2/90]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.264656</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-81/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-81/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-81/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:28:16,940]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0013810613435332516, 'num_train_epochs': 71, 'seed': 4, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 71\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 26767\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='26767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/26767 00:10 < 04:21, 98.69 it/s, Epoch 2/71]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.295650</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-82/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-82/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-82/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:28:27,323]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0013588019106475094, 'num_train_epochs': 94, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 94\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35438\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='35438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/35438 00:10 < 05:48, 98.92 it/s, Epoch 2/94]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.267048</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-83/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-83/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-83/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:28:37,700]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001330205036425354, 'num_train_epochs': 81, 'seed': 11, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 81\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30537\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='30537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/30537 00:10 < 04:58, 98.91 it/s, Epoch 2/81]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.265776</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-84/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-84/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-84/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:28:48,069]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0015176623024566892, 'num_train_epochs': 85, 'seed': 9, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 85\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32045\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='32045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/32045 00:10 < 05:14, 98.63 it/s, Epoch 2/85]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-85/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-85/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-85/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:28:58,470]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0018650058619830382, 'num_train_epochs': 91, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 91\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34307\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34307' max='34307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34307/34307 05:52, Epoch 91/91]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.271266</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.261876</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.276650</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.277376</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.261963</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.278786</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.261447</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.287486</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.263446</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.262493</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.261262</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.262433</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.261166</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.265444</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.264001</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.274324</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.261394</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.260929</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.262864</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.262940</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.261832</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.262025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.261903</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.262372</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.261773</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.260685</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.261616</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.260864</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.260813</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261558</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.260905</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.260847</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-86/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-86/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-86/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-86/checkpoint-33500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:34:50,959]\u001b[0m Trial 86 finished with value: 0.26084721088409424 and parameters: {'learning_rate': 0.0018650058619830382, 'num_train_epochs': 91, 'seed': 5, 'per_device_train_batch_size': 32}. Best is trial 86 with value: 0.26084721088409424.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0019401379336315458, 'num_train_epochs': 87, 'seed': 2, 'per_device_train_batch_size': 64}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 87\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16443\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='16443' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/16443 02:37 < 00:27, 89.15 it/s, Epoch 74/87]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.262510</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.262023</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.264153</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261782</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.265236</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.264183</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.261418</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.262445</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.262411</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.261720</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.263808</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.261884</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261640</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.263644</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-87/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-87/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-87/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-87/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:37:28,258]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0018304636381947872, 'num_train_epochs': 79, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 79\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 29783\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='29783' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/29783 02:23 < 02:41, 97.43 it/s, Epoch 37/79]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.263183</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.272340</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.261888</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.278189</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.276187</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.262153</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.278462</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.261708</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.285686</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.263227</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.263122</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.261408</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.261801</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261403</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-88/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-88/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-88/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-88/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:39:52,216]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001920781912445499, 'num_train_epochs': 90, 'seed': 4, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 90\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33930\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='33930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/33930 00:10 < 05:33, 98.63 it/s, Epoch 2/90]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.310532</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-89/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-89/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-89/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:40:02,607]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014600026328988808, 'num_train_epochs': 83, 'seed': 8, 'per_device_train_batch_size': 16}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 83\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 62499\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='62499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/62499 00:09 < 09:34, 107.02 it/s, Epoch 1/83]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.275818</td>\n",
       "      <td>0.499059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-90/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-90/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-90/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:40:12,202]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014107412480587045, 'num_train_epochs': 93, 'seed': 7, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 93\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35061\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35061' max='35061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35061/35061 06:00, Epoch 93/93]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.266582</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.279005</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.270496</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.278171</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.266102</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.261607</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.261415</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.268509</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.261330</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.267803</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.261198</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.261112</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.261338</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.262043</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.261738</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.264312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.261526</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.264038</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.262394</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.262083</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.261296</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.261018</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.261212</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.260955</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.261360</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.261334</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.262735</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.261010</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.261365</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.261047</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.261032</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.261142</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261033</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-14000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-14000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-14500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-14500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-15000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-15000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-15500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-15500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-16000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-16000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-16500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-16500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-17000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-17000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-17500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-17500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-18000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-18000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-18500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-18500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-19000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-19000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-19500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-19500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-19000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-20000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-20000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-20500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-20500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-21000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-21000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-21500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-21500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-22000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-22000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-22500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-22500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-23000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-23000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-23500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-23500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-24000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-24000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-24500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-24500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-25000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-25000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-25500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-25500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-26000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-26000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-26500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-26500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-27000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-27000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-27500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-27500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-28000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-28000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-28500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-28500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-29000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-29000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-29500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-29500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-30000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-30000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-30500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-30500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-31000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-31000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-31500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-31500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-32000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-32000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-32500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-32500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-33000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-33000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-33500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-33500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-34000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-34000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-34500\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-34500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-91/checkpoint-35000\n",
      "Configuration saved in Gentacimin_finetuning/run-91/checkpoint-35000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-91/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-91/checkpoint-34500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-12-16 03:46:12,557]\u001b[0m Trial 91 finished with value: 0.2610333263874054 and parameters: {'learning_rate': 0.0014107412480587045, 'num_train_epochs': 93, 'seed': 7, 'per_device_train_batch_size': 32}. Best is trial 86 with value: 0.26084721088409424.\u001b[0m\n",
      "Trial: {'learning_rate': 0.0014013685803172127, 'num_train_epochs': 97, 'seed': 6, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 97\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36569\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='36569' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/36569 00:10 < 06:04, 97.57 it/s, Epoch 2/97]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.266156</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-92/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-92/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-92/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:46:23,058]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0019972846133602955, 'num_train_epochs': 92, 'seed': 3, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 92\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34684\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='34684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/34684 00:10 < 05:44, 97.71 it/s, Epoch 2/92]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.266261</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-93/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-93/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-93/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:46:33,547]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0017236868058205894, 'num_train_epochs': 87, 'seed': 5, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 87\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32799\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='32799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/32799 02:23 < 03:12, 97.42 it/s, Epoch 37/87]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.273970</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.261925</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.279509</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.275120</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.262261</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.278205</td>\n",
       "      <td>0.499686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.261838</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.285031</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.263211</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.263214</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.261420</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.261851</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.261396</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-1000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-1000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-1500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-1500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-2000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-2000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-2500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-2500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-3000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-3000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-3500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-3500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-4000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-4000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-4500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-4500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-5000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-5000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-5500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-5500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-6000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-6000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-6500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-6500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-7000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-7000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-7500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-7500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-8000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-8000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-8500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-8500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-9000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-9000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-9500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-9500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-10000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-10000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-10500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-10500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-11000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-11000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-11500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-11500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-12000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-12000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-12500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-12500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-13000\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-13000/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to Gentacimin_finetuning/run-94/checkpoint-13500\n",
      "Configuration saved in Gentacimin_finetuning/run-94/checkpoint-13500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-94/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [Gentacimin_finetuning/run-94/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:48:57,505]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014505534924057182, 'num_train_epochs': 95, 'seed': 10, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 95\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35815\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='35815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/35815 00:09 < 05:45, 100.77 it/s, Epoch 2/95]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.274333</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-95/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-95/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-95/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:49:07,692]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.001864623487642731, 'num_train_epochs': 75, 'seed': 7, 'per_device_train_batch_size': 4}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 75\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 225750\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='225750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/225750 00:08 < 32:56, 113.73 it/s, Epoch 0/75]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.316400</td>\n",
       "      <td>0.288741</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-96/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-96/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-96/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:49:16,738]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014935766865491723, 'num_train_epochs': 89, 'seed': 14, 'per_device_train_batch_size': 8}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 89\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 133945\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='133945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1000/133945 00:09 < 20:10, 109.80 it/s, Epoch 0/89]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-97/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-97/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-97/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:49:26,112]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0014149294579052008, 'num_train_epochs': 98, 'seed': 1, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 98\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36946\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='36946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/36946 00:09 < 05:59, 100.03 it/s, Epoch 2/98]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.265233</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-98/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-98/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-98/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:49:36,367]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 0.0017739331217471893, 'num_train_epochs': 85, 'seed': 8, 'per_device_train_batch_size': 32}\n",
      "loading configuration file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 82603\n",
      "}\n",
      "\n",
      "loading weights file /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12038\n",
      "  Num Epochs = 85\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32045\n",
      "  Number of trainable parameters = 386\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='32045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1000/32045 00:10 < 05:12, 99.45 it/s, Epoch 2/85]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Gentacimin_finetuning/run-99/checkpoint-500\n",
      "Configuration saved in Gentacimin_finetuning/run-99/checkpoint-500/config.json\n",
      "Model weights saved in Gentacimin_finetuning/run-99/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1719\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: a, segment_ids, pt_id, input_mask. If a, segment_ids, pt_id, input_mask are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\u001b[32m[I 2022-12-16 03:49:46,673]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tunning with Optuna.\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import json\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"AUC\":roc_auc_score(p.label_ids, preds)}\n",
    "    # return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='Gentacimin_finetuning',    # output directory\n",
    "    #evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"steps\",      # Evaluation is done (and logged) every eval_steps.\n",
    "    eval_steps=1000,                  # Number of update steps between two evaluations \n",
    "    per_device_eval_batch_size=64,    # batch size for evaluation\n",
    "    save_total_limit=1,               # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    ")\n",
    "\n",
    "def model_init():\n",
    "    model = BertForSequenceClassification.from_pretrained(root_path + \"MedBERT Pretraining/\")\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # metrics to be computed\n",
    "    model_init=model_init                # Instantiate model before training starts\n",
    ")\n",
    "\n",
    "def my_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 2e-3, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 100),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64]),\n",
    "    }\n",
    "def my_objective(metrics):\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "best_run = trainer.hyperparameter_search(direction=\"minimize\", hp_space=my_hp_space, compute_objective=my_objective, n_trials=100)\n",
    "\n",
    "# with open(\"/content/drive/My Drive/cord19/best_run.json\", \"w+\") as f:\n",
    "#   f.write(json.dumps(best_run.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0018650058619830382,\n",
       " 'num_train_epochs': 91,\n",
       " 'seed': 5,\n",
       " 'per_device_train_batch_size': 32}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run.hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "847e22e810249873d4908bdc935e4d43d04de5ac553b0385e48d62b133981721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
