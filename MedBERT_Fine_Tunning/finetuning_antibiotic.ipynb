{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mai.kassem/.conda/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(82603, 192, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 192)\n",
       "      (token_type_embeddings): Embedding(1000, 192)\n",
       "      (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=192, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "root_path = \"/home/mai.kassem/Documents/Med-BERT/\"\n",
    "BertForSequenceClassification.from_pretrained(root_path + \"MedBERT_Pretraining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bert.embeddings.position_ids',\n",
       "              tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "                        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "                        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "                        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "                        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "                        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "                        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "                        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "                       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "                       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "                       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "                       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "                       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "                       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "                       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "                       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "                       224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "                       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "                       252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "                       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "                       280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "                       294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "                       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "                       322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "                       336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "                       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "                       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "                       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "                       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "                       406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "                       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "                       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "                       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
       "                       462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "                       476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "                       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
       "                       504, 505, 506, 507, 508, 509, 510, 511]])),\n",
       "             ('bert.embeddings.word_embeddings.weight',\n",
       "              tensor([[-0.0011,  0.0004, -0.0045,  ...,  0.0159, -0.0101,  0.0027],\n",
       "                      [-0.0006,  0.0035,  0.0629,  ..., -0.0453,  0.0473, -0.0018],\n",
       "                      [ 0.0705,  0.0396,  0.0785,  ...,  0.0846,  0.0274, -0.0472],\n",
       "                      ...,\n",
       "                      [-0.0316, -0.0742, -0.0694,  ..., -0.0674,  0.0521,  0.0679],\n",
       "                      [-0.0442, -0.0457, -0.0513,  ..., -0.0837,  0.0522,  0.0241],\n",
       "                      [-0.0279, -0.0400, -0.0354,  ..., -0.0921,  0.0450,  0.0153]])),\n",
       "             ('bert.embeddings.position_embeddings.weight',\n",
       "              tensor([[ 9.3884e-04,  2.0054e-02,  3.6423e-03,  ..., -6.4174e-03,\n",
       "                        1.6820e-02, -1.5715e-02],\n",
       "                      [-8.4210e-04,  7.8851e-03,  2.9606e-05,  ..., -8.2686e-03,\n",
       "                        2.5066e-02, -1.0212e-02],\n",
       "                      [ 3.4766e-03,  3.1109e-03,  1.9220e-03,  ..., -3.6584e-03,\n",
       "                        2.3661e-02, -1.2941e-02],\n",
       "                      ...,\n",
       "                      [-2.4363e-02, -1.8279e-02, -1.6384e-03,  ...,  3.5928e-03,\n",
       "                       -1.8235e-02,  1.6383e-02],\n",
       "                      [-1.4871e-02, -1.4826e-02, -4.1375e-03,  ...,  2.2456e-02,\n",
       "                        7.9456e-03, -7.8619e-03],\n",
       "                      [-1.9159e-02,  7.4150e-03,  3.4810e-03,  ..., -1.7728e-02,\n",
       "                       -1.8190e-02, -3.3084e-02]])),\n",
       "             ('bert.embeddings.token_type_embeddings.weight',\n",
       "              tensor([[-2.4736e-02, -6.7489e-03,  7.2180e-05,  ..., -2.2455e-02,\n",
       "                       -3.0579e-03, -3.1690e-03],\n",
       "                      [ 3.0013e-03, -2.9438e-04,  8.8294e-03,  ...,  8.7520e-03,\n",
       "                       -7.3407e-03,  1.4424e-03],\n",
       "                      [ 4.1184e-03,  3.6665e-03, -1.1664e-02,  ...,  2.1012e-02,\n",
       "                       -3.0616e-03, -8.4772e-03],\n",
       "                      ...,\n",
       "                      [ 2.5341e-02, -2.0056e-02,  6.0744e-03,  ...,  3.4701e-04,\n",
       "                        1.3433e-02, -1.8149e-02],\n",
       "                      [ 2.5132e-02,  1.2827e-02, -1.8674e-02,  ..., -6.0113e-03,\n",
       "                       -2.2775e-02, -1.1294e-02],\n",
       "                      [ 3.4658e-02, -1.4957e-02, -2.2392e-02,  ...,  2.9001e-03,\n",
       "                        2.1782e-02,  1.4470e-02]])),\n",
       "             ('bert.embeddings.LayerNorm.weight',\n",
       "              tensor([1.0002, 1.0167, 0.9905, 1.0062, 1.0070, 1.0214, 1.0089, 1.0177, 1.0036,\n",
       "                      1.0036, 1.0241, 0.9807, 0.9888, 1.0042, 1.0040, 1.0144, 1.0141, 0.9900,\n",
       "                      1.0166, 1.0202, 0.9783, 1.0109, 1.0094, 1.0078, 1.0175, 1.0173, 1.0003,\n",
       "                      0.9992, 1.0294, 0.9914, 0.9992, 1.0112, 0.9842, 1.0089, 1.0037, 1.0095,\n",
       "                      1.0126, 1.0157, 1.0295, 1.0023, 0.9982, 1.0042, 1.0019, 1.0111, 0.9856,\n",
       "                      1.0123, 0.9838, 1.0011, 1.0091, 1.0057, 0.9934, 1.0117, 1.0256, 0.9944,\n",
       "                      1.0194, 1.0150, 1.0110, 1.0191, 0.9940, 0.9983, 1.0138, 1.0134, 1.0098,\n",
       "                      1.0347, 1.0075, 0.9976, 1.0205, 1.0369, 1.0044, 1.0162, 0.9834, 0.9923,\n",
       "                      0.9923, 0.9948, 1.0145, 1.0185, 0.9950, 1.0052, 0.9978, 0.9988, 1.0025,\n",
       "                      0.9799, 1.0164, 1.0137, 1.0026, 1.0197, 0.9874, 1.0101, 1.0040, 1.0074,\n",
       "                      0.9881, 1.0389, 0.9992, 1.0081, 1.0237, 1.0228, 1.0068, 1.0199, 1.0065,\n",
       "                      1.0084, 0.9984, 1.0044, 1.0061, 1.0142, 1.0040, 1.0072, 0.9878, 1.0292,\n",
       "                      1.0231, 1.0099, 0.9934, 1.0197, 1.0026, 1.0353, 1.0818, 1.0145, 1.0150,\n",
       "                      1.0034, 0.9808, 1.0014, 1.0324, 1.0179, 0.9975, 1.0053, 0.9891, 0.9904,\n",
       "                      1.0114, 1.0095, 0.9900, 0.9933, 1.0242, 1.0294, 1.0107, 1.0014, 0.9960,\n",
       "                      1.0302, 1.0328, 1.0101, 1.0137, 1.0222, 1.0076, 1.0131, 1.0058, 1.0018,\n",
       "                      1.0092, 1.0002, 1.0275, 1.0190, 1.0172, 0.9979, 1.0160, 1.0346, 1.0174,\n",
       "                      1.0062, 1.0077, 1.0074, 1.0017, 1.0023, 1.0248, 1.0137, 1.0073, 0.9943,\n",
       "                      1.0307, 0.9999, 1.0108, 1.0260, 1.0011, 1.0082, 1.0064, 0.9985, 1.0244,\n",
       "                      0.9890, 0.9927, 0.9977, 1.0070, 1.0030, 1.0087, 1.0008, 1.0067, 1.0148,\n",
       "                      1.0479, 1.0126, 1.0246, 1.0074, 1.0018, 1.0183, 1.0154, 1.0077, 1.0327,\n",
       "                      1.0010, 1.0155, 1.0117])),\n",
       "             ('bert.embeddings.LayerNorm.bias',\n",
       "              tensor([ 1.0130e-02,  8.4799e-03,  1.9876e-02,  1.8879e-02, -3.2263e-03,\n",
       "                      -3.2475e-02, -3.0300e-03, -7.3575e-03,  4.2456e-03, -1.0752e-04,\n",
       "                       2.7130e-02,  2.3462e-02,  1.4520e-02, -1.1518e-03, -2.9247e-02,\n",
       "                      -9.8584e-03, -1.6170e-02, -3.4927e-02,  1.2359e-02, -1.9967e-02,\n",
       "                       2.7630e-02, -2.0359e-02, -5.1176e-03,  7.9096e-03, -2.1885e-02,\n",
       "                       1.0464e-02,  1.5008e-02, -7.0709e-03,  6.7204e-03,  1.6219e-02,\n",
       "                      -1.5073e-02, -1.7574e-03,  2.8149e-02,  5.2511e-03,  8.3892e-03,\n",
       "                      -3.3542e-03, -1.5894e-02,  1.1754e-02, -9.0678e-03,  1.5069e-02,\n",
       "                      -1.2864e-02, -6.5148e-03, -1.2674e-02, -1.3521e-02,  9.1207e-03,\n",
       "                      -6.4303e-03,  1.4082e-02, -2.6921e-02, -4.4425e-03, -3.9540e-03,\n",
       "                       1.9316e-02, -1.8818e-03,  3.1660e-02,  1.8838e-02,  3.9563e-03,\n",
       "                       1.7328e-03, -4.0410e-03, -8.8783e-03,  1.1411e-02,  6.3740e-03,\n",
       "                       1.5084e-02,  3.4976e-03, -1.8638e-03, -1.1935e-02,  2.4509e-02,\n",
       "                      -5.2593e-03, -1.0887e-02, -1.1968e-02,  1.3031e-02,  2.7713e-03,\n",
       "                       1.2650e-02,  2.7056e-02,  1.4068e-02,  6.1304e-03, -1.4987e-02,\n",
       "                      -1.1806e-02, -1.9014e-02, -8.0227e-03, -7.1948e-05, -1.1240e-02,\n",
       "                       1.2937e-02,  3.7865e-02,  8.6800e-03, -3.2822e-03,  9.5636e-03,\n",
       "                       9.1999e-03,  7.2032e-03, -8.4579e-03, -2.9012e-02, -1.2621e-02,\n",
       "                       2.5122e-02, -2.3948e-02,  4.8700e-03,  2.4026e-02,  1.5113e-02,\n",
       "                      -2.4001e-03,  2.1075e-02, -1.4309e-02, -1.2829e-03,  9.0337e-03,\n",
       "                       2.1165e-02, -2.2734e-02, -3.0207e-02, -1.0320e-02,  7.3526e-03,\n",
       "                       6.4736e-03,  2.6652e-02, -1.9457e-02, -3.1551e-03,  1.0888e-03,\n",
       "                       2.4900e-02, -8.3311e-03, -2.0085e-02, -2.9346e-02,  3.2392e-02,\n",
       "                      -7.1085e-03,  1.7417e-02,  3.0163e-04,  1.1735e-02,  1.2737e-02,\n",
       "                       9.7878e-03, -1.2596e-02,  6.5754e-03, -6.8883e-03, -6.9918e-03,\n",
       "                       1.5063e-02, -3.4436e-04,  1.0457e-02,  5.1713e-03, -1.5299e-03,\n",
       "                      -1.2024e-02, -8.3386e-03, -6.2699e-03, -3.1304e-03,  1.9407e-03,\n",
       "                      -1.1990e-02, -4.9779e-03,  6.1188e-03,  3.6706e-02, -8.2020e-03,\n",
       "                       3.2623e-02,  7.3272e-03, -3.2361e-02, -6.7002e-03,  2.0427e-02,\n",
       "                       2.8486e-02, -1.4819e-02, -2.1197e-02, -3.9605e-03, -1.9034e-02,\n",
       "                      -2.1399e-02, -1.9218e-02, -6.2479e-03, -1.3726e-02,  3.9683e-02,\n",
       "                      -1.5759e-02, -1.0924e-02,  1.9010e-02, -2.0322e-02,  5.3309e-03,\n",
       "                      -4.6366e-03,  4.0333e-02, -1.2688e-02, -2.3206e-02,  1.6870e-02,\n",
       "                      -9.2558e-03,  1.1702e-02, -3.3760e-02,  5.5469e-03,  1.9802e-02,\n",
       "                      -1.0114e-02,  2.1604e-02,  3.4144e-03,  2.3347e-02,  8.1407e-03,\n",
       "                       3.2627e-02, -2.9002e-02,  6.2143e-02, -1.8841e-02, -1.9688e-02,\n",
       "                      -2.9187e-02, -1.0048e-02, -8.9351e-03,  2.0898e-02, -3.1334e-02,\n",
       "                      -7.1842e-03, -2.1740e-02, -2.4021e-02, -1.4344e-02,  1.1811e-02,\n",
       "                      -1.6421e-02, -3.5197e-03])),\n",
       "             ('bert.encoder.layer.0.attention.self.query.weight',\n",
       "              tensor([[-0.0146, -0.0120, -0.0004,  ...,  0.0223, -0.0160, -0.0020],\n",
       "                      [-0.0395,  0.0171,  0.0405,  ...,  0.0027,  0.1093, -0.0177],\n",
       "                      [-0.0242,  0.0049, -0.0070,  ...,  0.0052,  0.0232, -0.0217],\n",
       "                      ...,\n",
       "                      [-0.0252, -0.0567,  0.0151,  ..., -0.0369, -0.0490,  0.0114],\n",
       "                      [ 0.0445,  0.0097, -0.0139,  ...,  0.0037, -0.0252, -0.0109],\n",
       "                      [ 0.0025,  0.0147, -0.0053,  ...,  0.0043,  0.0530, -0.0305]])),\n",
       "             ('bert.encoder.layer.0.attention.self.query.bias',\n",
       "              tensor([-0.0428,  0.0740,  0.0412, -0.0738, -0.0211,  0.0459, -0.0192, -0.0243,\n",
       "                       0.0680, -0.0137,  0.0232,  0.0574,  0.0614, -0.0451, -0.0639, -0.0432,\n",
       "                       0.0192, -0.0326, -0.0348,  0.0573,  0.0241,  0.0326,  0.0616, -0.0363,\n",
       "                       0.0063,  0.0083, -0.0353,  0.0196,  0.0182,  0.0283, -0.0475, -0.0201,\n",
       "                       0.0169,  0.0586,  0.0584,  0.0550, -0.0693, -0.0182,  0.0498, -0.0323,\n",
       "                       0.0559, -0.0305,  0.0589,  0.0160,  0.0503, -0.0127,  0.0675, -0.0188,\n",
       "                      -0.0721,  0.0651, -0.0534,  0.0670,  0.0175, -0.0307, -0.0425, -0.0134,\n",
       "                      -0.0341,  0.0310, -0.0298,  0.0169, -0.0355,  0.0423, -0.0325,  0.0585,\n",
       "                      -0.0467, -0.0187, -0.0614,  0.0169, -0.0002, -0.0470, -0.0155,  0.0642,\n",
       "                       0.0464,  0.0321, -0.0516, -0.0003,  0.0002, -0.0371, -0.0259,  0.0458,\n",
       "                      -0.0050, -0.0438, -0.0486, -0.0354, -0.0078,  0.0037, -0.0268,  0.0009,\n",
       "                      -0.0666,  0.0482,  0.0107,  0.0577, -0.0449, -0.0398,  0.0522,  0.0236,\n",
       "                      -0.0466, -0.0658,  0.0689, -0.0375, -0.0458,  0.0266,  0.0750, -0.0840,\n",
       "                       0.0654,  0.0385,  0.0868,  0.0632, -0.0759, -0.1023, -0.0584, -0.0167,\n",
       "                       0.0416, -0.0626,  0.0337,  0.0439,  0.0592,  0.0648,  0.0140, -0.0570,\n",
       "                      -0.0688, -0.0153, -0.0595,  0.0527, -0.0667,  0.0652,  0.0563, -0.0231,\n",
       "                       0.0171, -0.0406,  0.0269, -0.0527, -0.0034,  0.0286, -0.0187, -0.0207,\n",
       "                      -0.0218,  0.0334,  0.0261, -0.0037, -0.0236, -0.0226,  0.0204,  0.0530,\n",
       "                      -0.0228,  0.0139,  0.0153,  0.0043, -0.0306, -0.0221,  0.0117,  0.0045,\n",
       "                      -0.0173, -0.0224, -0.0150,  0.0253, -0.0075, -0.0240, -0.0197,  0.0070,\n",
       "                       0.0400,  0.0397, -0.0395, -0.0386, -0.0043,  0.0757, -0.0112, -0.0214,\n",
       "                       0.0288,  0.0664, -0.0677, -0.0390,  0.0160,  0.0507,  0.0555,  0.0408,\n",
       "                       0.0519,  0.0415,  0.0077, -0.0700,  0.0470, -0.0517,  0.0546, -0.0368,\n",
       "                      -0.0457,  0.0602,  0.0561,  0.0596,  0.0119, -0.0317,  0.0390, -0.0091])),\n",
       "             ('bert.encoder.layer.0.attention.self.key.weight',\n",
       "              tensor([[ 0.0140,  0.0355,  0.0155,  ...,  0.0163, -0.0443,  0.0092],\n",
       "                      [-0.0222,  0.0018, -0.0038,  ...,  0.0287,  0.0293, -0.0229],\n",
       "                      [-0.0044, -0.0062, -0.0341,  ..., -0.0040,  0.0162, -0.0398],\n",
       "                      ...,\n",
       "                      [ 0.0032, -0.0406,  0.0156,  ...,  0.0127,  0.0166,  0.0443],\n",
       "                      [-0.0330, -0.0238,  0.0055,  ...,  0.0335,  0.0180, -0.0643],\n",
       "                      [-0.0203,  0.0020, -0.0168,  ..., -0.0002, -0.0081,  0.0381]])),\n",
       "             ('bert.encoder.layer.0.attention.self.key.bias',\n",
       "              tensor([-2.9545e-08, -1.9508e-07, -1.7178e-07,  4.5495e-08,  1.3572e-08,\n",
       "                      -6.6046e-08,  1.0050e-07,  2.3379e-08, -1.0108e-07, -1.2992e-07,\n",
       "                      -6.9635e-09, -7.1436e-08,  1.0555e-09,  2.3343e-07,  7.6871e-08,\n",
       "                       9.2470e-08, -1.6006e-07,  7.3145e-08,  2.7720e-08, -5.3340e-08,\n",
       "                      -1.3707e-07, -1.2385e-07, -2.7170e-07, -3.7785e-08, -1.5469e-07,\n",
       "                      -6.7036e-08,  1.4762e-07,  2.7799e-08,  2.4864e-08, -7.7311e-08,\n",
       "                       8.2026e-08,  8.4656e-08,  8.6643e-08,  8.8632e-08, -5.5513e-08,\n",
       "                      -4.6821e-08,  1.6954e-09,  5.6272e-08, -1.0811e-07,  6.2012e-08,\n",
       "                      -6.6957e-08,  1.2976e-07,  2.7189e-08, -1.3257e-07, -1.4682e-07,\n",
       "                      -1.1405e-07,  2.5858e-08,  2.4174e-08, -2.6696e-08,  8.5207e-08,\n",
       "                      -1.1237e-07, -1.0193e-07, -1.1746e-08, -1.9723e-07,  1.2740e-07,\n",
       "                      -1.2830e-07, -1.0731e-08,  1.5801e-07, -1.3284e-09, -7.9189e-08,\n",
       "                      -5.6228e-08, -2.6088e-08, -1.0017e-07,  3.5201e-08,  3.8501e-08,\n",
       "                       5.4457e-08, -5.9075e-08, -9.7431e-08,  2.6468e-08,  1.4119e-07,\n",
       "                      -1.5851e-08,  6.9578e-08, -4.6022e-08, -1.3375e-07, -1.9081e-08,\n",
       "                       3.2909e-08, -7.6652e-09, -2.2305e-08, -3.0405e-08, -1.3052e-07,\n",
       "                       3.9697e-08,  1.1839e-07,  1.9927e-07,  1.7566e-07, -3.8784e-08,\n",
       "                      -2.7878e-08,  2.2483e-08,  2.9832e-08,  4.4272e-08, -1.4010e-07,\n",
       "                      -2.0480e-09, -7.0118e-09,  1.1869e-07,  9.5602e-08,  4.5227e-08,\n",
       "                      -7.7080e-08,  5.2662e-08,  1.5387e-09, -3.2007e-08,  5.2671e-08,\n",
       "                       4.7324e-08, -2.0240e-08, -5.9621e-08,  4.3165e-08, -9.0670e-08,\n",
       "                      -5.2477e-08, -1.2281e-07, -1.1840e-07,  8.0979e-08,  1.0270e-07,\n",
       "                       7.8041e-08, -2.9018e-08, -1.6998e-07,  4.7705e-08,  1.9684e-10,\n",
       "                      -7.1929e-08,  2.5367e-10, -9.2966e-08,  1.0047e-07,  1.2396e-07,\n",
       "                       8.4610e-08,  4.2382e-08,  1.2451e-07, -6.0107e-08,  7.5708e-08,\n",
       "                       9.6407e-10, -1.3498e-07,  7.3421e-08,  1.6389e-08,  3.5940e-08,\n",
       "                       1.5942e-07, -1.0870e-07, -1.0022e-07,  1.2125e-07, -1.1187e-07,\n",
       "                      -3.7305e-09, -1.0169e-07, -5.1799e-08,  6.3340e-08, -7.7639e-08,\n",
       "                       7.1511e-08, -5.7020e-08,  6.8388e-08, -1.4039e-07, -1.2703e-07,\n",
       "                       5.3213e-08,  7.7106e-08,  1.2569e-07, -5.9355e-08, -8.4036e-08,\n",
       "                       2.6689e-08,  5.6750e-08, -1.5022e-07, -7.3120e-08, -1.9098e-07,\n",
       "                       4.2111e-08, -1.9775e-08, -7.6724e-08,  7.3242e-10,  3.1431e-08,\n",
       "                       1.2860e-07,  3.3226e-08,  8.3187e-09, -3.9881e-08, -1.7554e-08,\n",
       "                       1.7711e-07,  9.7647e-08,  1.8212e-08, -5.3477e-08,  1.1821e-07,\n",
       "                      -8.9034e-08, -8.2883e-08,  5.6058e-08,  1.0693e-08,  5.1479e-08,\n",
       "                       4.7238e-08,  1.4653e-07,  1.8129e-08, -1.1242e-08,  7.1900e-09,\n",
       "                      -1.2331e-07, -9.1020e-08,  4.0720e-08, -6.0292e-08, -1.1274e-07,\n",
       "                       5.0349e-08,  8.2432e-08,  1.3481e-08,  1.3545e-07, -1.5549e-07,\n",
       "                       1.1066e-07,  9.9229e-08])),\n",
       "             ('bert.encoder.layer.0.attention.self.value.weight',\n",
       "              tensor([[ 0.0154,  0.0072,  0.0412,  ...,  0.0205,  0.0419,  0.0097],\n",
       "                      [ 0.0054, -0.0011,  0.0059,  ...,  0.0194, -0.0109,  0.0412],\n",
       "                      [ 0.0028,  0.0081,  0.0213,  ...,  0.0123,  0.0051,  0.0101],\n",
       "                      ...,\n",
       "                      [ 0.0134, -0.0203, -0.0012,  ..., -0.0190, -0.0040,  0.0272],\n",
       "                      [ 0.0256,  0.0260, -0.0395,  ...,  0.0342, -0.0082, -0.0063],\n",
       "                      [ 0.0421, -0.0191,  0.0165,  ...,  0.0072, -0.0283, -0.0261]])),\n",
       "             ('bert.encoder.layer.0.attention.self.value.bias',\n",
       "              tensor([ 5.0083e-03,  5.8359e-03,  6.5707e-03,  1.5994e-02,  9.1555e-03,\n",
       "                       3.2765e-03, -1.1497e-02, -1.5580e-02,  6.7473e-03,  1.0182e-02,\n",
       "                       8.9450e-03, -1.0520e-02, -4.4791e-03,  7.8413e-03, -1.4671e-03,\n",
       "                      -8.8981e-03, -1.2277e-02,  5.1235e-03, -4.2740e-03,  4.4258e-03,\n",
       "                       5.0266e-03,  3.1391e-03, -5.4741e-03, -2.7532e-04,  8.4733e-03,\n",
       "                      -8.2478e-04,  1.2761e-02,  1.2217e-02,  1.2801e-02, -5.8248e-03,\n",
       "                       2.1602e-02,  1.4712e-02,  3.3526e-03, -9.0076e-03, -4.3722e-03,\n",
       "                       1.1011e-02, -8.7456e-03,  4.1867e-03,  4.7792e-03, -3.0058e-03,\n",
       "                      -3.1627e-04,  1.1291e-02, -6.0462e-03,  1.2974e-02,  8.0424e-03,\n",
       "                       4.4962e-04,  1.2661e-02, -4.0880e-03,  1.7148e-03,  1.6344e-04,\n",
       "                       7.6688e-03,  3.5623e-03, -9.8303e-03,  5.4261e-03, -1.0945e-02,\n",
       "                       2.1472e-03,  4.1791e-03,  2.1111e-03,  1.1822e-02, -2.9739e-03,\n",
       "                       8.6600e-04,  4.2018e-03, -3.8564e-03,  1.2044e-02, -1.0919e-02,\n",
       "                      -6.2001e-03, -1.4304e-04,  4.3416e-03,  5.3599e-03,  1.9238e-03,\n",
       "                      -5.2399e-03, -1.4696e-03,  5.5511e-03,  2.0652e-03,  1.1619e-02,\n",
       "                      -9.1772e-04,  9.7527e-03, -1.5364e-02,  2.0226e-04,  4.9789e-03,\n",
       "                       3.7031e-04, -1.5283e-02,  4.7359e-03, -7.7539e-03, -4.9568e-03,\n",
       "                      -3.9149e-03, -1.0752e-03,  6.8383e-03,  3.9491e-03, -1.4029e-03,\n",
       "                      -2.6259e-03, -7.2083e-03, -1.2051e-02,  1.1254e-03,  4.2777e-03,\n",
       "                       7.0655e-03, -3.1114e-03,  8.1747e-03, -5.6765e-03, -1.3923e-03,\n",
       "                      -1.0985e-02,  1.4763e-03, -7.0217e-03,  1.6183e-03, -2.8864e-03,\n",
       "                       8.4335e-04,  2.6119e-03, -7.9610e-03,  8.9359e-03,  3.9434e-03,\n",
       "                      -2.9994e-03,  1.0322e-02,  7.8549e-05, -8.5570e-03,  7.6898e-03,\n",
       "                       3.5967e-04, -5.5557e-03, -2.4785e-03, -4.9161e-03, -3.9916e-03,\n",
       "                      -4.1541e-03,  1.0524e-02, -9.3542e-03,  4.4458e-03,  4.7859e-03,\n",
       "                       3.1754e-03, -6.5178e-03, -1.2002e-02,  3.4616e-03,  5.8277e-03,\n",
       "                      -2.3105e-03, -9.2842e-03, -6.2936e-03, -4.5358e-03,  1.2782e-02,\n",
       "                       1.2022e-02, -1.1046e-03, -4.9033e-03, -1.5939e-02,  6.1194e-03,\n",
       "                       5.9389e-03, -2.1770e-02,  1.0917e-02,  3.3752e-03, -3.5155e-03,\n",
       "                       8.1641e-03,  4.9548e-03, -3.1105e-03, -1.6523e-03,  3.7500e-03,\n",
       "                       2.1566e-03, -1.3271e-02,  6.1276e-03,  3.9082e-03, -1.3469e-02,\n",
       "                       1.1256e-02, -1.4912e-03, -8.0701e-03, -1.0625e-02,  6.5804e-03,\n",
       "                      -1.2196e-02, -3.2427e-03, -7.7879e-03,  2.4509e-03,  5.5704e-03,\n",
       "                       3.5686e-03,  1.3994e-02,  7.1118e-04,  6.7893e-03, -5.6636e-04,\n",
       "                      -9.3237e-03,  3.3044e-03,  7.3908e-03,  4.9750e-04, -1.0537e-03,\n",
       "                      -2.7962e-03, -3.4294e-03, -5.8778e-04,  7.1919e-04,  4.0085e-04,\n",
       "                      -1.7178e-04, -2.2694e-03, -9.4397e-03, -2.1710e-03, -1.5121e-02,\n",
       "                      -4.4573e-04, -1.0544e-02,  3.0793e-03, -1.8314e-02,  3.1052e-03,\n",
       "                      -7.1443e-04,  1.2549e-02])),\n",
       "             ('bert.encoder.layer.0.attention.output.dense.weight',\n",
       "              tensor([[ 0.0200,  0.0319, -0.0070,  ...,  0.0012, -0.0045, -0.0088],\n",
       "                      [ 0.0043, -0.0196, -0.0098,  ..., -0.0061,  0.0034, -0.0176],\n",
       "                      [ 0.0002, -0.0133, -0.0164,  ...,  0.0142,  0.0597,  0.0131],\n",
       "                      ...,\n",
       "                      [-0.0080, -0.0182, -0.0165,  ..., -0.0580,  0.0150,  0.0446],\n",
       "                      [-0.0200, -0.0298,  0.0070,  ..., -0.0163, -0.0200,  0.0133],\n",
       "                      [-0.0035,  0.0094, -0.0020,  ..., -0.0170, -0.0131,  0.0050]])),\n",
       "             ('bert.encoder.layer.0.attention.output.dense.bias',\n",
       "              tensor([ 3.8116e-03,  1.6772e-03,  3.6864e-03,  6.0624e-03, -1.8587e-03,\n",
       "                       2.1028e-03, -7.1036e-03, -1.5084e-03, -1.1517e-02, -1.0922e-02,\n",
       "                      -7.4636e-03,  4.5996e-03,  4.1747e-03, -7.2991e-03,  1.0957e-04,\n",
       "                       1.5912e-03,  2.9980e-03, -6.3410e-03, -2.3513e-03, -7.0422e-04,\n",
       "                       1.5718e-02, -4.9201e-03,  8.5015e-04,  5.4174e-03, -1.3627e-02,\n",
       "                       5.2632e-03,  2.8545e-03,  9.0363e-03, -9.5040e-03,  2.8214e-03,\n",
       "                      -7.8659e-03,  6.2011e-03,  1.7814e-03,  4.5812e-03, -7.1022e-03,\n",
       "                       5.1632e-03, -8.6919e-03, -1.2589e-02, -2.1590e-03,  4.7765e-03,\n",
       "                      -6.9248e-03, -7.0326e-03, -1.1617e-02, -4.8208e-03,  6.9228e-03,\n",
       "                      -2.2612e-03,  2.0899e-02,  4.5084e-03,  1.4595e-04, -1.4365e-03,\n",
       "                       2.4001e-03, -5.6992e-03, -1.4706e-02,  1.1284e-02,  1.8941e-03,\n",
       "                       5.1246e-03,  8.6898e-04, -2.0549e-03,  5.1207e-03,  8.1164e-03,\n",
       "                       4.7232e-03, -7.8498e-04,  4.0585e-03,  2.6443e-03,  5.1776e-03,\n",
       "                       5.7047e-03,  3.4301e-03,  1.2685e-02,  6.5702e-03,  6.1940e-04,\n",
       "                       5.0424e-03,  1.9678e-03,  6.7522e-03, -3.8054e-03, -1.9767e-03,\n",
       "                      -4.7989e-03, -2.3487e-03,  5.3003e-03, -2.5540e-03, -1.0238e-02,\n",
       "                      -2.1291e-03,  8.3706e-04,  6.5571e-03, -3.2670e-04,  1.8221e-04,\n",
       "                       2.2832e-03, -4.2383e-03, -1.4147e-03, -5.3800e-03,  5.8182e-03,\n",
       "                       5.3538e-03, -1.2189e-03, -8.5157e-03, -1.1553e-04,  1.5130e-03,\n",
       "                       3.0267e-02, -1.4447e-03,  8.1171e-03, -1.3098e-03, -1.4188e-03,\n",
       "                       3.3962e-03, -7.6795e-03, -2.6118e-03, -4.5024e-03, -5.5634e-03,\n",
       "                      -5.8212e-03,  5.4786e-03, -2.0573e-03,  1.8611e-03,  9.3079e-04,\n",
       "                      -5.7703e-04, -4.8242e-03, -3.8686e-03,  1.0114e-03, -2.8165e-02,\n",
       "                      -2.1967e-04, -7.9361e-04,  7.9586e-03,  5.9469e-03,  4.2406e-04,\n",
       "                      -9.1166e-03,  1.6575e-02,  1.1164e-03,  2.5816e-04, -2.6649e-03,\n",
       "                       5.1842e-03,  2.3967e-03,  4.9339e-03,  3.5957e-03,  6.5036e-03,\n",
       "                       7.9471e-03,  2.4307e-03, -4.6311e-03,  7.3201e-03, -6.7374e-04,\n",
       "                      -3.0009e-03, -9.7659e-03,  5.7855e-03, -8.6138e-03, -2.1025e-04,\n",
       "                       4.2356e-03,  1.2738e-02,  1.2077e-02, -3.7144e-04, -2.1941e-03,\n",
       "                       1.2417e-02,  5.9513e-05, -7.9490e-03,  3.1560e-03, -7.7480e-04,\n",
       "                      -7.6653e-04,  7.7733e-03, -6.0632e-04,  2.0302e-03,  9.5968e-03,\n",
       "                       6.8806e-03, -9.3536e-03,  7.0449e-03, -1.6450e-02, -1.2836e-02,\n",
       "                      -3.2430e-03,  6.0580e-04,  9.3338e-04,  3.0145e-03,  1.7073e-02,\n",
       "                      -1.4646e-02, -3.6076e-03, -1.8465e-03,  5.5727e-04,  4.6625e-03,\n",
       "                       2.6285e-04,  8.5987e-03,  4.3289e-05, -2.0645e-03, -1.7924e-03,\n",
       "                       4.1412e-03, -7.8518e-03, -1.4680e-02, -1.5473e-03,  1.6101e-03,\n",
       "                       3.0427e-02,  1.2741e-03,  3.0907e-03,  8.5666e-03, -7.0522e-03,\n",
       "                      -4.2761e-03, -2.2347e-03, -7.3432e-03,  6.6824e-04,  1.8420e-02,\n",
       "                      -8.4795e-03,  4.5697e-03])),\n",
       "             ('bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9925, 1.0084, 0.9867, 0.9971, 0.9981, 1.0091, 1.0009, 1.0141, 0.9895,\n",
       "                      0.9979, 1.0210, 0.9819, 0.9804, 0.9984, 0.9889, 1.0045, 1.0098, 0.9789,\n",
       "                      1.0041, 1.0094, 0.9775, 1.0055, 1.0037, 1.0030, 1.0056, 1.0108, 0.9903,\n",
       "                      0.9934, 1.0235, 0.9878, 0.9850, 1.0124, 0.9763, 1.0043, 0.9984, 1.0050,\n",
       "                      1.0061, 1.0141, 1.0181, 0.9968, 0.9904, 0.9975, 1.0006, 1.0010, 0.9832,\n",
       "                      1.0102, 0.9804, 0.9992, 1.0095, 1.0019, 0.9929, 1.0046, 1.0183, 0.9881,\n",
       "                      1.0087, 1.0108, 1.0034, 1.0192, 0.9927, 0.9929, 1.0049, 1.0054, 1.0087,\n",
       "                      1.0204, 0.9935, 0.9894, 1.0073, 1.0327, 0.9943, 1.0107, 0.9777, 0.9813,\n",
       "                      0.9848, 0.9891, 1.0038, 1.0108, 0.9877, 0.9967, 0.9859, 0.9964, 0.9942,\n",
       "                      0.9729, 1.0075, 1.0025, 0.9992, 1.0118, 0.9862, 1.0012, 0.9914, 0.9992,\n",
       "                      0.9803, 1.0273, 0.9919, 0.9992, 1.0131, 1.0193, 1.0009, 1.0160, 0.9995,\n",
       "                      1.0039, 0.9900, 0.9993, 0.9969, 1.0051, 0.9991, 1.0007, 0.9831, 1.0177,\n",
       "                      1.0114, 1.0088, 0.9893, 1.0098, 0.9955, 1.0291, 1.0814, 1.0069, 1.0102,\n",
       "                      0.9997, 0.9746, 1.0019, 1.0203, 1.0028, 0.9918, 0.9966, 0.9854, 0.9953,\n",
       "                      0.9978, 1.0033, 0.9799, 0.9873, 1.0207, 1.0168, 1.0001, 0.9944, 0.9925,\n",
       "                      1.0179, 1.0199, 1.0069, 1.0012, 1.0070, 0.9964, 1.0116, 0.9991, 0.9938,\n",
       "                      1.0058, 0.9993, 1.0177, 1.0070, 1.0086, 0.9915, 1.0112, 1.0334, 1.0042,\n",
       "                      1.0010, 0.9980, 1.0008, 0.9918, 0.9945, 1.0071, 1.0036, 0.9992, 0.9842,\n",
       "                      1.0209, 0.9958, 0.9972, 1.0150, 0.9843, 1.0032, 1.0015, 0.9853, 1.0124,\n",
       "                      0.9703, 0.9929, 0.9885, 1.0012, 0.9931, 0.9992, 0.9987, 1.0050, 1.0089,\n",
       "                      1.0459, 1.0073, 1.0178, 1.0022, 0.9988, 1.0096, 1.0119, 0.9976, 1.0180,\n",
       "                      0.9995, 1.0110, 1.0014])),\n",
       "             ('bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       "              tensor([ 0.0011,  0.0092,  0.0136,  0.0015, -0.0048,  0.0047,  0.0008, -0.0063,\n",
       "                      -0.0088, -0.0088, -0.0133,  0.0074,  0.0136, -0.0102, -0.0015, -0.0056,\n",
       "                       0.0105, -0.0073, -0.0072,  0.0021,  0.0271, -0.0016, -0.0029,  0.0160,\n",
       "                      -0.0210,  0.0023,  0.0045,  0.0195, -0.0145,  0.0055, -0.0030,  0.0040,\n",
       "                       0.0034,  0.0023, -0.0015,  0.0075, -0.0144, -0.0169, -0.0040,  0.0020,\n",
       "                      -0.0097, -0.0086, -0.0054, -0.0023,  0.0055,  0.0006,  0.0362, -0.0031,\n",
       "                      -0.0022,  0.0021,  0.0011, -0.0221, -0.0204,  0.0163,  0.0032,  0.0021,\n",
       "                      -0.0036,  0.0046,  0.0026,  0.0100, -0.0025, -0.0033,  0.0079, -0.0096,\n",
       "                      -0.0015,  0.0044, -0.0032,  0.0239,  0.0049,  0.0051,  0.0092, -0.0007,\n",
       "                       0.0091,  0.0050, -0.0006, -0.0043, -0.0049,  0.0089,  0.0003, -0.0072,\n",
       "                       0.0089,  0.0022, -0.0012, -0.0048,  0.0036, -0.0057, -0.0013, -0.0102,\n",
       "                      -0.0084,  0.0045,  0.0080,  0.0103, -0.0046, -0.0062, -0.0069,  0.0430,\n",
       "                      -0.0042,  0.0133, -0.0008, -0.0071,  0.0008, -0.0032, -0.0065, -0.0074,\n",
       "                      -0.0111,  0.0017,  0.0087,  0.0023, -0.0016,  0.0059, -0.0078,  0.0006,\n",
       "                       0.0005,  0.0019, -0.0300,  0.0061,  0.0074,  0.0087,  0.0189,  0.0010,\n",
       "                      -0.0171,  0.0239,  0.0045,  0.0017, -0.0095,  0.0040,  0.0076,  0.0143,\n",
       "                       0.0081,  0.0066,  0.0146,  0.0071, -0.0116,  0.0130, -0.0061, -0.0029,\n",
       "                      -0.0219,  0.0146, -0.0113,  0.0035,  0.0032,  0.0085,  0.0121, -0.0020,\n",
       "                      -0.0076,  0.0052,  0.0052, -0.0051,  0.0059, -0.0042, -0.0006,  0.0230,\n",
       "                      -0.0090, -0.0011,  0.0173,  0.0034, -0.0161,  0.0147, -0.0219, -0.0240,\n",
       "                      -0.0061, -0.0038, -0.0052, -0.0073,  0.0155, -0.0084,  0.0082, -0.0058,\n",
       "                       0.0013, -0.0008, -0.0137,  0.0109, -0.0082, -0.0046, -0.0052,  0.0163,\n",
       "                      -0.0177, -0.0220,  0.0018,  0.0059,  0.0433, -0.0034, -0.0056, -0.0026,\n",
       "                      -0.0117,  0.0008, -0.0108, -0.0127,  0.0091,  0.0303, -0.0125, -0.0051])),\n",
       "             ('bert.encoder.layer.0.intermediate.dense.weight',\n",
       "              tensor([[ 0.0105,  0.0292, -0.0311,  ..., -0.0180,  0.0316,  0.0179],\n",
       "                      [ 0.0126, -0.0294,  0.0054,  ..., -0.0362,  0.0027,  0.0272],\n",
       "                      [-0.0436,  0.0101, -0.0140,  ...,  0.0532,  0.0494,  0.0199],\n",
       "                      ...,\n",
       "                      [-0.0135, -0.0350, -0.0136,  ...,  0.0175,  0.0316,  0.0215],\n",
       "                      [-0.0331, -0.0293,  0.0069,  ..., -0.0034, -0.0197,  0.0216],\n",
       "                      [-0.0265, -0.0024, -0.0049,  ...,  0.0540,  0.0015,  0.0354]])),\n",
       "             ('bert.encoder.layer.0.intermediate.dense.bias',\n",
       "              tensor([-3.2676e-03,  1.5740e-02,  2.5168e-03, -4.4320e-03, -1.7908e-02,\n",
       "                      -1.4207e-02, -4.6492e-03,  2.5546e-03,  2.0954e-03,  6.6505e-03,\n",
       "                      -1.0801e-02, -1.7033e-02, -7.9235e-03,  1.3635e-03, -2.4826e-02,\n",
       "                      -2.1213e-02, -2.1545e-02, -2.1433e-02,  7.3253e-03,  2.7945e-03,\n",
       "                      -1.2160e-02,  4.2447e-04, -1.6700e-02, -1.4234e-03, -8.0554e-03,\n",
       "                       6.3586e-03, -2.8682e-03,  4.1546e-03,  1.5343e-03,  9.1187e-03,\n",
       "                       4.5775e-03,  1.7218e-05, -1.6978e-03,  7.4146e-03,  7.3831e-03,\n",
       "                       6.3125e-05,  1.2460e-04, -1.0196e-02, -1.9113e-02, -1.9991e-03,\n",
       "                      -9.4491e-03, -6.5111e-03, -4.6159e-03,  8.6926e-03,  2.5704e-02,\n",
       "                      -4.2394e-03, -1.0733e-02,  2.3712e-04, -1.3447e-02, -4.9313e-03,\n",
       "                       1.0035e-03, -3.1578e-03, -1.6162e-02, -2.1415e-02, -8.3641e-03,\n",
       "                       2.0528e-03, -1.8347e-02, -1.3777e-02, -3.1622e-03,  1.8576e-03,\n",
       "                      -1.7951e-02, -2.0124e-02, -1.1639e-02,  1.8948e-03])),\n",
       "             ('bert.encoder.layer.0.output.dense.weight',\n",
       "              tensor([[ 0.0007,  0.0242,  0.0201,  ..., -0.0191,  0.0343, -0.0077],\n",
       "                      [-0.0310,  0.0032, -0.0133,  ..., -0.0273,  0.0281, -0.0325],\n",
       "                      [-0.0139,  0.0295, -0.0226,  ..., -0.0182,  0.0285, -0.0051],\n",
       "                      ...,\n",
       "                      [ 0.0010, -0.0225, -0.0305,  ..., -0.0096,  0.0427, -0.0481],\n",
       "                      [-0.0113,  0.0010, -0.0067,  ..., -0.0075, -0.0039,  0.0244],\n",
       "                      [-0.0237, -0.0203, -0.0437,  ...,  0.0190,  0.0215,  0.0205]])),\n",
       "             ('bert.encoder.layer.0.output.dense.bias',\n",
       "              tensor([-1.1541e-03,  2.8083e-03,  8.4689e-03,  7.8023e-03, -3.1392e-03,\n",
       "                      -7.4482e-04,  1.6012e-03, -5.9817e-03, -6.1237e-04, -2.9313e-03,\n",
       "                      -8.0040e-03,  8.5565e-03, -1.4522e-04,  5.2200e-03,  4.1833e-03,\n",
       "                      -5.8069e-03,  8.2707e-03, -1.8176e-03, -3.2056e-03, -1.6719e-03,\n",
       "                       3.7285e-03, -3.8127e-03,  4.4154e-03,  6.2890e-04, -1.2367e-02,\n",
       "                      -4.6936e-04,  7.2805e-03,  7.1557e-03, -4.7719e-03,  2.7404e-03,\n",
       "                      -6.2292e-04,  1.5635e-03,  4.5201e-03, -5.8806e-03, -4.6220e-03,\n",
       "                      -2.1767e-03, -5.7005e-03, -1.2770e-02, -5.2035e-03,  4.6032e-03,\n",
       "                      -2.3275e-03, -1.5994e-02, -2.8904e-03, -1.9153e-03,  3.8457e-03,\n",
       "                      -4.2325e-03,  1.0140e-02,  6.4553e-03, -3.9696e-03,  3.4662e-03,\n",
       "                      -5.3907e-03, -6.9111e-03, -1.2481e-02,  3.3951e-03,  3.7496e-03,\n",
       "                       1.9912e-03, -5.9045e-03, -1.3115e-02,  4.6206e-03,  7.5099e-03,\n",
       "                      -6.2640e-03,  9.1977e-04,  6.4330e-03, -4.5354e-03,  9.0350e-03,\n",
       "                       6.5401e-03, -2.8602e-03,  1.4057e-02,  2.7492e-03,  7.4691e-04,\n",
       "                       1.4137e-03,  1.9437e-03, -9.8289e-03,  7.1227e-03, -1.2035e-03,\n",
       "                       2.1392e-03, -8.2753e-04,  8.6093e-03,  8.8582e-04, -2.6926e-03,\n",
       "                       3.2863e-03,  2.8492e-03,  3.5852e-03, -6.0705e-03,  7.4491e-03,\n",
       "                      -4.0605e-03, -7.8663e-03, -2.8296e-03, -2.2653e-03,  2.4339e-03,\n",
       "                      -1.8622e-03,  4.5385e-03, -2.9594e-03,  3.7103e-03, -2.5929e-03,\n",
       "                       2.8032e-02, -3.9511e-04,  4.8739e-03, -3.3103e-03, -4.2394e-03,\n",
       "                       5.7444e-04, -2.0627e-03, -4.8808e-03, -2.9688e-03, -4.7696e-03,\n",
       "                      -3.6657e-03,  6.7958e-03, -8.4046e-05, -5.3713e-03,  8.6438e-03,\n",
       "                       3.5921e-03,  2.1478e-03,  5.2189e-03, -2.9879e-03, -2.4102e-02,\n",
       "                       5.6554e-03, -6.5150e-04,  7.7739e-03,  5.2723e-03,  1.9572e-03,\n",
       "                      -3.1278e-03,  1.3843e-02, -1.2241e-03,  1.9605e-03, -9.8605e-03,\n",
       "                       6.4532e-04,  5.2868e-03, -4.8394e-04, -5.8444e-03,  2.2925e-03,\n",
       "                       5.1965e-03,  4.2180e-03, -1.0333e-02,  3.2390e-03, -1.6436e-03,\n",
       "                       4.6552e-03, -4.7990e-03,  4.6055e-03, -8.2663e-03,  4.4874e-03,\n",
       "                       6.2418e-03,  9.9904e-03,  4.7146e-03, -2.3858e-04, -1.7664e-03,\n",
       "                       4.0254e-03, -4.9507e-03, -2.3578e-03,  4.5227e-03, -4.7647e-03,\n",
       "                       6.3824e-04,  7.7556e-03, -1.8908e-03,  4.4356e-03,  7.8154e-03,\n",
       "                      -2.1529e-03, -1.0359e-02, -3.3988e-03, -1.2163e-02, -9.1683e-03,\n",
       "                      -5.2532e-03, -2.9347e-03, -1.2756e-03,  2.4777e-03,  7.9140e-03,\n",
       "                      -1.5277e-03,  1.9642e-03, -6.0516e-03,  2.6351e-03, -1.9903e-03,\n",
       "                      -1.8167e-03,  4.8880e-03, -3.6292e-03,  1.1912e-03,  1.0462e-02,\n",
       "                       9.2070e-04, -9.5774e-03, -1.4510e-02,  2.0948e-03,  1.5968e-03,\n",
       "                       3.0147e-02, -8.1495e-04,  1.5845e-03, -5.5101e-04, -4.4856e-03,\n",
       "                       1.6054e-03, -5.9828e-03, -1.9548e-03, -2.2913e-03,  1.1910e-02,\n",
       "                       2.2277e-03,  4.5404e-03])),\n",
       "             ('bert.encoder.layer.0.output.LayerNorm.weight',\n",
       "              tensor([1.0051, 1.0188, 1.0021, 1.0029, 1.0052, 1.0099, 1.0103, 1.0256, 0.9993,\n",
       "                      1.0066, 1.0155, 0.9932, 0.9889, 1.0035, 1.0055, 1.0135, 1.0217, 0.9826,\n",
       "                      1.0092, 1.0182, 0.9919, 1.0112, 1.0135, 1.0120, 1.0150, 1.0180, 1.0026,\n",
       "                      0.9991, 1.0189, 1.0006, 0.9986, 1.0192, 0.9906, 1.0139, 1.0107, 1.0116,\n",
       "                      1.0150, 1.0057, 1.0259, 1.0084, 1.0035, 1.0091, 1.0175, 1.0129, 0.9979,\n",
       "                      1.0177, 0.9893, 1.0136, 1.0147, 1.0148, 1.0030, 1.0010, 1.0085, 1.0007,\n",
       "                      1.0192, 1.0196, 1.0133, 1.0208, 1.0026, 1.0037, 1.0173, 1.0152, 1.0161,\n",
       "                      1.0235, 1.0045, 1.0019, 1.0155, 1.0219, 1.0044, 1.0161, 0.9893, 0.9950,\n",
       "                      0.9957, 1.0018, 1.0153, 1.0204, 0.9988, 1.0058, 0.9997, 1.0058, 0.9997,\n",
       "                      0.9894, 1.0190, 1.0134, 1.0116, 1.0191, 0.9989, 1.0087, 1.0033, 1.0084,\n",
       "                      0.9931, 1.0269, 1.0060, 1.0065, 1.0203, 1.0142, 1.0083, 1.0083, 1.0095,\n",
       "                      1.0105, 1.0006, 1.0107, 1.0071, 1.0146, 1.0121, 1.0098, 0.9962, 1.0257,\n",
       "                      1.0220, 1.0229, 0.9919, 1.0195, 1.0079, 1.0307, 1.0853, 1.0141, 1.0190,\n",
       "                      1.0109, 0.9849, 1.0191, 1.0206, 1.0061, 1.0029, 1.0052, 0.9973, 1.0053,\n",
       "                      1.0107, 1.0111, 0.9962, 1.0008, 1.0111, 1.0186, 1.0082, 1.0026, 1.0011,\n",
       "                      1.0298, 1.0211, 1.0178, 1.0014, 1.0165, 1.0097, 1.0228, 1.0102, 1.0056,\n",
       "                      1.0109, 1.0096, 1.0176, 1.0165, 1.0188, 1.0047, 1.0221, 1.0187, 1.0171,\n",
       "                      1.0107, 1.0085, 1.0119, 1.0039, 0.9999, 1.0202, 1.0017, 1.0117, 0.9905,\n",
       "                      1.0266, 1.0084, 1.0103, 1.0275, 0.9993, 1.0146, 1.0092, 1.0037, 1.0188,\n",
       "                      0.9869, 1.0059, 0.9990, 1.0088, 1.0016, 1.0095, 0.9945, 1.0122, 1.0175,\n",
       "                      1.0487, 1.0189, 1.0265, 1.0102, 1.0106, 1.0177, 1.0202, 1.0122, 1.0236,\n",
       "                      1.0007, 1.0138, 1.0147])),\n",
       "             ('bert.encoder.layer.0.output.LayerNorm.bias',\n",
       "              tensor([ 2.4965e-03,  9.9289e-03,  1.4612e-02, -1.8387e-03, -4.5989e-03,\n",
       "                       5.9319e-03,  9.7949e-04, -6.4234e-03, -7.7372e-03, -9.6409e-03,\n",
       "                      -1.1581e-02,  7.7522e-03,  1.0613e-02, -1.0374e-02,  3.9659e-03,\n",
       "                      -6.3791e-03,  1.0847e-02, -4.9689e-03, -1.0826e-02,  5.5542e-03,\n",
       "                       2.3975e-02,  1.5858e-03, -5.4230e-03,  1.5191e-02, -2.1409e-02,\n",
       "                       3.9596e-03,  2.6640e-03,  2.3155e-02, -1.4755e-02,  5.9651e-03,\n",
       "                      -1.9005e-03,  2.9955e-03, -2.3146e-03, -2.6220e-03, -4.4517e-04,\n",
       "                       7.9241e-03, -1.3159e-02, -1.8491e-02, -1.9479e-03, -6.5231e-04,\n",
       "                      -7.0530e-03, -6.7508e-03, -3.8177e-03, -1.8869e-03,  4.5619e-03,\n",
       "                       2.3755e-04,  2.9794e-02, -2.8506e-03, -2.0783e-03,  1.1217e-03,\n",
       "                      -2.0292e-04, -2.7616e-02, -1.8587e-02,  1.4897e-02,  3.4851e-03,\n",
       "                       3.1814e-03, -4.5617e-03,  5.2952e-03,  2.8130e-03,  8.8014e-03,\n",
       "                      -4.8223e-03, -3.5239e-03,  7.6556e-03, -1.2581e-02, -2.7235e-03,\n",
       "                       3.9602e-03, -2.6663e-03,  2.7993e-02,  3.7521e-03,  6.9152e-03,\n",
       "                       9.2580e-03, -5.9755e-03,  6.0532e-03,  6.3041e-03,  8.1887e-04,\n",
       "                      -3.5579e-03, -6.1349e-03,  8.1283e-03,  2.3709e-03, -5.5032e-03,\n",
       "                       1.0910e-02,  3.6297e-03, -3.7467e-03, -3.2223e-03,  2.2943e-03,\n",
       "                      -8.2635e-03, -2.2977e-04, -1.2236e-02, -3.3859e-03,  3.2151e-03,\n",
       "                       3.0545e-03,  1.3007e-02, -3.8157e-03, -1.1201e-03, -1.3805e-02,\n",
       "                       4.3768e-02, -4.9118e-03,  1.3196e-02, -2.4276e-03, -7.5538e-03,\n",
       "                       2.4784e-04,  3.1240e-03, -6.1701e-03, -5.7597e-03, -7.4147e-03,\n",
       "                       6.8276e-03,  6.8316e-03,  5.7707e-03, -2.0149e-03,  7.7508e-03,\n",
       "                      -9.7367e-03,  3.0144e-03,  2.2646e-03,  1.2649e-03, -3.5865e-02,\n",
       "                       5.1569e-03,  1.0846e-02,  6.8343e-03,  1.9823e-02, -1.8833e-03,\n",
       "                      -1.9113e-02,  2.1236e-02,  5.2729e-03,  5.6793e-04, -7.7928e-03,\n",
       "                       2.7660e-03,  8.9066e-03,  1.4760e-02,  1.1422e-02,  5.5066e-03,\n",
       "                       1.5357e-02,  3.6758e-03, -7.6264e-03,  8.2930e-03, -8.6535e-03,\n",
       "                      -3.0826e-03, -2.4012e-02,  1.2803e-02, -9.6555e-03,  3.9518e-03,\n",
       "                       7.2228e-04,  6.7408e-03,  1.4308e-02, -5.9388e-05, -7.0997e-03,\n",
       "                       2.3825e-03,  8.7147e-03, -4.4665e-03,  5.5107e-03, -4.6774e-03,\n",
       "                      -9.1186e-04,  2.3721e-02, -9.7420e-03, -3.2208e-03,  1.6280e-02,\n",
       "                       5.0146e-03, -1.6099e-02,  1.3009e-02, -1.9796e-02, -2.6376e-02,\n",
       "                      -6.7449e-03, -3.3654e-03, -7.5900e-03, -5.5938e-03,  1.0854e-02,\n",
       "                      -4.3641e-03,  6.3417e-03, -8.4304e-04,  8.4358e-04, -2.7515e-03,\n",
       "                      -1.5226e-02,  7.6739e-03, -5.8820e-03, -4.9675e-03, -3.7272e-03,\n",
       "                       1.4422e-02, -1.5219e-02, -2.7035e-02,  4.2537e-03,  6.4363e-03,\n",
       "                       5.4620e-02, -2.9861e-03, -9.8101e-03, -4.7977e-03, -1.0407e-02,\n",
       "                       7.4940e-04, -1.2044e-02, -1.0359e-02,  1.1275e-02,  2.9873e-02,\n",
       "                      -1.4933e-02, -4.4175e-03])),\n",
       "             ('bert.encoder.layer.1.attention.self.query.weight',\n",
       "              tensor([[ 0.0121, -0.0148, -0.0280,  ...,  0.0270, -0.0185,  0.0720],\n",
       "                      [-0.0092,  0.0286,  0.0199,  ...,  0.0213, -0.0055, -0.0382],\n",
       "                      [ 0.0242,  0.0078,  0.0033,  ..., -0.0394, -0.0113,  0.0357],\n",
       "                      ...,\n",
       "                      [ 0.0264, -0.0763,  0.0497,  ...,  0.0199, -0.0455,  0.0247],\n",
       "                      [ 0.0200, -0.0034,  0.0214,  ..., -0.0393, -0.0265, -0.0096],\n",
       "                      [-0.0028, -0.0324, -0.0440,  ..., -0.0338, -0.0296,  0.0298]])),\n",
       "             ('bert.encoder.layer.1.attention.self.query.bias',\n",
       "              tensor([-0.0246, -0.0333,  0.0120, -0.0184,  0.0505,  0.0289,  0.0164,  0.0679,\n",
       "                      -0.0379,  0.0265, -0.0382,  0.0098, -0.0446, -0.0556, -0.0287,  0.0178,\n",
       "                      -0.0559, -0.0285, -0.0375,  0.0027, -0.0637,  0.0338, -0.0318, -0.0218,\n",
       "                       0.0212,  0.0216,  0.0030, -0.0006, -0.0641, -0.0428,  0.0441,  0.0462,\n",
       "                       0.0361,  0.0153,  0.0130, -0.0108,  0.0349, -0.0539, -0.0419,  0.0137,\n",
       "                       0.0465,  0.0327,  0.0849,  0.0500,  0.0641, -0.0108,  0.0398,  0.0415,\n",
       "                       0.0864, -0.0133, -0.0528,  0.0225,  0.0334,  0.0388, -0.0336, -0.0317,\n",
       "                      -0.0515, -0.0647, -0.0185, -0.0605, -0.0252, -0.0188,  0.0589,  0.0427,\n",
       "                      -0.1093,  0.0853, -0.0713, -0.0755, -0.0994,  0.0609,  0.0850, -0.0413,\n",
       "                       0.0699,  0.0866, -0.0552, -0.0840, -0.0674,  0.0546, -0.0468, -0.0527,\n",
       "                      -0.0968,  0.0866, -0.0123,  0.0527,  0.0642, -0.0511,  0.0320, -0.1049,\n",
       "                      -0.0818,  0.0547,  0.0985,  0.0547, -0.0721, -0.0708, -0.0864,  0.0470,\n",
       "                       0.0026, -0.0070,  0.0313, -0.0124,  0.0583,  0.0048,  0.0340, -0.0169,\n",
       "                       0.0052, -0.0271, -0.0421, -0.0265,  0.0042, -0.0156,  0.0126, -0.0180,\n",
       "                      -0.0318,  0.0201,  0.0288,  0.0057, -0.0296,  0.0511,  0.0244, -0.0369,\n",
       "                      -0.0335, -0.0584, -0.0342, -0.0134, -0.0256,  0.0515,  0.0461,  0.0048,\n",
       "                       0.0253,  0.0102,  0.0193, -0.0266,  0.0481,  0.0236,  0.0349, -0.0295,\n",
       "                      -0.0470,  0.0385,  0.0444, -0.0127, -0.0004, -0.0215,  0.0203, -0.0094,\n",
       "                       0.0710, -0.0212, -0.0158, -0.0149, -0.0341, -0.0060,  0.0217,  0.0481,\n",
       "                       0.0314,  0.0213,  0.0252, -0.0294, -0.0611,  0.0390,  0.0094,  0.0210,\n",
       "                      -0.0058,  0.0637, -0.0491,  0.0117,  0.0410, -0.0544, -0.0445, -0.0331,\n",
       "                      -0.0432, -0.0236, -0.0238, -0.0004,  0.0067,  0.0152,  0.0355, -0.0157,\n",
       "                       0.0170, -0.0243, -0.0568,  0.0296, -0.0012, -0.0565, -0.0143, -0.0074,\n",
       "                       0.0233,  0.0112, -0.0065, -0.0088,  0.0149,  0.0169, -0.0421, -0.0423])),\n",
       "             ('bert.encoder.layer.1.attention.self.key.weight',\n",
       "              tensor([[ 0.0153, -0.0145, -0.0194,  ..., -0.0262,  0.0105, -0.0319],\n",
       "                      [ 0.0067,  0.0500, -0.0133,  ...,  0.0253,  0.0122,  0.0109],\n",
       "                      [ 0.0115, -0.0337, -0.0183,  ..., -0.0266,  0.0183,  0.0072],\n",
       "                      ...,\n",
       "                      [-0.0192,  0.0017,  0.0204,  ...,  0.0048, -0.0209,  0.0228],\n",
       "                      [-0.0030,  0.0005,  0.0102,  ...,  0.0174, -0.0414, -0.0066],\n",
       "                      [ 0.0056,  0.0099, -0.0077,  ..., -0.0215, -0.0040,  0.0365]])),\n",
       "             ('bert.encoder.layer.1.attention.self.key.bias',\n",
       "              tensor([ 1.7278e-09,  2.8299e-08, -4.2695e-08,  5.7883e-08, -3.8401e-08,\n",
       "                       1.2252e-07, -3.7698e-08,  1.3317e-07,  4.9798e-08, -4.8591e-08,\n",
       "                      -1.5867e-08,  5.6281e-08, -2.7406e-07, -7.7629e-08,  5.0530e-08,\n",
       "                      -1.1695e-07,  5.3209e-08, -9.3465e-09, -3.6348e-08, -1.2045e-07,\n",
       "                       6.6943e-08,  3.2412e-08,  8.3262e-08,  3.0897e-08, -1.5200e-08,\n",
       "                      -2.8109e-08, -7.7940e-08, -6.3100e-08,  9.2838e-08,  1.4817e-07,\n",
       "                       6.1493e-08,  1.1218e-09,  1.4876e-07,  1.4707e-09, -5.3203e-08,\n",
       "                       7.8938e-08,  2.6753e-08,  5.4591e-08, -1.3481e-08, -3.3979e-09,\n",
       "                      -4.3827e-08, -1.9251e-07,  4.3561e-08,  9.1805e-08, -3.7739e-08,\n",
       "                      -4.8627e-08, -2.5264e-08, -2.1783e-07, -8.0118e-08,  5.6339e-08,\n",
       "                       6.2654e-08, -2.8503e-08, -4.5995e-08,  1.2572e-07,  3.5186e-08,\n",
       "                      -4.7592e-08,  7.7872e-08,  1.5698e-07,  1.5538e-07, -5.7922e-08,\n",
       "                       4.7649e-08,  1.9140e-08, -2.3249e-08,  2.4166e-08, -1.0161e-07,\n",
       "                      -8.3065e-08,  2.0512e-08,  6.4457e-08, -1.3777e-08,  1.6428e-07,\n",
       "                      -2.0331e-07,  1.3957e-07, -2.4845e-07, -8.3527e-08,  2.7633e-10,\n",
       "                       8.6448e-09,  9.1088e-08, -3.7945e-07, -5.9800e-08,  1.8994e-07,\n",
       "                       1.6393e-07,  6.6211e-08, -2.7363e-07, -2.1476e-07,  5.0421e-08,\n",
       "                      -2.6133e-07,  3.5035e-08,  9.1488e-08,  4.5140e-08, -2.5021e-07,\n",
       "                      -8.6917e-08,  1.7661e-07,  1.5306e-07,  3.6402e-07, -5.2474e-08,\n",
       "                       1.5421e-07, -2.3518e-07,  2.0242e-07, -8.1467e-08, -2.9717e-07,\n",
       "                      -3.0158e-09,  9.8084e-08,  2.6315e-07, -7.9069e-08, -1.8554e-07,\n",
       "                      -2.4398e-07, -1.1597e-07, -2.2761e-07,  3.2487e-07, -5.3442e-08,\n",
       "                      -1.1844e-07,  2.9556e-07,  1.8860e-07, -2.5295e-07,  5.4911e-07,\n",
       "                       2.2744e-07,  6.8110e-08,  1.6553e-07, -2.2070e-07,  1.9622e-07,\n",
       "                       1.2044e-07,  1.4340e-07, -1.6823e-07, -1.5586e-07,  1.7456e-07,\n",
       "                       5.7062e-09, -2.1127e-07, -1.8141e-07,  2.6568e-07,  1.2021e-07,\n",
       "                       1.1137e-07,  3.3375e-08, -2.6827e-08, -1.2239e-07, -1.7073e-08,\n",
       "                      -6.1245e-08,  1.0802e-07,  7.8774e-10, -1.7258e-07,  1.1796e-07,\n",
       "                       5.8795e-08, -1.0802e-08, -9.7460e-08, -3.7602e-08, -7.4079e-08,\n",
       "                       1.4871e-07, -2.2828e-07,  1.2815e-07,  2.7902e-07,  6.1509e-08,\n",
       "                      -6.0872e-08,  1.0926e-07, -1.6075e-07, -1.8937e-07, -1.4980e-08,\n",
       "                       6.5935e-08,  8.0571e-09, -1.1742e-07, -2.7802e-07, -1.0543e-07,\n",
       "                       9.7049e-08,  4.3215e-08, -9.4291e-08, -8.6896e-08, -4.7383e-08,\n",
       "                       1.1824e-07,  1.1774e-07,  1.5675e-07, -3.6874e-09,  1.3147e-07,\n",
       "                       7.6269e-08,  1.4688e-09,  3.2477e-08,  8.5254e-08,  2.1537e-08,\n",
       "                      -1.8745e-07, -2.1070e-09,  1.4236e-08,  4.0043e-08,  5.7924e-08,\n",
       "                       7.2302e-08, -3.6953e-09,  7.2665e-08,  4.7579e-09, -3.3049e-07,\n",
       "                      -2.0417e-08, -8.8313e-08,  5.5553e-08,  1.0818e-07, -7.5785e-08,\n",
       "                      -3.6790e-08, -3.2666e-08])),\n",
       "             ('bert.encoder.layer.1.attention.self.value.weight',\n",
       "              tensor([[-0.0309, -0.0059,  0.0457,  ..., -0.0306, -0.0260, -0.0340],\n",
       "                      [-0.0073, -0.0216,  0.0043,  ..., -0.0124,  0.0153,  0.0363],\n",
       "                      [-0.0494,  0.0249, -0.0291,  ...,  0.0165,  0.0010,  0.0219],\n",
       "                      ...,\n",
       "                      [ 0.0150, -0.0174, -0.0076,  ...,  0.0169, -0.0012,  0.0200],\n",
       "                      [ 0.0156,  0.0299, -0.0179,  ..., -0.0003, -0.0005, -0.0265],\n",
       "                      [ 0.0009, -0.0488, -0.0093,  ..., -0.0021,  0.0129, -0.0309]])),\n",
       "             ('bert.encoder.layer.1.attention.self.value.bias',\n",
       "              tensor([-0.0105,  0.0034, -0.0017, -0.0081, -0.0043,  0.0097,  0.0028, -0.0071,\n",
       "                       0.0012,  0.0020, -0.0152, -0.0016,  0.0041, -0.0059,  0.0073, -0.0012,\n",
       "                      -0.0034, -0.0003,  0.0023,  0.0080, -0.0107, -0.0005, -0.0018,  0.0025,\n",
       "                       0.0050,  0.0030,  0.0077, -0.0007, -0.0092, -0.0057, -0.0005, -0.0013,\n",
       "                      -0.0068, -0.0126, -0.0064,  0.0005, -0.0019,  0.0016,  0.0012, -0.0010,\n",
       "                      -0.0047,  0.0034,  0.0133, -0.0082, -0.0053,  0.0016, -0.0037,  0.0033,\n",
       "                      -0.0034,  0.0060,  0.0061, -0.0042,  0.0089, -0.0008, -0.0117, -0.0090,\n",
       "                      -0.0068,  0.0046,  0.0040,  0.0107, -0.0020,  0.0079,  0.0029,  0.0052,\n",
       "                      -0.0146, -0.0141, -0.0003,  0.0108,  0.0015, -0.0003,  0.0221, -0.0166,\n",
       "                      -0.0030, -0.0121, -0.0050, -0.0078,  0.0025,  0.0029,  0.0007, -0.0004,\n",
       "                       0.0046,  0.0024,  0.0036, -0.0016, -0.0030, -0.0181, -0.0060,  0.0094,\n",
       "                       0.0064,  0.0005, -0.0119,  0.0059, -0.0016,  0.0134, -0.0015, -0.0131,\n",
       "                       0.0006,  0.0020, -0.0016,  0.0007, -0.0052,  0.0037,  0.0072,  0.0030,\n",
       "                      -0.0028,  0.0115,  0.0087, -0.0034, -0.0030, -0.0104, -0.0099,  0.0052,\n",
       "                      -0.0092, -0.0235,  0.0062, -0.0068, -0.0017,  0.0017,  0.0090, -0.0081,\n",
       "                      -0.0172,  0.0040, -0.0095, -0.0105, -0.0010, -0.0072, -0.0091,  0.0003,\n",
       "                       0.0077,  0.0051,  0.0063,  0.0025,  0.0129, -0.0086, -0.0011, -0.0054,\n",
       "                      -0.0039,  0.0095, -0.0036, -0.0041, -0.0095, -0.0037, -0.0039, -0.0040,\n",
       "                      -0.0110,  0.0042, -0.0044,  0.0035, -0.0075,  0.0012, -0.0071, -0.0090,\n",
       "                      -0.0110,  0.0139,  0.0095, -0.0006,  0.0018,  0.0165, -0.0004, -0.0002,\n",
       "                      -0.0034, -0.0090,  0.0032,  0.0006, -0.0124,  0.0047,  0.0007,  0.0012,\n",
       "                      -0.0009, -0.0003,  0.0083,  0.0013,  0.0018, -0.0010, -0.0115,  0.0049,\n",
       "                       0.0033, -0.0002,  0.0007,  0.0008, -0.0059, -0.0080, -0.0060, -0.0040,\n",
       "                      -0.0047,  0.0047, -0.0076, -0.0049, -0.0012,  0.0022,  0.0033, -0.0041])),\n",
       "             ('bert.encoder.layer.1.attention.output.dense.weight',\n",
       "              tensor([[ 0.0146,  0.0179, -0.0378,  ...,  0.0101,  0.0024,  0.0129],\n",
       "                      [-0.0314, -0.0156,  0.0436,  ..., -0.0212,  0.0104, -0.0326],\n",
       "                      [ 0.0058,  0.0092,  0.0279,  ..., -0.0281, -0.0310, -0.0164],\n",
       "                      ...,\n",
       "                      [ 0.0356, -0.0020,  0.0375,  ...,  0.0032,  0.0291, -0.0205],\n",
       "                      [ 0.0398, -0.0113,  0.0189,  ..., -0.0027, -0.0161,  0.0025],\n",
       "                      [-0.0197,  0.0376,  0.0342,  ...,  0.0377,  0.0041,  0.0180]])),\n",
       "             ('bert.encoder.layer.1.attention.output.dense.bias',\n",
       "              tensor([ 0.0113,  0.0055,  0.0099,  0.0018, -0.0088, -0.0047, -0.0027, -0.0044,\n",
       "                      -0.0053, -0.0033, -0.0081,  0.0039,  0.0019, -0.0088, -0.0102, -0.0019,\n",
       "                       0.0024, -0.0062,  0.0021,  0.0005,  0.0154,  0.0006,  0.0021,  0.0061,\n",
       "                      -0.0126,  0.0011,  0.0034,  0.0172, -0.0158,  0.0007, -0.0083,  0.0015,\n",
       "                       0.0042, -0.0006,  0.0014,  0.0021, -0.0088, -0.0152, -0.0062,  0.0016,\n",
       "                      -0.0041, -0.0029, -0.0114, -0.0070,  0.0029, -0.0026,  0.0170, -0.0031,\n",
       "                       0.0012, -0.0035,  0.0021, -0.0154, -0.0163,  0.0083,  0.0005,  0.0024,\n",
       "                      -0.0052, -0.0042,  0.0040,  0.0072,  0.0057, -0.0058, -0.0021, -0.0037,\n",
       "                       0.0042,  0.0018, -0.0019,  0.0246,  0.0033,  0.0013,  0.0071,  0.0092,\n",
       "                       0.0029, -0.0024, -0.0007, -0.0072, -0.0090,  0.0024,  0.0023, -0.0079,\n",
       "                       0.0037,  0.0017,  0.0046,  0.0056,  0.0004, -0.0060,  0.0021, -0.0066,\n",
       "                      -0.0115,  0.0031,  0.0124, -0.0007, -0.0040,  0.0013, -0.0022,  0.0339,\n",
       "                      -0.0036,  0.0126,  0.0036, -0.0066, -0.0008, -0.0006, -0.0018, -0.0005,\n",
       "                      -0.0138,  0.0050,  0.0041, -0.0022,  0.0031,  0.0024, -0.0059,  0.0047,\n",
       "                      -0.0044, -0.0008, -0.0344, -0.0016,  0.0034,  0.0043,  0.0087,  0.0057,\n",
       "                      -0.0104,  0.0151,  0.0039, -0.0005, -0.0048,  0.0020, -0.0012,  0.0069,\n",
       "                       0.0032,  0.0071,  0.0090,  0.0041, -0.0052,  0.0043, -0.0011, -0.0053,\n",
       "                      -0.0136,  0.0090, -0.0065, -0.0002,  0.0036,  0.0066,  0.0077, -0.0027,\n",
       "                      -0.0014,  0.0018,  0.0068, -0.0057,  0.0027, -0.0061, -0.0038,  0.0140,\n",
       "                      -0.0019, -0.0013,  0.0116, -0.0005, -0.0065,  0.0095, -0.0131, -0.0181,\n",
       "                      -0.0027,  0.0052, -0.0064, -0.0068,  0.0083, -0.0091,  0.0106, -0.0012,\n",
       "                       0.0023, -0.0015, -0.0023,  0.0067, -0.0043,  0.0008,  0.0005,  0.0077,\n",
       "                      -0.0093, -0.0134, -0.0019,  0.0008,  0.0449, -0.0040, -0.0026,  0.0037,\n",
       "                      -0.0109, -0.0061, -0.0020, -0.0046, -0.0008,  0.0178, -0.0095, -0.0050])),\n",
       "             ('bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9991, 1.0088, 0.9925, 0.9986, 0.9961, 0.9953, 1.0039, 1.0121, 0.9819,\n",
       "                      1.0073, 0.9998, 0.9865, 0.9876, 1.0010, 0.9833, 1.0040, 1.0126, 0.9644,\n",
       "                      0.9997, 1.0138, 0.9850, 1.0115, 1.0063, 1.0071, 1.0051, 1.0164, 0.9945,\n",
       "                      0.9900, 1.0087, 0.9990, 0.9821, 1.0146, 0.9773, 1.0076, 1.0022, 0.9967,\n",
       "                      1.0020, 0.9935, 1.0084, 1.0044, 0.9921, 1.0029, 1.0070, 0.9979, 0.9925,\n",
       "                      1.0163, 0.9851, 1.0053, 1.0094, 1.0049, 0.9933, 0.9922, 1.0031, 0.9935,\n",
       "                      1.0124, 1.0137, 1.0015, 1.0190, 1.0030, 0.9922, 1.0009, 1.0018, 1.0128,\n",
       "                      1.0072, 0.9961, 0.9997, 1.0011, 1.0107, 0.9965, 1.0127, 0.9826, 0.9764,\n",
       "                      0.9907, 0.9964, 1.0068, 1.0004, 0.9989, 0.9959, 0.9972, 0.9972, 0.9947,\n",
       "                      0.9862, 1.0083, 1.0064, 1.0132, 1.0056, 0.9949, 1.0041, 0.9903, 1.0019,\n",
       "                      0.9789, 1.0132, 1.0008, 0.9995, 1.0092, 1.0028, 1.0025, 0.9931, 1.0031,\n",
       "                      1.0068, 0.9896, 1.0052, 1.0045, 1.0068, 1.0029, 0.9966, 0.9924, 1.0083,\n",
       "                      1.0145, 1.0186, 0.9854, 1.0120, 1.0007, 1.0182, 1.0806, 1.0031, 1.0075,\n",
       "                      1.0062, 0.9871, 1.0092, 1.0122, 0.9873, 0.9960, 0.9955, 0.9966, 0.9984,\n",
       "                      0.9979, 1.0049, 0.9912, 0.9947, 1.0046, 1.0062, 1.0026, 0.9927, 0.9979,\n",
       "                      1.0164, 1.0123, 1.0097, 0.9839, 1.0134, 0.9994, 1.0097, 1.0027, 1.0044,\n",
       "                      0.9996, 0.9990, 1.0032, 1.0069, 1.0098, 0.9997, 1.0070, 1.0093, 1.0039,\n",
       "                      1.0055, 1.0018, 1.0140, 0.9971, 0.9861, 1.0091, 0.9929, 0.9987, 0.9783,\n",
       "                      1.0146, 0.9993, 1.0036, 1.0172, 0.9808, 1.0110, 1.0021, 0.9779, 1.0047,\n",
       "                      0.9721, 1.0029, 0.9919, 1.0025, 0.9909, 1.0027, 0.9823, 1.0074, 1.0016,\n",
       "                      1.0445, 1.0085, 1.0131, 1.0099, 1.0008, 1.0085, 1.0116, 1.0030, 1.0113,\n",
       "                      0.9916, 1.0110, 1.0030])),\n",
       "             ('bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       "              tensor([ 4.6567e-03,  1.1426e-02,  1.7615e-02, -1.0493e-04, -1.0929e-02,\n",
       "                      -1.2592e-03,  4.9358e-03, -6.8695e-03, -3.2709e-03, -1.4050e-02,\n",
       "                      -1.3854e-02,  8.1499e-03,  1.1625e-02, -1.0953e-02, -3.5545e-03,\n",
       "                      -7.0247e-03,  7.9568e-03, -5.9367e-03, -6.2988e-03,  1.5027e-03,\n",
       "                       2.5181e-02,  1.6039e-03, -6.0946e-03,  1.4691e-02, -2.2751e-02,\n",
       "                       2.7107e-03,  5.1689e-03,  2.2187e-02, -1.9826e-02,  2.6938e-03,\n",
       "                      -5.8102e-03,  8.0372e-04,  5.1729e-03, -5.4825e-04,  1.2556e-03,\n",
       "                       1.2307e-02, -1.2607e-02, -2.1822e-02, -3.4489e-03, -3.3975e-03,\n",
       "                      -8.0884e-03, -6.5244e-03, -4.1549e-03, -3.8142e-03,  2.4421e-03,\n",
       "                      -2.5587e-03,  3.0798e-02, -5.8685e-03, -6.3805e-04,  3.2830e-03,\n",
       "                       4.5943e-03, -3.0478e-02, -2.0790e-02,  1.3390e-02, -8.8787e-04,\n",
       "                       2.7060e-03, -7.3103e-03,  4.3377e-03,  2.5342e-03,  1.0774e-02,\n",
       "                      -3.0169e-03, -2.4602e-03,  4.3731e-03, -1.4978e-02,  1.4094e-03,\n",
       "                       2.1052e-03, -6.3426e-03,  2.9992e-02,  5.5445e-03,  7.0480e-03,\n",
       "                       1.0812e-02,  2.2550e-03,  8.2163e-03,  3.8872e-03,  6.4337e-04,\n",
       "                      -3.1150e-03, -5.7264e-03,  8.9725e-03,  1.5234e-03, -7.4519e-03,\n",
       "                       1.0284e-02,  1.5266e-03, -6.8218e-04, -3.5564e-03,  4.7392e-04,\n",
       "                      -1.2310e-02,  8.6565e-04, -1.2356e-02, -1.0996e-02, -1.1845e-03,\n",
       "                       1.0630e-02,  1.2946e-02, -2.6120e-03, -4.3699e-03, -1.2487e-02,\n",
       "                       4.8086e-02,  3.1617e-04,  1.7356e-02,  8.4355e-04, -7.6341e-03,\n",
       "                      -1.9356e-03,  2.0499e-03, -6.7299e-03, -6.3223e-03, -1.1525e-02,\n",
       "                       6.8112e-03,  6.8404e-03,  2.3665e-03,  1.5718e-03,  7.1154e-03,\n",
       "                      -9.0604e-03,  2.6828e-03,  6.1383e-04, -1.1991e-03, -4.7157e-02,\n",
       "                       5.8849e-03,  1.3415e-02,  5.8932e-03,  1.9717e-02,  1.3009e-03,\n",
       "                      -1.7944e-02,  2.2673e-02,  8.1233e-03,  2.1510e-03, -9.8563e-03,\n",
       "                       3.7611e-03,  6.9540e-03,  1.2731e-02,  6.6753e-03,  5.1067e-03,\n",
       "                       2.0281e-02,  9.2704e-03, -1.2784e-02,  8.6820e-03, -4.7860e-03,\n",
       "                      -5.3452e-03, -2.5838e-02,  1.5866e-02, -7.1452e-03,  3.8926e-03,\n",
       "                       4.9030e-03,  3.9685e-03,  8.5157e-03, -2.4593e-03, -6.5407e-03,\n",
       "                       5.1684e-05,  8.1105e-03, -4.4477e-03,  4.6119e-03, -6.9298e-03,\n",
       "                      -3.2611e-03,  2.3294e-02, -1.2582e-02, -4.1587e-03,  2.0101e-02,\n",
       "                       6.6396e-04, -1.4043e-02,  2.0416e-02, -1.6970e-02, -2.6693e-02,\n",
       "                      -6.4742e-03, -1.1131e-03, -8.8920e-03, -1.0518e-02,  1.3575e-02,\n",
       "                      -4.1972e-03,  1.3424e-02, -3.1029e-03,  4.3044e-04, -5.7201e-03,\n",
       "                      -1.4331e-02,  1.0370e-02, -7.5623e-03, -1.5648e-03, -5.3000e-03,\n",
       "                       1.5363e-02, -1.4065e-02, -2.3828e-02,  2.0101e-03,  5.5367e-03,\n",
       "                       5.9286e-02, -2.1760e-03, -1.0383e-02, -1.1923e-04, -9.8837e-03,\n",
       "                       1.3286e-03, -1.0750e-02, -1.2353e-02,  1.2038e-02,  2.8720e-02,\n",
       "                      -1.9549e-02, -6.8650e-03])),\n",
       "             ('bert.encoder.layer.1.intermediate.dense.weight',\n",
       "              tensor([[ 0.0017,  0.0220,  0.0015,  ...,  0.0009, -0.0266,  0.0099],\n",
       "                      [ 0.0151,  0.0290,  0.0078,  ...,  0.0469,  0.0112,  0.0035],\n",
       "                      [-0.0033, -0.0134, -0.0294,  ..., -0.0020,  0.0058, -0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0138,  0.0124,  0.0270,  ..., -0.0116, -0.0441, -0.0312],\n",
       "                      [ 0.0106,  0.0225, -0.0204,  ..., -0.0201, -0.0010,  0.0079],\n",
       "                      [-0.0047, -0.0228,  0.0036,  ...,  0.0195,  0.0271,  0.0459]])),\n",
       "             ('bert.encoder.layer.1.intermediate.dense.bias',\n",
       "              tensor([-2.1708e-03, -2.8875e-03, -6.9701e-03,  1.2859e-02, -8.9607e-03,\n",
       "                      -4.0310e-04, -3.4221e-03,  4.9135e-03,  1.1593e-02,  4.0530e-03,\n",
       "                       5.3280e-03,  2.2134e-02,  4.2970e-04, -1.2732e-02,  3.4590e-04,\n",
       "                      -1.4397e-02, -8.5449e-04, -1.2381e-02, -1.0339e-02,  4.2784e-03,\n",
       "                      -2.3496e-03,  1.3284e-02,  3.1484e-03,  7.8775e-03,  9.6858e-03,\n",
       "                       7.8692e-03, -1.5826e-02,  1.2955e-02,  3.5781e-03, -7.9990e-03,\n",
       "                      -1.9524e-05, -7.2202e-03, -7.1471e-03, -6.6254e-03,  4.9196e-03,\n",
       "                       8.7553e-03, -4.6637e-03, -1.0644e-02, -8.3703e-03,  1.0566e-02,\n",
       "                      -1.6694e-02, -3.1253e-03, -1.6489e-02, -3.3667e-03,  1.0528e-02,\n",
       "                      -3.4996e-03, -2.9655e-03, -6.3279e-03, -4.4249e-04, -6.0534e-03,\n",
       "                      -2.6128e-03,  1.1245e-02, -1.3536e-04,  1.8982e-03, -4.9873e-03,\n",
       "                      -3.0052e-03,  5.6974e-03, -6.1524e-03, -7.4845e-03,  8.9226e-03,\n",
       "                       1.1007e-03,  3.9097e-03,  4.8221e-03,  5.7029e-03])),\n",
       "             ('bert.encoder.layer.1.output.dense.weight',\n",
       "              tensor([[-0.0074, -0.0170,  0.0075,  ..., -0.0012,  0.0472,  0.0083],\n",
       "                      [ 0.0377, -0.0310, -0.0071,  ..., -0.0081,  0.0271,  0.0145],\n",
       "                      [-0.0210, -0.0406,  0.0057,  ..., -0.0082, -0.0305,  0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0312, -0.0115,  0.0311,  ..., -0.0300,  0.0411,  0.0365],\n",
       "                      [-0.0207, -0.0067, -0.0058,  ..., -0.0047,  0.0069, -0.0405],\n",
       "                      [-0.0078,  0.0076,  0.0125,  ..., -0.0207, -0.0265, -0.0167]])),\n",
       "             ('bert.encoder.layer.1.output.dense.bias',\n",
       "              tensor([ 4.9147e-03,  1.5967e-03,  1.0557e-02,  3.2440e-03, -7.3011e-04,\n",
       "                      -2.3971e-03,  8.6296e-04, -3.9624e-03, -1.1629e-03, -8.0903e-03,\n",
       "                      -8.1150e-03,  7.9219e-03,  1.2876e-02, -2.0480e-03, -8.2965e-04,\n",
       "                       1.2939e-04,  2.1394e-03,  3.7159e-03,  3.7205e-04,  2.2744e-03,\n",
       "                       9.4320e-03, -4.4102e-04,  9.1937e-04, -1.0608e-03, -9.3021e-03,\n",
       "                      -5.4745e-03,  1.2902e-03,  8.0526e-03, -7.8466e-03,  2.2542e-03,\n",
       "                       5.4486e-03, -6.4215e-03, -3.6908e-03,  3.5482e-04, -3.8146e-03,\n",
       "                       2.0514e-03, -3.8112e-03, -1.2359e-02, -5.3261e-03, -1.1992e-02,\n",
       "                      -1.1970e-03, -4.8613e-03, -5.9912e-03, -3.7496e-03,  1.9999e-03,\n",
       "                       8.9281e-04,  1.5034e-02, -4.4457e-03, -4.5706e-03,  4.7438e-03,\n",
       "                       1.6809e-03, -1.0418e-02, -1.1913e-02,  3.4752e-03,  3.0220e-03,\n",
       "                      -1.5480e-03,  2.6629e-03, -6.1265e-03,  2.4220e-03,  1.5429e-03,\n",
       "                       7.9535e-04,  1.3175e-03,  6.8080e-03,  1.7772e-04,  4.9506e-03,\n",
       "                       1.9622e-03, -5.4037e-03,  1.8710e-02,  2.1382e-03, -1.9073e-03,\n",
       "                       8.0102e-03,  6.0341e-03,  4.6134e-03,  8.2562e-03, -6.9269e-03,\n",
       "                      -3.2262e-03, -2.8967e-03,  4.1409e-03, -4.3718e-03, -1.0213e-03,\n",
       "                      -4.2106e-04,  6.4502e-03,  4.6540e-03, -5.6771e-03, -2.6180e-03,\n",
       "                      -1.3266e-02, -4.2423e-03, -9.8534e-04, -1.3088e-03, -2.5386e-03,\n",
       "                       3.1972e-03, -1.0048e-03, -8.2076e-03, -9.1562e-04, -1.0929e-03,\n",
       "                       2.9192e-02,  3.5730e-03,  5.7118e-03,  4.0574e-03,  3.0757e-03,\n",
       "                      -5.6813e-04,  6.9661e-04, -2.0057e-03, -8.5972e-03, -8.8646e-03,\n",
       "                      -1.0040e-03,  4.4822e-03, -3.8214e-03, -1.6841e-03,  5.8666e-03,\n",
       "                       5.6620e-04,  1.7981e-03,  3.0357e-03, -5.7140e-03, -3.9111e-02,\n",
       "                       7.3722e-03,  3.0988e-03,  3.4307e-03,  1.5619e-02,  6.2887e-03,\n",
       "                      -6.6377e-03,  8.9743e-03, -8.5931e-04,  5.7650e-03, -1.8721e-03,\n",
       "                       3.1171e-03,  3.3601e-03,  3.3343e-03,  2.6444e-03,  1.1591e-03,\n",
       "                       9.3667e-03,  6.9341e-03, -5.2289e-03,  6.7915e-03, -2.8622e-03,\n",
       "                      -3.8132e-03, -1.2929e-02,  1.1604e-02, -2.5298e-03,  5.9203e-03,\n",
       "                       1.9558e-03,  7.0384e-03,  6.2567e-03,  1.7364e-03, -4.7774e-03,\n",
       "                       2.6791e-03,  5.1787e-05,  6.2241e-03,  2.1761e-04, -2.6320e-03,\n",
       "                      -1.2165e-03,  1.1063e-02, -5.7397e-03, -4.1972e-03,  1.0688e-02,\n",
       "                       2.8361e-03, -1.1212e-02,  1.0985e-02, -8.8178e-03, -1.3510e-02,\n",
       "                      -2.8895e-03,  3.4365e-03, -1.4901e-03,  1.1578e-03,  1.3235e-02,\n",
       "                      -5.0684e-03,  4.0452e-03, -1.5149e-03,  2.5080e-03, -2.2583e-03,\n",
       "                      -4.2634e-03,  3.3498e-03, -5.9955e-03, -5.5655e-03, -2.4412e-03,\n",
       "                       4.7313e-03, -1.2742e-02, -9.6739e-03, -1.9231e-03,  5.2667e-03,\n",
       "                       4.0277e-02, -2.2843e-03, -6.3794e-03, -2.7629e-03, -9.7185e-03,\n",
       "                      -5.2139e-04, -1.1152e-02, -1.5167e-03, -2.6306e-03,  1.1338e-02,\n",
       "                      -5.7400e-04, -3.2870e-03])),\n",
       "             ('bert.encoder.layer.1.output.LayerNorm.weight',\n",
       "              tensor([1.0106, 1.0245, 1.0083, 1.0068, 1.0091, 1.0007, 1.0172, 1.0285, 0.9922,\n",
       "                      1.0174, 1.0020, 0.9991, 1.0037, 1.0088, 1.0027, 1.0127, 1.0227, 0.9829,\n",
       "                      0.9994, 1.0256, 1.0022, 1.0246, 1.0128, 1.0194, 1.0154, 1.0274, 1.0118,\n",
       "                      0.9925, 1.0037, 1.0171, 1.0018, 1.0255, 1.0003, 1.0193, 1.0172, 1.0031,\n",
       "                      1.0126, 0.9883, 1.0210, 1.0146, 1.0074, 1.0149, 1.0196, 1.0154, 1.0091,\n",
       "                      1.0278, 0.9966, 1.0229, 1.0194, 1.0200, 1.0077, 0.9829, 0.9839, 1.0134,\n",
       "                      1.0243, 1.0195, 1.0160, 1.0227, 1.0179, 1.0070, 1.0129, 1.0130, 1.0249,\n",
       "                      1.0095, 1.0124, 1.0149, 1.0161, 0.9845, 1.0096, 1.0153, 0.9948, 0.9973,\n",
       "                      1.0069, 1.0120, 1.0206, 1.0102, 1.0134, 1.0076, 1.0099, 1.0117, 1.0029,\n",
       "                      1.0043, 1.0214, 1.0168, 1.0255, 1.0171, 1.0091, 1.0114, 1.0060, 1.0130,\n",
       "                      0.9963, 1.0181, 1.0143, 1.0118, 1.0124, 0.9872, 1.0124, 0.9892, 1.0165,\n",
       "                      1.0159, 1.0026, 1.0178, 1.0159, 1.0186, 1.0138, 1.0118, 1.0121, 1.0199,\n",
       "                      1.0260, 1.0325, 0.9959, 1.0235, 1.0172, 1.0227, 1.0032, 1.0165, 1.0170,\n",
       "                      1.0158, 1.0004, 1.0271, 1.0135, 0.9900, 1.0088, 1.0091, 1.0097, 1.0114,\n",
       "                      1.0126, 1.0199, 1.0075, 1.0094, 0.9921, 1.0178, 1.0170, 0.9999, 1.0078,\n",
       "                      1.0289, 1.0116, 1.0232, 0.9918, 1.0278, 1.0162, 1.0163, 1.0143, 1.0171,\n",
       "                      1.0012, 1.0124, 1.0058, 1.0176, 1.0214, 1.0164, 1.0175, 0.9925, 1.0205,\n",
       "                      1.0199, 1.0220, 1.0276, 1.0088, 0.9989, 1.0207, 0.9844, 1.0100, 0.9903,\n",
       "                      1.0200, 1.0137, 1.0157, 1.0284, 1.0026, 1.0235, 1.0144, 0.9975, 1.0107,\n",
       "                      0.9946, 1.0160, 1.0098, 1.0153, 1.0039, 1.0201, 0.9765, 1.0178, 1.0109,\n",
       "                      0.9871, 1.0220, 1.0232, 1.0214, 1.0154, 1.0214, 1.0222, 1.0168, 1.0170,\n",
       "                      0.9970, 1.0175, 1.0191])),\n",
       "             ('bert.encoder.layer.1.output.LayerNorm.bias',\n",
       "              tensor([ 4.7838e-03,  9.2875e-03,  1.5588e-02, -1.1936e-03, -6.5596e-03,\n",
       "                      -2.3390e-03,  4.6002e-03, -7.6771e-03,  8.3929e-04, -1.2531e-02,\n",
       "                      -7.0834e-03,  6.7596e-03,  5.6777e-03, -5.4789e-03,  4.4231e-03,\n",
       "                      -4.3397e-03,  7.1818e-03,  2.3758e-03, -6.8010e-03,  4.4563e-03,\n",
       "                       1.6272e-02,  2.6625e-03, -3.5433e-03,  9.3269e-03, -1.5590e-02,\n",
       "                       3.7159e-04,  3.8490e-03,  1.8406e-02, -1.2694e-02,  1.8037e-03,\n",
       "                       1.9439e-03,  7.8224e-05, -1.2720e-03, -4.1243e-03,  1.1673e-03,\n",
       "                       8.2445e-03, -1.0949e-02, -1.8405e-02, -8.1656e-04, -7.9526e-03,\n",
       "                      -4.5551e-03, -3.2771e-03, -4.0931e-04,  1.7777e-03,  6.0147e-05,\n",
       "                      -2.7246e-03,  1.8954e-02, -2.1575e-03, -3.6151e-04,  3.0750e-03,\n",
       "                      -1.6758e-03, -2.0992e-02, -1.2262e-02,  8.1553e-03, -2.7313e-04,\n",
       "                       3.3530e-04, -5.1829e-03,  3.1338e-03, -1.0630e-03,  6.7070e-03,\n",
       "                      -5.0095e-03, -1.3352e-03,  9.4812e-03, -1.2195e-02,  2.1334e-04,\n",
       "                      -7.1667e-04, -5.0779e-03,  2.3775e-02,  2.8310e-03,  5.5980e-03,\n",
       "                       8.7873e-03, -3.9402e-03,  2.0190e-03,  4.7332e-03,  2.8578e-03,\n",
       "                       1.0013e-04, -3.0281e-03,  8.7997e-03,  4.8269e-04, -4.4506e-03,\n",
       "                       2.9802e-03,  1.0649e-03, -4.1210e-03, -1.6752e-03, -4.9637e-04,\n",
       "                      -1.2101e-02, -1.2527e-03, -8.9903e-03, -1.5297e-03, -7.5474e-04,\n",
       "                       1.9755e-04,  1.0610e-02,  5.4583e-04, -3.5086e-03, -1.3418e-02,\n",
       "                       3.3512e-02,  4.4276e-05,  1.1468e-02,  9.1766e-04, -7.1152e-03,\n",
       "                      -7.3256e-03,  8.8283e-03, -5.9651e-03, -4.7070e-03, -6.7600e-03,\n",
       "                       4.9340e-03,  2.5161e-03,  6.0451e-03,  3.1706e-04,  6.6769e-03,\n",
       "                      -1.0414e-02,  4.1434e-03,  3.5273e-03, -3.8538e-03, -2.5601e-02,\n",
       "                       7.1071e-03,  1.0356e-02,  4.2122e-03,  1.7512e-02, -3.8300e-03,\n",
       "                      -1.3688e-02,  1.0256e-02,  6.0881e-03,  3.0463e-03, -1.0022e-02,\n",
       "                       1.5341e-03,  8.8213e-03,  9.7604e-03,  7.8621e-03,  1.4402e-03,\n",
       "                       1.5050e-02,  9.9926e-03, -5.3964e-03,  4.6129e-03, -3.4691e-03,\n",
       "                      -2.7020e-03, -1.7917e-02,  1.0017e-02, -4.9841e-03,  6.3815e-03,\n",
       "                       2.4935e-03,  6.0441e-03,  9.1135e-03,  1.4490e-03, -4.5292e-03,\n",
       "                      -2.6756e-03,  5.0388e-03, -1.4199e-03,  5.4633e-03, -5.0610e-03,\n",
       "                      -1.6378e-03,  1.6677e-02, -8.4573e-03, -3.9287e-03,  1.2297e-02,\n",
       "                       2.0968e-03, -1.4358e-02,  9.1655e-03, -1.1390e-02, -1.8608e-02,\n",
       "                      -6.2014e-03, -1.5260e-03, -5.3729e-03, -7.1456e-03,  8.1209e-03,\n",
       "                       2.0281e-05,  6.1759e-03,  3.8615e-03,  2.8710e-03, -8.5374e-03,\n",
       "                      -1.1390e-02,  2.8684e-03, -2.5413e-03, -5.4253e-03, -5.3328e-04,\n",
       "                       7.2471e-03, -9.4070e-03, -1.8876e-02,  3.3137e-03,  6.5132e-03,\n",
       "                       3.5275e-02, -1.0587e-03, -1.0075e-02, -3.3714e-03, -6.2587e-03,\n",
       "                       4.3743e-03, -1.1047e-02, -5.3022e-03,  1.1838e-02,  1.9324e-02,\n",
       "                      -1.4477e-02, -2.5188e-03])),\n",
       "             ('bert.encoder.layer.2.attention.self.query.weight',\n",
       "              tensor([[-1.2574e-02, -1.3522e-02, -6.1337e-02,  ..., -2.9003e-02,\n",
       "                        2.2076e-02,  6.8969e-03],\n",
       "                      [-1.5904e-02,  4.6852e-02,  4.4527e-02,  ...,  5.5211e-02,\n",
       "                       -7.0165e-02, -2.0977e-02],\n",
       "                      [ 2.6699e-02,  8.7610e-05, -5.5250e-02,  ..., -3.8837e-02,\n",
       "                        3.0681e-02,  9.4270e-03],\n",
       "                      ...,\n",
       "                      [-8.9360e-03, -5.0026e-02,  1.3437e-02,  ..., -5.0383e-02,\n",
       "                       -2.5833e-02,  3.4112e-02],\n",
       "                      [ 2.3810e-02,  1.0665e-02, -1.5869e-02,  ..., -5.2982e-02,\n",
       "                        5.1571e-02,  1.6428e-02],\n",
       "                      [-2.9066e-02, -3.8527e-02, -4.6691e-02,  ...,  7.5747e-03,\n",
       "                       -1.9795e-02,  6.9674e-03]])),\n",
       "             ('bert.encoder.layer.2.attention.self.query.bias',\n",
       "              tensor([-0.0500,  0.0801, -0.0535, -0.0474,  0.0632,  0.0630, -0.0597,  0.0450,\n",
       "                       0.0741,  0.0695,  0.0643, -0.0469, -0.0640,  0.0593, -0.0432, -0.0183,\n",
       "                      -0.0512,  0.0566, -0.0622,  0.0347, -0.0634,  0.0607,  0.0634, -0.0853,\n",
       "                      -0.0122,  0.0060,  0.0491,  0.0751,  0.0042,  0.0556,  0.0495, -0.0438,\n",
       "                      -0.0370,  0.0251,  0.0322,  0.0452,  0.0691, -0.0406, -0.0120, -0.0418,\n",
       "                       0.0117, -0.0607,  0.0518, -0.0552, -0.0623, -0.0205, -0.0404,  0.0504,\n",
       "                      -0.0420,  0.0358, -0.0406, -0.0456,  0.0056, -0.0303,  0.0097,  0.0342,\n",
       "                      -0.0188,  0.0362, -0.0631, -0.0287, -0.0507, -0.0275,  0.0107,  0.0147,\n",
       "                      -0.0439, -0.0587,  0.0607,  0.0548, -0.0635,  0.0215,  0.0460, -0.0331,\n",
       "                       0.0552, -0.0618,  0.0189, -0.0635, -0.0442,  0.0028, -0.0064,  0.0010,\n",
       "                       0.0241, -0.0132, -0.0294,  0.0090,  0.0156,  0.0632, -0.0286,  0.0464,\n",
       "                       0.0791,  0.0067, -0.0664,  0.0261,  0.0364,  0.0733,  0.0715,  0.0151,\n",
       "                       0.0260,  0.0135, -0.0377, -0.0599, -0.0149, -0.0164,  0.0135, -0.0004,\n",
       "                      -0.0076, -0.0456, -0.0320,  0.0548,  0.0026, -0.0093,  0.0270,  0.0184,\n",
       "                      -0.0347, -0.0085, -0.0154, -0.0420,  0.0001, -0.0164, -0.0164, -0.0187,\n",
       "                       0.0376,  0.0439,  0.0437,  0.0168, -0.0193, -0.0123,  0.0300, -0.0184,\n",
       "                      -0.0211, -0.0792, -0.0925, -0.0998, -0.0620, -0.0681,  0.0720, -0.0850,\n",
       "                      -0.0858, -0.0892,  0.0746, -0.0724,  0.0026, -0.0672,  0.0266, -0.0816,\n",
       "                      -0.0743, -0.0867, -0.0607,  0.0618,  0.0173, -0.0842,  0.0485, -0.0001,\n",
       "                      -0.0825,  0.0302,  0.0869,  0.0596, -0.0720, -0.0582, -0.0374, -0.0925,\n",
       "                      -0.0296,  0.0213, -0.0314, -0.0208, -0.0371,  0.0149,  0.0258,  0.0250,\n",
       "                       0.0217, -0.0238, -0.0213, -0.0030,  0.0026, -0.0134, -0.0004,  0.0025,\n",
       "                      -0.0161,  0.0406,  0.0083, -0.0329, -0.0066, -0.0256, -0.0099, -0.0137,\n",
       "                       0.0410, -0.0224,  0.0267,  0.0085, -0.0325, -0.0235, -0.0007, -0.0288])),\n",
       "             ('bert.encoder.layer.2.attention.self.key.weight',\n",
       "              tensor([[ 0.0215,  0.0212, -0.0187,  ...,  0.0431, -0.0145,  0.0180],\n",
       "                      [-0.0013, -0.0073, -0.0047,  ..., -0.0325,  0.0118,  0.0191],\n",
       "                      [-0.0087, -0.0035,  0.0030,  ...,  0.0526, -0.0143, -0.0493],\n",
       "                      ...,\n",
       "                      [ 0.0113,  0.0264,  0.0179,  ...,  0.0280, -0.0033,  0.0113],\n",
       "                      [-0.0344,  0.0419,  0.0042,  ...,  0.0295, -0.0240, -0.0010],\n",
       "                      [ 0.0191, -0.0289, -0.0088,  ...,  0.0244,  0.0034,  0.0417]])),\n",
       "             ('bert.encoder.layer.2.attention.self.key.bias',\n",
       "              tensor([ 2.3807e-08, -3.6490e-07,  2.6833e-07,  4.4148e-07, -3.7282e-07,\n",
       "                      -2.0726e-07, -5.5506e-08, -3.5841e-07, -4.2737e-07, -3.4275e-07,\n",
       "                      -4.5654e-07,  2.5364e-08,  2.9728e-07, -2.6307e-07,  2.4482e-07,\n",
       "                       1.2881e-07,  1.5052e-07, -2.1906e-07,  2.9660e-07,  4.3693e-08,\n",
       "                       4.8779e-07, -3.4627e-07, -2.2586e-07,  8.1846e-08,  7.9647e-09,\n",
       "                      -3.9623e-08, -8.1263e-08, -2.1527e-07,  4.3083e-07, -1.9280e-07,\n",
       "                      -1.7498e-07,  2.0192e-07,  3.5540e-08,  1.2338e-07,  3.5806e-07,\n",
       "                       2.4534e-07, -1.6037e-07,  2.5115e-07,  2.1020e-07, -3.2019e-07,\n",
       "                      -3.0760e-07, -1.0147e-07,  1.9588e-07, -2.4085e-07,  3.6577e-08,\n",
       "                      -7.5107e-08, -2.6710e-07, -8.8543e-08, -1.5235e-07,  3.4709e-07,\n",
       "                      -8.3118e-08, -3.4885e-07,  2.2233e-07, -9.0446e-08, -2.4806e-07,\n",
       "                       3.3873e-07,  4.3423e-07,  4.4275e-08, -2.0709e-07,  3.9633e-08,\n",
       "                      -3.1511e-08,  2.4840e-07,  2.8163e-07, -3.4057e-07, -8.8942e-08,\n",
       "                       6.3816e-08,  5.9827e-08,  1.3214e-07, -1.4959e-07,  2.1912e-07,\n",
       "                      -1.2598e-07, -1.7641e-07,  1.5362e-07, -2.3041e-08,  2.2610e-08,\n",
       "                       1.0443e-07, -5.4823e-08, -1.9823e-07,  5.6622e-08,  4.8568e-08,\n",
       "                      -4.2823e-08,  3.0357e-07, -1.6502e-07,  8.2832e-08,  3.0715e-08,\n",
       "                       7.6459e-08, -8.9241e-09, -1.0067e-07, -1.0172e-07, -6.4372e-08,\n",
       "                       2.8501e-08, -8.9314e-08,  7.3685e-08, -1.8857e-08,  4.4999e-08,\n",
       "                      -7.1605e-08, -2.0934e-07, -5.6173e-08,  4.1727e-08,  1.0430e-07,\n",
       "                       2.4979e-07,  4.5455e-09, -1.5597e-07, -1.7146e-07, -7.7716e-08,\n",
       "                       1.0084e-07, -1.7558e-08,  1.3014e-07,  3.3827e-08, -1.9400e-08,\n",
       "                       6.8879e-08, -2.1742e-07,  9.5722e-09, -1.3291e-07, -1.3128e-07,\n",
       "                      -8.2995e-08, -1.7240e-07,  1.8164e-07,  9.9154e-08,  1.2041e-07,\n",
       "                       2.0205e-08, -2.1857e-08,  1.1281e-07,  1.3162e-08, -1.4102e-07,\n",
       "                      -4.8241e-08, -2.0475e-09,  1.6323e-07,  9.7087e-07,  5.8044e-07,\n",
       "                       7.9279e-07,  1.7105e-06,  9.8411e-07,  3.1612e-08, -2.6420e-07,\n",
       "                       1.5691e-06,  9.9436e-07,  1.5156e-06, -7.3319e-07,  1.1895e-06,\n",
       "                       6.4160e-07,  9.8883e-07, -6.1319e-08,  1.0314e-06,  1.2501e-06,\n",
       "                       1.1603e-06,  1.2269e-06, -8.3107e-08, -1.0798e-06,  1.0475e-06,\n",
       "                      -2.0197e-07, -8.1997e-07,  6.5325e-07, -7.3021e-07, -7.7471e-07,\n",
       "                      -2.9624e-07,  7.7085e-07,  2.1382e-07,  1.4019e-07,  9.5979e-07,\n",
       "                      -6.5133e-08,  3.9806e-07,  9.0069e-08, -5.2629e-08, -1.5075e-08,\n",
       "                      -2.0077e-07,  9.7878e-08, -1.2532e-07,  3.3237e-07,  9.8998e-08,\n",
       "                      -3.5406e-07, -7.2726e-08, -3.9200e-08,  1.6535e-07, -2.5153e-07,\n",
       "                      -1.8292e-07, -1.7856e-08, -2.1195e-07, -8.5752e-08, -1.8259e-07,\n",
       "                       1.6636e-07,  2.3118e-07, -1.7066e-07,  1.4180e-07, -1.2704e-07,\n",
       "                       2.0735e-07,  3.5725e-07,  7.6126e-08,  1.6445e-07,  1.4679e-07,\n",
       "                      -2.2092e-07, -7.2159e-09])),\n",
       "             ('bert.encoder.layer.2.attention.self.value.weight',\n",
       "              tensor([[-0.0017, -0.0017,  0.0092,  ..., -0.0064,  0.0126,  0.0113],\n",
       "                      [-0.0331,  0.0457,  0.0174,  ...,  0.0173, -0.0028,  0.0026],\n",
       "                      [ 0.0261, -0.0125, -0.0152,  ...,  0.0526, -0.0108, -0.0273],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0004, -0.0070,  ..., -0.0280, -0.0120,  0.0077],\n",
       "                      [ 0.0353, -0.0127, -0.0257,  ...,  0.0150,  0.0127,  0.0573],\n",
       "                      [-0.0185, -0.0157,  0.0186,  ...,  0.0256, -0.0201, -0.0117]])),\n",
       "             ('bert.encoder.layer.2.attention.self.value.bias',\n",
       "              tensor([-6.1203e-03,  1.5749e-03,  1.3736e-02, -3.1362e-03,  3.0792e-03,\n",
       "                      -8.0274e-03,  6.6394e-03, -1.7808e-03, -8.0275e-04,  1.5520e-02,\n",
       "                       1.4235e-03, -4.0208e-04,  4.9350e-03,  3.5171e-03,  2.5401e-03,\n",
       "                       5.7035e-03,  4.2536e-03,  5.6843e-03,  8.6992e-03, -3.3763e-03,\n",
       "                       4.2730e-03, -1.0847e-02,  4.6361e-03,  2.2404e-03,  9.5751e-03,\n",
       "                      -8.8333e-03, -2.2022e-04,  1.1624e-03,  5.1054e-03,  4.8555e-04,\n",
       "                      -1.4310e-03,  1.1601e-02,  3.1064e-03,  1.0150e-02, -7.1250e-03,\n",
       "                      -8.2728e-03,  7.7956e-04, -1.7584e-04,  9.0539e-03,  3.5477e-03,\n",
       "                       6.6197e-03, -3.4587e-03, -4.7379e-03,  1.0044e-02,  3.1564e-03,\n",
       "                       1.2809e-03, -6.4757e-03,  2.1506e-05, -3.1291e-03, -6.2689e-03,\n",
       "                      -3.5918e-03, -2.7249e-03,  3.0525e-03,  1.3380e-02,  3.4463e-03,\n",
       "                       2.5070e-03,  2.5159e-03,  4.5943e-03, -1.4167e-03,  5.2827e-03,\n",
       "                      -8.0778e-03, -3.3368e-03, -4.9641e-03, -8.2626e-03,  2.4128e-04,\n",
       "                      -1.4208e-03, -9.8169e-03,  3.1131e-03,  6.7223e-03,  1.2824e-03,\n",
       "                       6.0811e-03,  1.6649e-03,  3.9662e-03,  1.9130e-03,  1.3027e-02,\n",
       "                      -9.3953e-03, -1.1012e-03,  3.1281e-03, -1.4562e-03, -1.8281e-03,\n",
       "                       8.6566e-04, -7.5417e-04, -9.3568e-03,  1.3734e-03, -5.2447e-03,\n",
       "                       4.4798e-03,  1.1281e-02, -7.9397e-03, -5.2452e-03,  4.6141e-03,\n",
       "                      -3.7367e-03,  7.1793e-03, -1.7480e-03, -1.0159e-02, -2.3504e-03,\n",
       "                       2.5041e-03,  1.9783e-03, -5.5852e-03, -3.6490e-03,  2.7246e-03,\n",
       "                      -9.7249e-03,  2.2395e-03, -8.2974e-03,  8.8578e-03, -2.1270e-03,\n",
       "                       6.3976e-03, -1.1143e-03,  1.0060e-03, -1.4321e-03, -1.1575e-02,\n",
       "                       1.3729e-02,  2.4705e-03,  7.3730e-03,  4.2582e-04,  1.2148e-04,\n",
       "                      -1.1052e-02, -4.1793e-03, -8.2457e-03, -3.4696e-03,  7.2811e-03,\n",
       "                       1.1515e-02, -6.1337e-03, -1.5103e-03, -6.5767e-04,  1.5088e-03,\n",
       "                      -1.7873e-03,  4.7839e-03, -8.9833e-04, -5.2971e-04,  6.4702e-03,\n",
       "                       9.1218e-03,  7.5860e-03, -1.4914e-02,  8.2525e-03, -1.4581e-02,\n",
       "                      -8.8605e-03, -1.2759e-02, -5.3989e-03, -2.0981e-03, -2.8988e-04,\n",
       "                       3.5431e-03,  5.1413e-04, -2.1964e-03, -4.2987e-03,  5.1754e-03,\n",
       "                       5.8596e-03,  6.2181e-04,  5.8562e-03,  9.2265e-03, -1.6663e-02,\n",
       "                      -1.1396e-03, -4.3969e-03, -6.6926e-04,  1.3604e-02,  1.3688e-03,\n",
       "                      -6.9448e-03, -7.3391e-04,  4.4048e-03,  2.8672e-03,  4.1782e-03,\n",
       "                      -1.1154e-02,  6.5111e-04, -7.1964e-03,  6.8172e-04, -1.1126e-03,\n",
       "                       2.0446e-03, -1.4573e-03, -8.2868e-03, -5.9292e-03, -1.3839e-02,\n",
       "                       8.3862e-03,  1.4832e-03,  4.8643e-03, -1.0027e-02,  3.6430e-03,\n",
       "                       4.0104e-03,  7.0090e-03, -5.1571e-03, -4.1436e-03,  2.8961e-03,\n",
       "                      -3.2851e-03,  6.5103e-03, -3.7050e-03, -7.7685e-03, -3.2450e-03,\n",
       "                       3.1083e-04,  8.3179e-03, -5.7089e-03,  1.3170e-02, -5.3545e-03,\n",
       "                       5.8450e-05,  7.5299e-04])),\n",
       "             ('bert.encoder.layer.2.attention.output.dense.weight',\n",
       "              tensor([[ 0.0357, -0.0477, -0.0080,  ...,  0.0125,  0.0145, -0.0160],\n",
       "                      [-0.0301,  0.0426, -0.0061,  ...,  0.0033, -0.0321,  0.0236],\n",
       "                      [ 0.0292,  0.0203,  0.0132,  ...,  0.0327,  0.0299, -0.0288],\n",
       "                      ...,\n",
       "                      [-0.0096, -0.0007, -0.0446,  ...,  0.0053, -0.0200,  0.0267],\n",
       "                      [ 0.0462, -0.0027, -0.0144,  ..., -0.0514, -0.0189, -0.0170],\n",
       "                      [ 0.0096,  0.0062, -0.0098,  ...,  0.0154, -0.0612, -0.0010]])),\n",
       "             ('bert.encoder.layer.2.attention.output.dense.bias',\n",
       "              tensor([ 4.6935e-03,  5.0308e-03,  6.0777e-03, -3.6876e-03, -4.0258e-03,\n",
       "                      -7.5838e-03,  5.6245e-03, -8.1512e-03,  3.5593e-03,  2.0706e-03,\n",
       "                      -5.3713e-03,  3.4562e-03,  9.8343e-04, -5.6933e-03, -1.8067e-03,\n",
       "                       7.8039e-04,  5.8223e-03, -1.5497e-04,  3.0137e-03,  4.9496e-03,\n",
       "                       5.2403e-03, -3.8633e-04,  1.8857e-03, -3.3873e-03,  1.1133e-03,\n",
       "                      -6.2412e-03, -1.2783e-04,  1.2097e-02, -9.3171e-03,  4.1395e-03,\n",
       "                      -6.7368e-03,  2.4236e-03,  2.5915e-03, -4.0279e-03,  7.5563e-04,\n",
       "                       4.2600e-03, -8.2971e-04, -1.2576e-02, -3.4547e-03, -2.0164e-03,\n",
       "                      -5.1707e-03, -2.8598e-03, -2.0898e-03, -1.9491e-03,  4.3949e-03,\n",
       "                       5.3430e-04,  1.0115e-02,  7.9214e-05,  4.9336e-03, -1.0764e-03,\n",
       "                       4.6255e-03, -7.9620e-03, -1.1316e-02,  1.0008e-02, -3.9782e-04,\n",
       "                       4.7069e-03, -6.8741e-03, -9.7499e-04, -4.7951e-04,  2.5475e-03,\n",
       "                       5.7891e-04,  1.9139e-03,  3.8320e-03, -1.2950e-03, -9.1838e-04,\n",
       "                       1.1693e-03, -3.9080e-03,  2.2116e-02,  2.8334e-03,  5.2969e-03,\n",
       "                       8.6187e-03,  9.7099e-04,  2.0085e-03,  1.4707e-03, -2.6603e-04,\n",
       "                       1.9063e-03, -3.8664e-03,  3.5667e-03, -2.6871e-03, -4.8647e-03,\n",
       "                       1.6606e-03,  4.0860e-03,  1.3452e-03,  2.2136e-03, -5.7457e-03,\n",
       "                      -4.1387e-03, -6.8131e-04, -2.7414e-03, -8.4025e-03,  1.9428e-03,\n",
       "                       3.9155e-03,  4.9837e-03, -6.4095e-04, -1.2716e-03, -1.3816e-02,\n",
       "                       2.5983e-02, -3.3589e-03,  5.8731e-03,  4.0824e-04, -3.6461e-03,\n",
       "                      -4.1912e-03,  8.8738e-04, -1.2482e-03, -3.9733e-03, -4.0781e-03,\n",
       "                       1.6349e-03,  3.4994e-03,  5.3388e-03,  2.5533e-03, -1.3432e-03,\n",
       "                      -6.4071e-03, -2.2049e-04, -6.2831e-04, -6.2863e-04, -2.0772e-02,\n",
       "                       6.8760e-05,  7.3169e-04, -1.0678e-03,  6.1998e-03,  1.0352e-03,\n",
       "                      -5.7821e-03,  8.9516e-03,  6.6535e-03,  5.0808e-04, -2.9750e-03,\n",
       "                       3.5125e-03,  1.0237e-03,  6.6705e-03,  5.7283e-03,  2.4848e-03,\n",
       "                       8.3635e-03,  5.5824e-03,  1.3888e-03, -3.3481e-03, -2.6104e-03,\n",
       "                      -5.4557e-03, -9.2601e-03,  3.0781e-03, -4.2645e-04,  2.1788e-03,\n",
       "                       7.3094e-03,  6.6308e-04,  4.0988e-03, -2.9161e-03,  6.2183e-03,\n",
       "                       3.9160e-03,  6.8887e-04, -2.2143e-03,  2.7560e-03, -4.4939e-03,\n",
       "                      -8.5349e-04,  7.5221e-03, -2.9200e-03,  2.3349e-04,  9.4602e-03,\n",
       "                       1.7702e-03, -5.9834e-03,  1.0689e-02, -1.0163e-02, -1.2445e-02,\n",
       "                       1.6767e-03,  1.6883e-03, -5.1267e-03, -8.8032e-03,  5.6145e-03,\n",
       "                       7.9905e-05,  7.3493e-03,  7.8481e-03,  1.2244e-03, -3.2563e-03,\n",
       "                      -6.0360e-03,  7.9072e-03, -1.0334e-04,  2.8885e-04, -1.4059e-03,\n",
       "                       9.4339e-03, -5.4435e-04, -4.0162e-03, -1.1298e-03, -2.0818e-04,\n",
       "                       2.8869e-02,  1.2944e-03, -5.2075e-03,  2.4490e-03, -6.6130e-03,\n",
       "                      -4.8910e-03, -3.1891e-03, -3.7702e-03,  8.2120e-03,  1.1629e-02,\n",
       "                      -6.0980e-03,  5.6619e-04])),\n",
       "             ('bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0120, 1.0158, 0.9962, 0.9966, 1.0044, 0.9917, 1.0067, 1.0145, 0.9824,\n",
       "                      1.0159, 0.9870, 0.9927, 1.0000, 1.0039, 0.9863, 1.0077, 1.0156, 0.9752,\n",
       "                      0.9915, 1.0224, 1.0013, 1.0196, 0.9996, 1.0053, 1.0027, 1.0182, 1.0034,\n",
       "                      0.9850, 0.9981, 1.0099, 0.9901, 1.0162, 0.9859, 1.0132, 1.0081, 0.9934,\n",
       "                      1.0025, 0.9786, 1.0101, 1.0069, 1.0049, 1.0034, 1.0075, 0.9941, 1.0046,\n",
       "                      1.0276, 0.9962, 1.0153, 1.0029, 1.0146, 1.0047, 0.9759, 0.9800, 1.0107,\n",
       "                      1.0145, 1.0036, 1.0061, 1.0182, 1.0055, 0.9994, 1.0012, 0.9950, 1.0204,\n",
       "                      1.0077, 1.0076, 1.0002, 1.0077, 0.9742, 1.0006, 1.0074, 0.9976, 0.9811,\n",
       "                      0.9954, 1.0035, 1.0107, 1.0006, 1.0036, 1.0056, 1.0089, 1.0105, 1.0025,\n",
       "                      0.9969, 1.0145, 1.0115, 1.0184, 1.0056, 1.0052, 1.0022, 1.0027, 0.9994,\n",
       "                      0.9854, 1.0049, 1.0118, 1.0078, 1.0006, 0.9814, 1.0037, 0.9827, 1.0068,\n",
       "                      1.0120, 0.9947, 1.0191, 1.0101, 1.0127, 1.0054, 0.9987, 1.0025, 1.0109,\n",
       "                      1.0187, 1.0191, 0.9903, 1.0161, 1.0133, 1.0091, 0.9929, 1.0058, 1.0090,\n",
       "                      1.0099, 0.9918, 1.0144, 1.0037, 0.9797, 1.0009, 0.9987, 1.0097, 1.0106,\n",
       "                      1.0042, 1.0067, 1.0053, 1.0021, 0.9860, 1.0026, 1.0083, 0.9898, 1.0026,\n",
       "                      1.0197, 1.0060, 1.0140, 0.9818, 1.0100, 1.0087, 1.0109, 0.9992, 1.0113,\n",
       "                      0.9892, 1.0066, 0.9961, 1.0088, 1.0126, 1.0070, 1.0014, 0.9825, 1.0101,\n",
       "                      1.0162, 1.0108, 1.0250, 0.9982, 0.9940, 1.0102, 0.9742, 1.0001, 0.9854,\n",
       "                      1.0068, 1.0105, 1.0062, 1.0166, 0.9966, 1.0129, 1.0074, 0.9889, 0.9985,\n",
       "                      0.9873, 1.0065, 0.9971, 1.0124, 0.9998, 1.0112, 0.9680, 1.0109, 1.0023,\n",
       "                      0.9821, 1.0101, 1.0051, 1.0152, 1.0120, 1.0024, 1.0124, 1.0080, 1.0048,\n",
       "                      0.9908, 1.0085, 1.0106])),\n",
       "             ('bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       "              tensor([ 0.0050,  0.0099,  0.0125, -0.0035, -0.0112, -0.0035,  0.0045, -0.0055,\n",
       "                       0.0039, -0.0121, -0.0064,  0.0123,  0.0022, -0.0063,  0.0013, -0.0078,\n",
       "                       0.0065,  0.0031, -0.0078,  0.0016,  0.0163,  0.0017, -0.0082,  0.0079,\n",
       "                      -0.0101, -0.0006,  0.0047,  0.0206, -0.0151,  0.0039, -0.0008, -0.0005,\n",
       "                       0.0002, -0.0059, -0.0023,  0.0082, -0.0064, -0.0194,  0.0002, -0.0096,\n",
       "                      -0.0020, -0.0024, -0.0013,  0.0041, -0.0002, -0.0056,  0.0176, -0.0040,\n",
       "                       0.0017,  0.0014,  0.0024, -0.0193, -0.0156,  0.0078, -0.0013,  0.0025,\n",
       "                      -0.0088,  0.0036, -0.0006,  0.0067, -0.0038,  0.0020,  0.0056, -0.0153,\n",
       "                       0.0025, -0.0034, -0.0073,  0.0275,  0.0030,  0.0092,  0.0083,  0.0011,\n",
       "                       0.0051,  0.0032,  0.0021,  0.0067, -0.0039,  0.0063, -0.0020, -0.0041,\n",
       "                       0.0041,  0.0009, -0.0043, -0.0003, -0.0040, -0.0121,  0.0010, -0.0062,\n",
       "                      -0.0054, -0.0011,  0.0023,  0.0137, -0.0004, -0.0007, -0.0138,  0.0381,\n",
       "                      -0.0003,  0.0115,  0.0015, -0.0044, -0.0083,  0.0106, -0.0079, -0.0015,\n",
       "                      -0.0067,  0.0081,  0.0017,  0.0071,  0.0014,  0.0059, -0.0089,  0.0033,\n",
       "                       0.0030, -0.0038, -0.0339,  0.0116,  0.0107, -0.0020,  0.0147, -0.0058,\n",
       "                      -0.0127,  0.0085,  0.0063,  0.0033, -0.0057,  0.0002,  0.0071,  0.0074,\n",
       "                       0.0096, -0.0015,  0.0187,  0.0094, -0.0062,  0.0032, -0.0024, -0.0035,\n",
       "                      -0.0175,  0.0097, -0.0036,  0.0066,  0.0009,  0.0027,  0.0026, -0.0020,\n",
       "                      -0.0039, -0.0025,  0.0062, -0.0026,  0.0026, -0.0071, -0.0029,  0.0198,\n",
       "                      -0.0096, -0.0055,  0.0119,  0.0002, -0.0115,  0.0160, -0.0074, -0.0184,\n",
       "                      -0.0062, -0.0015, -0.0089, -0.0107,  0.0059,  0.0002,  0.0083,  0.0015,\n",
       "                       0.0029, -0.0068, -0.0111,  0.0035, -0.0008, -0.0041, -0.0015,  0.0113,\n",
       "                      -0.0075, -0.0135,  0.0019,  0.0028,  0.0424, -0.0004, -0.0106, -0.0051,\n",
       "                      -0.0034,  0.0011, -0.0128, -0.0063,  0.0142,  0.0197, -0.0159, -0.0020])),\n",
       "             ('bert.encoder.layer.2.intermediate.dense.weight',\n",
       "              tensor([[-0.0320, -0.0336, -0.0015,  ...,  0.0230,  0.0415, -0.0268],\n",
       "                      [-0.0039, -0.0068,  0.0485,  ..., -0.0179, -0.0144,  0.0077],\n",
       "                      [ 0.0284, -0.0066, -0.0234,  ..., -0.0354, -0.0091,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0044, -0.0514,  0.0242,  ..., -0.0048,  0.0416, -0.0434],\n",
       "                      [-0.0116, -0.0149,  0.0098,  ..., -0.0007, -0.0393, -0.0337],\n",
       "                      [ 0.0153, -0.0262, -0.0121,  ..., -0.0337,  0.0270,  0.0208]])),\n",
       "             ('bert.encoder.layer.2.intermediate.dense.bias',\n",
       "              tensor([-0.0081, -0.0105,  0.0005,  0.0210, -0.0078, -0.0063,  0.0030, -0.0189,\n",
       "                      -0.0059,  0.0048,  0.0094, -0.0055, -0.0082, -0.0085,  0.0102, -0.0136,\n",
       "                       0.0077, -0.0086,  0.0055,  0.0013,  0.0014,  0.0068, -0.0040,  0.0014,\n",
       "                      -0.0009, -0.0074, -0.0133, -0.0170, -0.0015, -0.0016, -0.0192, -0.0048,\n",
       "                      -0.0089, -0.0211,  0.0163, -0.0003,  0.0037,  0.0104, -0.0002, -0.0065,\n",
       "                      -0.0233, -0.0077, -0.0034, -0.0054, -0.0201, -0.0164, -0.0077,  0.0006,\n",
       "                      -0.0039, -0.0097,  0.0023,  0.0009,  0.0039,  0.0076, -0.0077, -0.0164,\n",
       "                       0.0054,  0.0046, -0.0156, -0.0168, -0.0050,  0.0055, -0.0033, -0.0053])),\n",
       "             ('bert.encoder.layer.2.output.dense.weight',\n",
       "              tensor([[ 0.0020,  0.0419, -0.0123,  ...,  0.0184, -0.0313, -0.0184],\n",
       "                      [ 0.0250, -0.0098, -0.0013,  ..., -0.0221,  0.0183,  0.0041],\n",
       "                      [ 0.0233, -0.0390,  0.0268,  ..., -0.0542, -0.0058,  0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0349, -0.0097,  0.0605,  ..., -0.0313,  0.0372,  0.0264],\n",
       "                      [ 0.0086,  0.0118,  0.0015,  ..., -0.0440,  0.0035,  0.0227],\n",
       "                      [ 0.0133,  0.0402,  0.0290,  ...,  0.0519, -0.0365, -0.0162]])),\n",
       "             ('bert.encoder.layer.2.output.dense.bias',\n",
       "              tensor([ 2.1823e-03,  6.7116e-03,  1.2704e-02, -2.3155e-03, -9.9336e-03,\n",
       "                      -3.9426e-03,  1.2531e-03, -6.2298e-03,  2.9201e-03, -5.7606e-03,\n",
       "                       3.5482e-03,  1.0180e-02,  4.1485e-04,  1.6579e-03, -1.8932e-03,\n",
       "                      -7.2143e-03,  1.2732e-03,  1.1813e-03, -4.9149e-03,  2.0036e-03,\n",
       "                       7.8071e-03,  1.5414e-03, -1.0039e-02,  8.7046e-03, -6.7112e-03,\n",
       "                      -1.4837e-03,  2.0407e-03,  1.6915e-02, -1.4201e-02,  5.3542e-03,\n",
       "                       5.5476e-04, -3.3299e-03, -3.2254e-04, -5.2180e-03, -4.3836e-04,\n",
       "                       1.1700e-02,  2.2470e-03, -1.4369e-02,  4.5484e-04, -5.7541e-03,\n",
       "                      -2.6257e-03, -2.2439e-03, -1.6325e-03,  7.7926e-03,  1.5040e-03,\n",
       "                      -1.5567e-03,  8.1435e-03, -1.1852e-03, -2.9397e-03,  1.6583e-03,\n",
       "                      -3.1105e-03, -1.5308e-02, -1.2954e-02,  1.9241e-03,  3.0357e-03,\n",
       "                       2.3867e-03, -3.8626e-03,  3.3762e-03,  5.0033e-04,  1.1219e-03,\n",
       "                      -4.0159e-03, -2.4707e-04,  1.4872e-03, -1.0896e-02,  3.5045e-03,\n",
       "                      -1.3419e-03, -5.7721e-03,  2.2291e-02,  2.3085e-03,  7.2689e-03,\n",
       "                       5.5731e-03, -1.8823e-03,  7.4372e-03,  5.7312e-03,  5.3253e-04,\n",
       "                       4.8321e-03, -6.1038e-03,  6.9181e-03, -1.4189e-03, -6.1021e-03,\n",
       "                       8.9835e-03,  5.0800e-03, -3.9200e-03, -4.6759e-03, -9.1324e-03,\n",
       "                      -1.2901e-02,  4.6957e-03, -8.1624e-03, -3.4440e-03, -3.7136e-03,\n",
       "                       9.0541e-03,  5.2773e-03,  2.9927e-03,  1.1578e-05, -1.5563e-02,\n",
       "                       2.8072e-02,  1.2487e-03,  8.8785e-03,  4.7999e-03, -2.4269e-03,\n",
       "                      -9.5283e-03,  4.4689e-03, -7.5395e-03,  1.8939e-03, -2.0008e-03,\n",
       "                       9.1678e-03,  2.7114e-03,  1.0188e-02,  2.2862e-03,  9.3103e-03,\n",
       "                      -4.4861e-03,  7.2991e-03,  3.2751e-04, -2.7185e-03, -3.3558e-02,\n",
       "                       7.2138e-03,  1.1499e-02, -4.4646e-04,  9.3124e-03, -5.9252e-03,\n",
       "                      -6.3088e-03,  8.0512e-03,  4.8593e-03, -1.0386e-03, -3.6733e-03,\n",
       "                       2.8375e-03,  7.4881e-03,  5.6591e-04,  6.0997e-03, -1.6415e-03,\n",
       "                       1.5007e-02,  7.9227e-03, -5.3053e-03,  1.9586e-03, -3.5426e-03,\n",
       "                      -1.9163e-04, -1.5803e-02,  3.0777e-03,  1.0036e-04,  8.9051e-03,\n",
       "                       7.4148e-03,  2.9688e-03,  4.3280e-04, -2.7608e-03,  6.6411e-04,\n",
       "                      -2.4595e-03,  4.5315e-03,  1.7159e-03,  4.9969e-03, -1.1542e-02,\n",
       "                      -4.0834e-03,  9.7642e-03, -1.1036e-02, -5.5086e-03,  3.4306e-03,\n",
       "                      -1.2262e-02, -1.3073e-02,  9.4034e-03, -1.9874e-03, -1.4390e-02,\n",
       "                      -8.2292e-03, -1.9813e-03, -7.4356e-03, -7.9218e-03,  2.8291e-03,\n",
       "                       3.1243e-04,  8.5731e-03, -1.1134e-03,  6.8955e-03, -8.1067e-03,\n",
       "                      -8.2831e-03,  2.0134e-03, -1.0684e-03, -4.9893e-03,  2.0596e-03,\n",
       "                       8.2528e-03, -9.9920e-03, -8.8664e-03, -1.1838e-03,  2.4478e-04,\n",
       "                       3.6016e-02, -5.1380e-04, -4.9852e-03, -8.5846e-03, -6.3456e-03,\n",
       "                      -3.9168e-04, -1.0218e-02, -8.0220e-03,  1.1333e-02,  1.7585e-02,\n",
       "                      -1.1816e-02,  7.5592e-04])),\n",
       "             ('bert.encoder.layer.2.output.LayerNorm.weight',\n",
       "              tensor([1.0261, 1.0291, 1.0091, 1.0046, 1.0172, 0.9965, 1.0193, 1.0279, 0.9985,\n",
       "                      1.0227, 0.9921, 1.0078, 1.0130, 1.0136, 1.0049, 1.0196, 1.0251, 0.9926,\n",
       "                      0.9991, 1.0314, 1.0146, 1.0289, 1.0102, 1.0172, 1.0127, 1.0309, 1.0192,\n",
       "                      0.9900, 0.9981, 1.0283, 1.0089, 1.0305, 1.0003, 1.0254, 1.0217, 1.0010,\n",
       "                      1.0186, 0.9767, 1.0240, 1.0189, 1.0173, 1.0177, 1.0220, 1.0105, 1.0208,\n",
       "                      1.0360, 1.0072, 1.0344, 1.0129, 1.0282, 1.0197, 0.9704, 0.9689, 1.0263,\n",
       "                      1.0281, 1.0175, 1.0198, 1.0240, 1.0195, 1.0124, 1.0132, 1.0058, 1.0350,\n",
       "                      1.0140, 1.0228, 1.0158, 1.0205, 0.9588, 1.0180, 1.0124, 1.0091, 0.9934,\n",
       "                      1.0119, 1.0201, 1.0258, 1.0121, 1.0207, 1.0165, 1.0225, 1.0297, 1.0119,\n",
       "                      1.0139, 1.0271, 1.0258, 1.0345, 1.0172, 1.0205, 1.0125, 1.0188, 1.0118,\n",
       "                      1.0011, 1.0112, 1.0289, 1.0193, 1.0078, 0.9737, 1.0127, 0.9813, 1.0232,\n",
       "                      1.0246, 1.0143, 1.0284, 1.0246, 1.0235, 1.0263, 1.0133, 1.0181, 1.0208,\n",
       "                      1.0337, 1.0341, 0.9948, 1.0286, 1.0270, 1.0180, 0.9453, 1.0161, 1.0169,\n",
       "                      1.0210, 1.0058, 1.0293, 1.0079, 0.9818, 1.0134, 1.0126, 1.0261, 1.0242,\n",
       "                      1.0212, 1.0206, 1.0223, 1.0161, 0.9822, 1.0122, 1.0219, 1.0069, 1.0164,\n",
       "                      1.0337, 1.0069, 1.0250, 0.9894, 1.0288, 1.0232, 1.0212, 1.0150, 1.0265,\n",
       "                      0.9943, 1.0156, 1.0002, 1.0201, 1.0263, 1.0241, 1.0147, 0.9745, 1.0217,\n",
       "                      1.0275, 1.0241, 1.0376, 1.0109, 1.0049, 1.0220, 0.9719, 1.0141, 0.9957,\n",
       "                      1.0153, 1.0246, 1.0154, 1.0306, 1.0114, 1.0236, 1.0230, 1.0026, 1.0020,\n",
       "                      1.0029, 1.0219, 1.0123, 1.0244, 1.0095, 1.0241, 0.9695, 1.0232, 1.0150,\n",
       "                      0.9555, 1.0226, 1.0168, 1.0295, 1.0254, 1.0155, 1.0238, 1.0241, 1.0112,\n",
       "                      0.9951, 1.0133, 1.0277])),\n",
       "             ('bert.encoder.layer.2.output.LayerNorm.bias',\n",
       "              tensor([ 3.3512e-03,  8.8127e-03,  1.1079e-02, -2.1774e-03, -7.6391e-03,\n",
       "                      -5.7844e-03,  2.1004e-03, -6.3386e-03,  5.7248e-03, -1.0400e-02,\n",
       "                       3.4213e-03,  1.1898e-02, -1.3744e-03, -4.8509e-04,  5.2485e-03,\n",
       "                      -3.2251e-03,  6.7184e-03,  7.8344e-03, -5.9025e-03,  5.0458e-04,\n",
       "                       1.1237e-02,  1.6407e-05, -6.8733e-03,  5.7231e-03, -4.9072e-03,\n",
       "                      -2.1673e-03,  5.8187e-03,  1.8759e-02, -7.9952e-03,  4.1126e-03,\n",
       "                       5.0888e-03, -5.7879e-04, -1.9313e-03, -8.7078e-03, -2.8839e-03,\n",
       "                       5.2217e-03, -6.2616e-03, -1.4000e-02,  2.8068e-03, -1.0579e-02,\n",
       "                       8.7289e-04, -3.3214e-03, -3.0084e-03,  7.0316e-03, -8.1127e-04,\n",
       "                      -4.6352e-03,  1.0855e-02, -2.0037e-03, -7.6606e-05,  1.5065e-03,\n",
       "                      -1.1415e-03, -1.2679e-02, -5.8900e-03,  5.7137e-03, -1.0445e-03,\n",
       "                      -1.3402e-03, -6.9456e-03,  1.5331e-03, -1.3555e-03,  3.9589e-03,\n",
       "                      -6.0459e-03, -1.3023e-03,  8.3202e-03, -1.2386e-02,  1.1924e-03,\n",
       "                      -3.5431e-03, -6.2267e-03,  1.9579e-02,  4.4950e-04,  5.3629e-03,\n",
       "                       6.6139e-03, -3.5413e-03,  7.1701e-04,  4.8453e-03,  1.0209e-03,\n",
       "                       7.8815e-03, -1.4818e-03,  6.8524e-03, -6.8403e-04, -2.4926e-03,\n",
       "                       1.0990e-03,  3.1836e-03, -6.6351e-03, -3.7197e-03, -2.6400e-03,\n",
       "                      -1.2280e-02, -2.8555e-03, -6.7070e-03,  1.0534e-03, -3.4360e-03,\n",
       "                      -5.0756e-03,  1.0823e-02,  2.2922e-03,  8.2443e-04, -1.4221e-02,\n",
       "                       2.8686e-02,  4.2564e-04,  2.2809e-03,  1.9577e-03, -4.0882e-03,\n",
       "                      -9.6998e-03,  1.1948e-02, -6.1557e-03, -1.8709e-03, -1.9979e-03,\n",
       "                       8.4086e-03, -5.5107e-04,  8.5797e-03, -2.7344e-03,  6.2103e-03,\n",
       "                      -8.0141e-03,  3.4647e-03,  4.4044e-03, -6.8791e-03, -1.8696e-02,\n",
       "                       1.1568e-02,  7.0576e-03, -2.0883e-03,  1.1952e-02, -9.5230e-03,\n",
       "                      -9.9010e-03, -4.8517e-03,  5.8113e-03,  1.1308e-03, -4.4222e-03,\n",
       "                       9.3849e-04,  8.8469e-03,  6.4021e-03,  1.0976e-02, -2.6073e-03,\n",
       "                       1.0523e-02,  6.8640e-03, -1.3714e-04, -1.0117e-03, -1.8070e-03,\n",
       "                       1.2943e-03, -1.1813e-02,  8.8105e-03,  7.4961e-04,  9.4783e-03,\n",
       "                       5.8836e-04,  3.5473e-03,  5.8790e-03, -4.7261e-04, -2.5436e-03,\n",
       "                      -5.9839e-03,  4.5318e-03, -8.0461e-04,  3.2724e-03, -6.2537e-03,\n",
       "                      -2.0234e-03,  1.1730e-02, -7.5721e-03, -5.3491e-03,  3.5350e-03,\n",
       "                       6.0966e-04, -1.2151e-02,  9.4887e-03, -3.5813e-03, -1.2714e-02,\n",
       "                      -6.2153e-03,  5.3165e-04, -5.9954e-03, -6.1385e-03,  3.1338e-03,\n",
       "                       4.3252e-03,  4.4835e-03,  4.5967e-03,  4.6114e-03, -7.4222e-03,\n",
       "                      -8.3858e-03, -4.4001e-03,  1.8684e-03, -5.4057e-03,  2.8061e-03,\n",
       "                       4.6258e-03, -6.7209e-03, -1.0152e-02,  2.8504e-03,  2.9119e-03,\n",
       "                       2.7621e-02, -7.0954e-04, -1.0383e-02, -5.9674e-03, -1.6554e-03,\n",
       "                       2.6146e-03, -1.1885e-02, -1.9005e-03,  9.7558e-03,  1.4189e-02,\n",
       "                      -1.1359e-02, -5.0127e-04])),\n",
       "             ('bert.encoder.layer.3.attention.self.query.weight',\n",
       "              tensor([[ 0.0110,  0.0406,  0.0098,  ...,  0.0119, -0.0229,  0.0372],\n",
       "                      [ 0.0235, -0.0536, -0.0077,  ..., -0.0757,  0.0138, -0.0166],\n",
       "                      [-0.0259,  0.0031,  0.0402,  ...,  0.0053,  0.0150, -0.0190],\n",
       "                      ...,\n",
       "                      [-0.0049,  0.0500,  0.0032,  ...,  0.0011,  0.0102, -0.0074],\n",
       "                      [ 0.0437, -0.0411, -0.0487,  ..., -0.0246,  0.0724,  0.0092],\n",
       "                      [-0.0577,  0.0056,  0.0158,  ..., -0.0189,  0.0365, -0.0312]])),\n",
       "             ('bert.encoder.layer.3.attention.self.query.bias',\n",
       "              tensor([ 3.2901e-02, -4.9334e-02,  2.5452e-02,  1.6293e-02,  4.4585e-02,\n",
       "                       1.1363e-02,  3.0512e-02, -4.0213e-02, -1.6464e-03,  7.8905e-04,\n",
       "                       4.8068e-02, -2.6590e-02,  3.6246e-02, -2.5093e-02,  4.6015e-02,\n",
       "                       1.7574e-02, -2.8924e-02, -1.9503e-02,  1.1198e-02,  3.2882e-02,\n",
       "                      -3.8286e-02, -3.8549e-03, -5.8971e-03, -2.5265e-03,  5.8645e-02,\n",
       "                       8.0232e-03, -1.9114e-02,  3.3959e-03, -3.0322e-03,  3.5608e-03,\n",
       "                       1.7200e-05,  1.4635e-02,  5.8760e-02, -6.0225e-02, -4.7268e-02,\n",
       "                      -3.1246e-02,  3.2353e-02, -1.7711e-02,  5.7022e-02, -3.3240e-02,\n",
       "                       4.6604e-03,  3.7349e-02,  3.8205e-02,  2.4539e-02, -4.9396e-02,\n",
       "                      -7.6584e-03, -6.6734e-02, -6.4709e-02, -6.9279e-02,  7.7576e-02,\n",
       "                      -4.2825e-02, -8.8381e-03,  2.2474e-02, -6.6566e-02, -5.8422e-02,\n",
       "                       4.2459e-02,  7.0508e-02,  3.4418e-03,  5.5254e-02,  7.1903e-02,\n",
       "                      -4.7509e-02,  2.4753e-02,  3.0810e-02,  5.5326e-02, -6.0268e-02,\n",
       "                      -1.3401e-02, -2.2086e-02,  4.9004e-02,  2.1529e-03,  4.4209e-03,\n",
       "                       4.0599e-02, -1.8622e-02,  3.6232e-02,  2.9126e-03,  4.8000e-02,\n",
       "                      -2.4404e-02, -3.1610e-03,  5.4400e-02, -1.9127e-02, -3.4440e-02,\n",
       "                       5.0261e-02, -2.0059e-03,  1.6901e-02, -4.5130e-02, -3.9578e-02,\n",
       "                       2.1306e-02,  1.7576e-02, -4.3185e-02,  3.7396e-02,  4.1597e-02,\n",
       "                      -5.2699e-02, -3.2559e-02, -4.1314e-02,  5.7899e-02, -1.6816e-02,\n",
       "                       2.1415e-02, -3.0000e-02,  4.8942e-02, -2.7042e-02,  3.9021e-02,\n",
       "                       4.0421e-02,  3.5248e-02, -2.9782e-03,  8.2449e-03, -8.6755e-05,\n",
       "                       2.1400e-02,  2.9645e-02,  1.2691e-02,  3.1908e-02, -8.5846e-03,\n",
       "                      -3.7249e-02, -9.6624e-03, -2.2115e-02,  1.3330e-02,  3.8750e-02,\n",
       "                      -5.5364e-02, -6.7697e-03, -4.2183e-02,  1.4613e-02,  3.1915e-02,\n",
       "                      -2.4727e-02, -3.7478e-02, -2.2740e-02, -1.0781e-02, -3.4125e-02,\n",
       "                       2.2843e-02,  1.5559e-03, -5.0710e-03, -6.0826e-03, -1.3012e-02,\n",
       "                      -4.4828e-03, -1.2566e-02, -2.6976e-03, -4.3738e-03,  1.8742e-02,\n",
       "                       2.2853e-02, -1.4404e-02, -8.9888e-05, -2.9523e-02,  2.3092e-02,\n",
       "                       4.8299e-03,  1.7605e-02,  2.9965e-02, -1.5475e-03,  2.2210e-02,\n",
       "                      -1.3031e-03, -1.4312e-02,  4.6779e-03, -2.7967e-02,  1.0320e-02,\n",
       "                      -4.3592e-03, -1.6641e-02,  9.1669e-03,  1.7780e-03,  1.1202e-02,\n",
       "                      -8.4453e-03,  3.5242e-03,  8.9441e-03, -1.1205e-02, -5.4037e-03,\n",
       "                       2.0616e-02, -9.3902e-03,  3.5607e-02,  1.4446e-02,  1.2137e-02,\n",
       "                       1.3063e-02, -2.9421e-02, -1.0368e-02, -2.4799e-02, -1.5067e-02,\n",
       "                       2.7057e-02,  2.6636e-02,  1.4994e-02, -4.0596e-03, -3.4710e-02,\n",
       "                      -3.3708e-02,  2.1217e-02, -2.8080e-02,  2.0037e-02,  2.6049e-02,\n",
       "                       1.0594e-02, -2.6400e-02, -1.9938e-02,  4.7311e-03, -3.0988e-02,\n",
       "                      -2.3656e-02, -2.9184e-02,  2.3544e-02,  3.5406e-02,  1.1100e-02,\n",
       "                      -2.8895e-02, -1.3023e-02])),\n",
       "             ('bert.encoder.layer.3.attention.self.key.weight',\n",
       "              tensor([[ 0.0177, -0.0230, -0.0351,  ..., -0.0391,  0.0333,  0.0256],\n",
       "                      [-0.0288,  0.0536, -0.0041,  ...,  0.0195, -0.0117, -0.0002],\n",
       "                      [-0.0445, -0.0389,  0.0101,  ..., -0.0268, -0.0007, -0.0383],\n",
       "                      ...,\n",
       "                      [-0.0040,  0.0338, -0.0065,  ...,  0.0180,  0.0051,  0.0354],\n",
       "                      [-0.0055,  0.0280, -0.0449,  ..., -0.0089,  0.0156,  0.0059],\n",
       "                      [ 0.0394,  0.0219,  0.0027,  ...,  0.0138,  0.0064, -0.0181]])),\n",
       "             ('bert.encoder.layer.3.attention.self.key.bias',\n",
       "              tensor([ 5.3563e-10, -1.6085e-07,  1.2939e-07,  4.3942e-08,  5.0748e-08,\n",
       "                       1.6224e-08, -6.6816e-08, -2.2554e-08,  1.4262e-09,  8.7609e-08,\n",
       "                       7.4574e-08,  2.7148e-08, -2.3052e-08, -8.7300e-10, -5.3753e-08,\n",
       "                      -1.1004e-07, -1.7770e-08,  1.0519e-08,  7.8285e-08, -2.4086e-08,\n",
       "                      -1.2275e-07, -2.1477e-07,  6.8743e-08,  2.3552e-08, -1.0911e-07,\n",
       "                       6.7188e-08,  5.5589e-08, -2.2339e-08,  2.0400e-08,  9.6739e-08,\n",
       "                      -9.1353e-08, -4.5930e-08,  5.6597e-08, -3.5141e-07,  1.5809e-08,\n",
       "                      -1.8578e-07,  6.0804e-08, -1.5012e-07,  9.1533e-08,  8.3632e-08,\n",
       "                       1.7167e-07, -1.0962e-07,  5.1810e-07,  4.3196e-08, -9.6417e-09,\n",
       "                       7.1597e-08, -1.4568e-07, -2.7695e-08, -2.4699e-07,  5.2898e-07,\n",
       "                      -5.5736e-08, -2.2391e-08, -3.0631e-08,  4.6873e-08, -3.1657e-07,\n",
       "                       2.9216e-07,  5.2325e-08,  8.5813e-09,  5.9471e-07,  1.5512e-07,\n",
       "                      -2.2444e-07,  5.5688e-08,  1.2846e-07, -5.2294e-08,  4.4439e-07,\n",
       "                       1.3684e-07,  3.2503e-07, -7.1299e-08,  2.0151e-07,  1.9959e-07,\n",
       "                      -1.8904e-07,  2.7277e-07, -5.6985e-07,  1.2478e-07, -1.7752e-07,\n",
       "                       1.2408e-07, -1.5399e-07, -5.1650e-07,  2.3308e-07,  6.4127e-08,\n",
       "                      -2.4029e-07, -2.3148e-07, -3.6219e-07,  4.1882e-07,  1.6120e-07,\n",
       "                      -4.1035e-07, -1.7950e-07,  4.4446e-07, -5.0789e-07, -3.4104e-07,\n",
       "                       1.8680e-07,  2.7647e-07,  1.9832e-07, -1.4439e-07, -1.2890e-07,\n",
       "                      -1.7469e-07,  2.1942e-07, -5.5538e-08, -4.8875e-08, -1.2444e-08,\n",
       "                       9.9798e-08,  1.6766e-07, -6.9852e-08, -1.5045e-07,  1.4068e-07,\n",
       "                       6.1129e-08, -7.4726e-08,  1.8481e-07, -1.3153e-07,  1.1632e-07,\n",
       "                      -9.1448e-09,  1.0375e-07, -6.4898e-08, -1.3408e-07, -4.4638e-08,\n",
       "                       1.9798e-07,  1.7086e-07,  4.3683e-10,  4.4537e-08,  7.3824e-08,\n",
       "                      -2.6307e-08, -1.7064e-08, -2.4165e-07,  4.0079e-08, -7.2160e-08,\n",
       "                      -6.9494e-08,  2.4145e-07, -3.8945e-08, -5.1898e-08, -7.6980e-08,\n",
       "                      -7.7216e-08, -3.0658e-07, -1.2276e-07, -1.3014e-07, -8.9628e-08,\n",
       "                      -3.6159e-09, -5.3750e-08,  8.8832e-08, -1.1036e-07,  9.0500e-08,\n",
       "                      -1.3540e-09, -4.3703e-08,  1.6751e-07, -4.2738e-08,  2.3774e-07,\n",
       "                       1.0146e-07, -2.4956e-07,  3.4273e-08, -2.4180e-07, -8.3279e-08,\n",
       "                      -2.5668e-08, -8.7027e-08, -1.2627e-07, -2.3913e-08, -4.5487e-08,\n",
       "                      -4.1831e-08,  1.1381e-07, -1.6146e-08,  1.0137e-07,  1.5459e-09,\n",
       "                      -1.6850e-07,  3.8429e-08,  1.8465e-07,  2.6109e-07,  3.3813e-07,\n",
       "                       2.0579e-07, -3.5166e-07,  1.7126e-08, -4.5112e-09, -2.4239e-07,\n",
       "                       3.3021e-07,  3.1580e-07,  2.3082e-07,  1.6754e-07, -2.9404e-07,\n",
       "                      -1.6188e-07,  5.7684e-08, -2.5888e-07,  4.8006e-07,  3.2660e-07,\n",
       "                       4.1979e-07, -3.1477e-07, -2.1494e-07,  3.0562e-08, -2.1136e-07,\n",
       "                       7.4604e-08, -3.1284e-07,  7.0958e-08,  4.7593e-07, -1.5302e-07,\n",
       "                      -5.4376e-07, -3.4538e-07])),\n",
       "             ('bert.encoder.layer.3.attention.self.value.weight',\n",
       "              tensor([[-0.0299, -0.0214, -0.0276,  ...,  0.0228, -0.0211,  0.0197],\n",
       "                      [ 0.0173,  0.0047, -0.0018,  ...,  0.0173,  0.0133,  0.0199],\n",
       "                      [-0.0005,  0.0094, -0.0489,  ...,  0.0184,  0.0459,  0.0060],\n",
       "                      ...,\n",
       "                      [-0.0006, -0.0186, -0.0038,  ...,  0.0357, -0.0158, -0.0052],\n",
       "                      [-0.0103,  0.0440,  0.0129,  ...,  0.0076, -0.0146,  0.0095],\n",
       "                      [-0.0221, -0.0151,  0.0156,  ...,  0.0074,  0.0277,  0.0396]])),\n",
       "             ('bert.encoder.layer.3.attention.self.value.bias',\n",
       "              tensor([ 1.0894e-02, -6.2145e-04,  4.2166e-03,  4.2378e-03, -5.9663e-03,\n",
       "                      -7.8271e-03,  1.1173e-02, -2.6743e-03, -3.5950e-03,  7.0469e-04,\n",
       "                      -9.5947e-05,  1.1416e-02,  2.6199e-03, -1.3213e-02,  1.8703e-02,\n",
       "                      -7.8746e-03,  9.8747e-03,  7.2780e-03,  9.1517e-03, -2.1390e-03,\n",
       "                       6.8756e-03,  6.1301e-03, -7.9001e-03,  2.3845e-03, -5.0652e-03,\n",
       "                      -1.7227e-03, -5.6862e-03,  3.6285e-03, -8.0403e-03, -6.6759e-03,\n",
       "                       1.9593e-02,  1.1217e-02,  1.0021e-03,  2.8888e-03, -1.2646e-03,\n",
       "                       4.9359e-03, -5.4225e-04, -6.4773e-03, -6.3523e-03,  1.0257e-03,\n",
       "                       4.1695e-03, -5.3843e-03, -1.0175e-02, -6.4957e-03,  1.8747e-03,\n",
       "                      -2.0485e-04, -7.2867e-03, -1.3397e-03,  8.0873e-03,  1.7935e-03,\n",
       "                      -9.5811e-03,  8.2366e-03, -8.0056e-03, -1.1170e-02, -1.1883e-02,\n",
       "                       3.1263e-03, -2.0730e-03, -7.0817e-03, -2.5321e-03,  4.4335e-03,\n",
       "                       3.8507e-03,  1.5111e-02, -1.7800e-03,  7.7976e-03, -1.0789e-02,\n",
       "                      -2.9693e-03, -1.0564e-02, -3.2817e-03, -2.9575e-03,  1.7290e-02,\n",
       "                       6.4951e-03,  6.8433e-03, -5.6951e-03, -1.3433e-03, -3.0443e-03,\n",
       "                      -1.6045e-02,  5.9730e-03, -4.3585e-03,  5.2116e-03,  3.6606e-03,\n",
       "                      -1.9097e-03,  2.0892e-03,  9.8158e-03,  6.4093e-03,  4.8687e-03,\n",
       "                      -1.2207e-02, -4.3839e-03,  1.4009e-02,  6.1161e-03,  2.3729e-03,\n",
       "                      -3.8167e-03, -7.6904e-03, -3.8983e-03,  3.4703e-03, -3.3389e-03,\n",
       "                       6.8303e-03,  7.3844e-04, -5.8052e-03, -7.8351e-03,  1.5574e-03,\n",
       "                       5.3700e-04,  7.4153e-03,  1.4272e-03,  2.8782e-03, -9.0815e-04,\n",
       "                       3.2956e-03, -2.0219e-03,  5.9701e-03,  1.0758e-03,  1.4271e-03,\n",
       "                      -5.5153e-03,  2.3545e-03,  4.5626e-03, -7.2818e-04,  2.7517e-03,\n",
       "                      -1.6697e-02,  5.8910e-03,  4.4620e-03,  6.1108e-03,  2.8632e-03,\n",
       "                      -1.1115e-03, -1.0985e-02,  9.3088e-04,  1.9707e-03, -1.6024e-02,\n",
       "                      -2.3296e-03, -7.9012e-03,  2.8248e-03, -5.4187e-03, -6.2757e-03,\n",
       "                      -1.1676e-02,  7.0333e-03,  1.5120e-02,  3.4615e-03,  9.9775e-03,\n",
       "                      -8.8135e-03, -1.3032e-02, -1.1849e-02,  3.0375e-03, -4.5224e-03,\n",
       "                       5.1975e-03, -9.7422e-03,  5.4366e-04,  1.1508e-02,  1.5197e-03,\n",
       "                      -1.4065e-02, -5.1282e-04, -2.7967e-03, -3.2267e-03,  8.3499e-03,\n",
       "                      -4.4898e-03,  4.3462e-03,  1.0045e-02, -6.7469e-03,  1.9518e-03,\n",
       "                       9.5866e-03, -3.5262e-03,  1.7222e-02, -1.8897e-04, -9.4702e-03,\n",
       "                       1.3031e-02,  1.9377e-03, -7.0928e-03,  1.8955e-03,  4.5047e-03,\n",
       "                      -4.2103e-03,  4.5606e-03,  1.1067e-02, -1.6309e-02, -6.6555e-03,\n",
       "                      -4.3718e-03,  1.5302e-03,  2.1065e-03, -4.1065e-03,  9.1776e-03,\n",
       "                       5.0702e-03, -2.5795e-04, -2.3721e-03, -4.7142e-03,  8.0128e-04,\n",
       "                       1.1872e-02,  1.0499e-02,  6.7437e-03,  1.1501e-02,  8.3773e-03,\n",
       "                       1.0160e-02, -3.1037e-03,  4.0496e-03, -1.1967e-02, -8.8712e-03,\n",
       "                      -9.0065e-03,  1.4386e-02])),\n",
       "             ('bert.encoder.layer.3.attention.output.dense.weight',\n",
       "              tensor([[-0.0083,  0.0020,  0.0134,  ...,  0.0245,  0.0061, -0.0498],\n",
       "                      [ 0.0334,  0.0214, -0.0330,  ..., -0.0192, -0.0377,  0.0388],\n",
       "                      [-0.0091, -0.0209, -0.0170,  ...,  0.0423, -0.0406,  0.0350],\n",
       "                      ...,\n",
       "                      [-0.0174, -0.0275, -0.0237,  ..., -0.0279,  0.0112,  0.0071],\n",
       "                      [ 0.0164, -0.0749, -0.0021,  ...,  0.0030,  0.0269, -0.0278],\n",
       "                      [ 0.0084, -0.0249, -0.0358,  ...,  0.0180, -0.0032,  0.0223]])),\n",
       "             ('bert.encoder.layer.3.attention.output.dense.bias',\n",
       "              tensor([ 2.3225e-04,  1.6640e-03,  3.1334e-03,  2.4139e-03, -7.3706e-03,\n",
       "                      -5.9715e-03,  2.4601e-03, -7.9567e-03,  1.1938e-03, -3.5097e-03,\n",
       "                      -1.3968e-03,  1.0586e-02, -5.5155e-03, -2.8794e-03,  1.3879e-03,\n",
       "                      -1.7956e-03,  3.9840e-03,  1.0319e-03,  3.3521e-04, -2.5925e-03,\n",
       "                       7.9735e-03, -2.8958e-03, -5.5829e-03,  5.4149e-03, -3.2634e-03,\n",
       "                      -4.4541e-03,  1.2114e-04,  1.6795e-02, -8.7339e-03,  3.1209e-03,\n",
       "                       4.1399e-03,  5.0874e-04, -1.0354e-03, -6.9124e-03, -2.4031e-03,\n",
       "                       1.3773e-03, -1.2388e-03, -7.2909e-03,  9.5041e-04,  1.0271e-03,\n",
       "                      -7.7741e-03, -5.7827e-04, -3.8917e-03,  2.5098e-03, -3.8213e-03,\n",
       "                      -2.4459e-03,  6.2959e-03,  1.6324e-03,  5.7860e-05, -7.3182e-04,\n",
       "                       1.9719e-03, -9.3367e-03, -1.1124e-02,  3.6541e-03,  2.6230e-03,\n",
       "                       1.4525e-03, -8.3776e-03, -5.0954e-03, -1.6043e-03, -3.9970e-03,\n",
       "                      -5.5812e-04, -2.4202e-03,  7.3062e-03, -7.2186e-03, -8.4411e-03,\n",
       "                      -1.0715e-03, -8.1668e-03,  1.2317e-02,  4.1704e-04,  1.2013e-03,\n",
       "                      -1.0869e-04, -3.0201e-04, -2.6445e-03, -1.2294e-03, -6.7031e-03,\n",
       "                       3.8226e-03, -1.6065e-03,  3.2350e-03, -3.2463e-03, -5.9908e-03,\n",
       "                      -3.5177e-03,  1.6705e-03, -2.3968e-03,  2.8569e-03, -4.6850e-03,\n",
       "                      -5.8688e-03,  1.3387e-03, -4.9099e-03, -1.1991e-03, -2.5619e-03,\n",
       "                       9.1621e-04,  8.4014e-03,  3.5667e-03,  2.6699e-03, -1.2043e-02,\n",
       "                       2.2087e-02,  1.9846e-03, -1.6198e-03,  3.3048e-03, -1.7843e-03,\n",
       "                       3.7579e-04,  3.1591e-03, -1.2129e-02,  2.3334e-03,  2.9726e-04,\n",
       "                       6.0750e-03, -4.2576e-03, -8.5552e-05,  2.2039e-03, -4.8627e-04,\n",
       "                       3.2417e-04, -1.4839e-03,  2.1051e-03,  1.5584e-03, -1.5658e-02,\n",
       "                       3.8740e-03,  5.6011e-03, -1.7737e-03,  9.0968e-03, -6.9550e-03,\n",
       "                      -7.6870e-03,  2.8955e-03,  5.8981e-03, -9.3170e-04,  3.6248e-04,\n",
       "                       1.8341e-04,  1.3129e-05,  1.5959e-03,  6.6662e-03, -9.0194e-04,\n",
       "                       9.0224e-03,  5.0490e-03,  2.8115e-03, -1.2074e-03, -3.3564e-03,\n",
       "                      -2.9144e-05, -8.2692e-03,  1.4143e-03,  1.7701e-03,  3.3878e-03,\n",
       "                       5.2963e-04,  4.4364e-03,  5.4755e-03, -1.2158e-03, -2.7154e-03,\n",
       "                      -2.9468e-03,  2.0425e-05, -3.2144e-03, -2.2862e-03, -8.0371e-03,\n",
       "                      -4.1524e-03,  8.6665e-03, -4.8287e-03, -1.2343e-03,  1.0928e-04,\n",
       "                      -2.3734e-03, -1.1883e-02,  3.7286e-03, -1.4754e-03, -5.1617e-03,\n",
       "                      -3.8247e-03,  1.7207e-03, -6.6069e-03, -6.9270e-03,  1.2132e-03,\n",
       "                       3.2418e-03,  5.6375e-03,  1.9679e-03,  1.0039e-03, -2.0273e-03,\n",
       "                      -1.3317e-03,  1.5760e-03,  4.1896e-03, -1.5743e-03,  2.4038e-03,\n",
       "                       2.3954e-03, -5.4689e-03, -7.3114e-03, -5.7708e-04, -1.0641e-03,\n",
       "                       1.4148e-02, -4.9430e-03, -4.4194e-03,  7.1228e-04,  3.9515e-03,\n",
       "                      -2.3017e-03, -4.9791e-04, -5.2738e-03,  5.6459e-03,  8.1240e-03,\n",
       "                      -7.9021e-03,  1.0018e-03])),\n",
       "             ('bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0158, 1.0182, 0.9961, 0.9993, 1.0103, 0.9861, 1.0075, 1.0049, 0.9894,\n",
       "                      1.0201, 0.9799, 1.0037, 1.0091, 1.0092, 0.9926, 1.0046, 1.0153, 0.9784,\n",
       "                      0.9902, 1.0211, 1.0070, 1.0187, 0.9960, 1.0121, 1.0016, 1.0139, 1.0086,\n",
       "                      0.9815, 0.9857, 1.0212, 0.9996, 1.0150, 0.9971, 1.0100, 1.0143, 0.9949,\n",
       "                      1.0029, 0.9667, 1.0123, 1.0146, 1.0095, 1.0097, 1.0076, 1.0000, 1.0124,\n",
       "                      1.0232, 1.0080, 1.0209, 1.0020, 1.0133, 1.0117, 0.9632, 0.9634, 1.0135,\n",
       "                      1.0180, 1.0082, 1.0072, 1.0147, 1.0165, 1.0062, 1.0030, 0.9871, 1.0247,\n",
       "                      1.0060, 1.0149, 1.0058, 1.0086, 0.9472, 1.0026, 1.0043, 1.0039, 0.9797,\n",
       "                      1.0040, 1.0122, 1.0172, 0.9942, 1.0152, 1.0118, 1.0217, 1.0199, 1.0073,\n",
       "                      1.0058, 1.0184, 1.0180, 1.0343, 0.9999, 1.0229, 1.0069, 1.0117, 1.0017,\n",
       "                      0.9962, 0.9944, 1.0187, 1.0073, 0.9937, 0.9667, 1.0054, 0.9729, 1.0077,\n",
       "                      1.0131, 0.9973, 1.0199, 1.0151, 1.0070, 1.0171, 1.0025, 1.0119, 1.0004,\n",
       "                      1.0231, 1.0262, 0.9895, 1.0150, 1.0175, 1.0032, 0.9444, 1.0080, 1.0064,\n",
       "                      1.0110, 0.9997, 1.0153, 0.9967, 0.9724, 1.0037, 1.0059, 1.0209, 1.0216,\n",
       "                      1.0107, 1.0082, 1.0133, 1.0064, 0.9789, 0.9920, 1.0141, 1.0006, 1.0187,\n",
       "                      1.0134, 0.9984, 1.0145, 0.9802, 1.0123, 1.0143, 1.0114, 1.0005, 1.0236,\n",
       "                      0.9886, 1.0082, 0.9903, 1.0118, 1.0070, 1.0200, 0.9980, 0.9635, 1.0129,\n",
       "                      1.0248, 0.9969, 1.0309, 1.0081, 0.9972, 1.0090, 0.9664, 1.0006, 0.9902,\n",
       "                      1.0050, 1.0121, 1.0060, 1.0165, 1.0045, 1.0152, 1.0136, 0.9887, 0.9961,\n",
       "                      0.9990, 1.0058, 0.9987, 1.0199, 1.0015, 1.0097, 0.9614, 1.0132, 1.0017,\n",
       "                      0.9544, 1.0132, 1.0020, 1.0189, 1.0142, 1.0018, 1.0158, 1.0129, 0.9971,\n",
       "                      0.9902, 1.0078, 1.0131])),\n",
       "             ('bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       "              tensor([ 3.3470e-03,  8.7432e-03,  1.0357e-02, -1.1259e-03, -6.3320e-03,\n",
       "                      -8.8924e-03,  1.7827e-04, -8.0102e-03,  3.5679e-03, -6.9274e-03,\n",
       "                       5.7017e-03,  1.4418e-02, -3.1200e-03,  2.8536e-03,  5.2725e-03,\n",
       "                      -9.4625e-03,  4.2837e-03,  7.7597e-03, -3.1208e-03, -2.0194e-03,\n",
       "                       1.2450e-02, -7.5057e-06, -9.1730e-03,  6.1141e-03, -6.9424e-03,\n",
       "                      -2.1460e-03,  5.4096e-03,  2.2806e-02, -8.4453e-03,  9.3364e-03,\n",
       "                       6.5319e-03, -1.4394e-03, -4.2554e-04, -1.0034e-02, -6.4582e-03,\n",
       "                       2.4161e-03, -5.7385e-03, -1.2069e-02,  1.7761e-03, -8.2557e-03,\n",
       "                      -8.3389e-04, -4.4606e-03, -2.6024e-03,  8.7844e-03,  2.1962e-05,\n",
       "                      -9.1607e-03,  9.9010e-03, -2.9364e-03,  5.6465e-04, -1.7857e-04,\n",
       "                      -2.2713e-04, -1.1149e-02, -9.2818e-03,  5.8225e-03,  1.5590e-03,\n",
       "                      -1.9325e-03, -8.0934e-03, -3.6543e-04, -5.6856e-04,  3.5816e-03,\n",
       "                      -3.8600e-03,  1.5376e-04,  7.4528e-03, -1.4034e-02,  1.1445e-03,\n",
       "                      -2.6345e-03, -8.5873e-03,  1.6929e-02,  2.2118e-03,  7.6269e-03,\n",
       "                       7.4620e-03,  2.6694e-03,  3.2102e-03,  5.1122e-03, -5.7030e-04,\n",
       "                       9.4499e-03, -1.4957e-03,  5.0707e-03, -1.7439e-03, -2.8967e-03,\n",
       "                       1.7594e-03,  6.2551e-03, -6.1123e-03, -4.3928e-03, -1.9774e-03,\n",
       "                      -1.0520e-02, -2.1833e-03, -6.7301e-03,  1.5922e-03, -1.3986e-03,\n",
       "                      -4.0368e-03,  1.2982e-02,  2.5296e-03,  3.2481e-03, -1.5120e-02,\n",
       "                       3.2547e-02,  1.2172e-03,  3.5483e-03,  3.7016e-04, -7.1136e-04,\n",
       "                      -5.2757e-03,  1.0513e-02, -7.0928e-03,  1.1555e-03,  1.9801e-03,\n",
       "                       8.1164e-03, -2.7707e-03,  6.7227e-03, -3.2816e-03,  4.0597e-03,\n",
       "                      -5.6144e-03,  8.6728e-04,  2.3403e-03, -7.7858e-03, -2.6710e-02,\n",
       "                       1.2797e-02,  8.8954e-03, -6.8426e-03,  1.1115e-02, -1.0575e-02,\n",
       "                      -8.5705e-03, -1.9130e-03,  5.4601e-03,  1.2669e-05,  4.1821e-04,\n",
       "                       2.6337e-04,  9.9521e-03,  3.4343e-03,  1.3418e-02, -4.4120e-03,\n",
       "                       1.4033e-02,  5.1448e-03,  2.8545e-03, -2.8404e-03, -3.0944e-03,\n",
       "                       2.3252e-03, -1.2270e-02,  9.3694e-03,  2.4051e-03,  1.0339e-02,\n",
       "                       1.8828e-04,  4.1811e-04,  2.8392e-03, -4.8649e-04, -5.0101e-03,\n",
       "                      -7.0026e-03,  5.7968e-03, -3.4951e-04,  2.9604e-03, -8.8762e-03,\n",
       "                      -4.4873e-03,  1.1481e-02, -1.0237e-02, -8.3430e-03,  6.6666e-03,\n",
       "                      -2.8753e-03, -1.2181e-02,  1.0412e-02,  2.1484e-03, -1.1858e-02,\n",
       "                      -7.5833e-03,  9.0343e-04, -5.8357e-03, -8.1748e-03,  9.7656e-04,\n",
       "                       5.7373e-03,  3.8330e-03,  2.8913e-03,  5.5731e-03, -5.7278e-03,\n",
       "                      -7.6718e-03, -2.7873e-03,  2.1449e-03, -2.2408e-03,  6.1903e-03,\n",
       "                       8.3520e-03, -7.9247e-03, -8.5721e-03, -2.1895e-04, -3.1138e-04,\n",
       "                       2.9314e-02, -2.2271e-03, -1.0059e-02, -6.1015e-03,  1.2101e-03,\n",
       "                       2.2516e-03, -1.1667e-02, -5.3458e-03,  1.2787e-02,  1.5576e-02,\n",
       "                      -1.4153e-02, -4.5147e-04])),\n",
       "             ('bert.encoder.layer.3.intermediate.dense.weight',\n",
       "              tensor([[ 0.0307, -0.0154,  0.0187,  ..., -0.0167, -0.0135,  0.0392],\n",
       "                      [-0.0056, -0.0058, -0.0319,  ..., -0.0011,  0.0382, -0.0031],\n",
       "                      [ 0.0306, -0.0464, -0.0179,  ..., -0.0292, -0.0416,  0.0323],\n",
       "                      ...,\n",
       "                      [-0.0376, -0.0518,  0.0187,  ..., -0.0340, -0.0084, -0.0187],\n",
       "                      [-0.0232,  0.0157, -0.0162,  ...,  0.0198,  0.0201,  0.0093],\n",
       "                      [-0.0042, -0.0324, -0.0416,  ..., -0.0816,  0.0568,  0.0263]])),\n",
       "             ('bert.encoder.layer.3.intermediate.dense.bias',\n",
       "              tensor([-0.0153,  0.0074, -0.0078,  0.0043,  0.0068,  0.0045, -0.0173,  0.0339,\n",
       "                       0.0094, -0.0191,  0.0196, -0.0146, -0.0177,  0.0306, -0.0045,  0.0177,\n",
       "                       0.0148, -0.0063, -0.0018, -0.0027, -0.0118, -0.0056,  0.0010, -0.0123,\n",
       "                       0.0052, -0.0107,  0.0017,  0.0127, -0.0057, -0.0012,  0.0039, -0.0019,\n",
       "                      -0.0110,  0.0016, -0.0003,  0.0058, -0.0093, -0.0004, -0.0074,  0.0063,\n",
       "                      -0.0108,  0.0130, -0.0175,  0.0127,  0.0051, -0.0062, -0.0062,  0.0014,\n",
       "                       0.0158, -0.0100,  0.0297, -0.0086, -0.0109,  0.0149,  0.0068, -0.0143,\n",
       "                      -0.0119,  0.0039, -0.0070, -0.0041, -0.0030,  0.0108, -0.0072, -0.0154])),\n",
       "             ('bert.encoder.layer.3.output.dense.weight',\n",
       "              tensor([[ 0.0149,  0.0038, -0.0074,  ..., -0.0129,  0.0258,  0.0067],\n",
       "                      [ 0.0196, -0.0107, -0.0252,  ...,  0.0118, -0.0043, -0.0043],\n",
       "                      [ 0.0091,  0.0527,  0.0208,  ...,  0.0079,  0.0010,  0.0418],\n",
       "                      ...,\n",
       "                      [-0.0028,  0.0138,  0.0173,  ..., -0.0079,  0.0088, -0.0465],\n",
       "                      [-0.0199, -0.0304, -0.0198,  ...,  0.0002,  0.0116, -0.0176],\n",
       "                      [ 0.0292, -0.0139,  0.0401,  ..., -0.0213,  0.0081, -0.0018]])),\n",
       "             ('bert.encoder.layer.3.output.dense.bias',\n",
       "              tensor([ 4.9835e-03,  7.2881e-03,  1.1506e-02, -8.1892e-04, -3.0007e-03,\n",
       "                      -1.4193e-03, -3.0464e-03, -1.0402e-02,  1.8708e-03, -1.0652e-03,\n",
       "                       4.3170e-03,  1.1089e-02, -2.1248e-03,  1.5925e-03,  1.6889e-03,\n",
       "                      -8.3542e-03, -2.6220e-03,  7.4979e-03, -6.1322e-03, -6.5390e-04,\n",
       "                       8.7114e-03, -5.6345e-04, -8.2934e-03,  9.2931e-03, -7.8997e-03,\n",
       "                       2.7221e-03,  7.2520e-03,  1.9290e-02, -1.0111e-02,  9.1625e-03,\n",
       "                       2.7073e-03, -3.0945e-03, -1.5786e-03, -7.8880e-03, -4.4350e-03,\n",
       "                       2.6215e-03, -7.4414e-03, -1.0425e-02,  3.5801e-03, -1.3551e-03,\n",
       "                       9.3366e-04,  5.2878e-03, -6.4709e-04,  3.8809e-03, -1.1058e-03,\n",
       "                      -7.4556e-03,  6.0609e-03, -3.7188e-03,  3.5028e-03, -4.7727e-03,\n",
       "                       1.2835e-03, -6.6392e-03, -1.2775e-02,  5.0225e-03,  2.9744e-04,\n",
       "                       2.6966e-03, -8.8764e-03,  2.7157e-04,  2.3311e-03, -1.6778e-03,\n",
       "                      -5.3501e-03,  5.2682e-03,  4.7091e-03, -9.8623e-03, -2.8114e-03,\n",
       "                      -6.6773e-04, -7.3199e-03,  1.3216e-02,  2.9352e-03,  3.2262e-03,\n",
       "                       9.2002e-03,  1.9065e-03,  7.0112e-03,  5.2963e-03, -1.1336e-03,\n",
       "                       7.9584e-03,  4.1874e-03,  3.9376e-03, -4.1786e-04, -1.5705e-03,\n",
       "                       9.3057e-05,  5.8227e-03, -3.2152e-03, -1.8044e-03, -2.5202e-04,\n",
       "                      -7.4499e-03, -5.7266e-03, -6.9505e-03,  3.2510e-03, -1.2838e-03,\n",
       "                      -5.4810e-03,  1.0359e-02, -3.1674e-04,  4.4522e-03, -1.4520e-02,\n",
       "                       2.6164e-02, -1.1249e-03,  1.2193e-03,  2.7375e-04, -3.2129e-03,\n",
       "                      -4.5131e-03,  7.2007e-03, -1.1055e-02,  7.8032e-04,  4.8009e-03,\n",
       "                       7.7729e-03, -1.9186e-03,  5.9902e-03, -2.6298e-03, -4.2032e-04,\n",
       "                      -2.5156e-03,  3.2587e-03, -1.8702e-03, -6.1471e-03, -2.5121e-02,\n",
       "                       1.6023e-02,  7.1456e-03, -7.3909e-03,  9.8957e-03, -9.8778e-03,\n",
       "                      -9.3524e-03, -6.9536e-03,  2.5708e-03, -2.6505e-03,  1.9190e-04,\n",
       "                      -1.6159e-03,  7.3656e-03,  5.0923e-03,  1.0192e-02, -4.7743e-03,\n",
       "                       1.3128e-02,  8.1471e-03,  3.6782e-03, -5.8136e-03, -3.4954e-03,\n",
       "                       7.2451e-04, -8.5994e-03,  5.3072e-03, -2.4021e-03,  1.1104e-02,\n",
       "                      -3.1818e-03,  9.6279e-04,  4.4819e-03,  2.3189e-06, -5.7211e-03,\n",
       "                      -7.2772e-03,  3.3790e-03,  1.9424e-04,  1.4693e-03, -7.0149e-03,\n",
       "                      -5.8346e-03,  9.8964e-03, -8.8527e-03, -5.3234e-03,  3.3078e-03,\n",
       "                      -7.3518e-03, -1.5273e-02,  8.5192e-03,  5.6870e-03, -1.3904e-02,\n",
       "                      -6.1062e-03,  2.1017e-03, -2.7879e-03, -3.5482e-03,  3.3985e-04,\n",
       "                       7.0020e-03,  2.9059e-03,  4.1859e-03,  4.5113e-03, -6.0049e-03,\n",
       "                      -6.4948e-03, -3.8442e-03,  9.0363e-04, -1.1216e-03,  6.8734e-03,\n",
       "                       6.6488e-03, -1.0003e-02, -9.0455e-03,  2.5895e-04, -3.3234e-03,\n",
       "                       2.7787e-02, -6.1749e-03, -6.8369e-03, -5.4653e-03,  6.3525e-04,\n",
       "                       3.0142e-03, -6.9842e-03, -3.0236e-03,  1.0633e-02,  1.0034e-02,\n",
       "                      -1.4650e-02,  1.5861e-03])),\n",
       "             ('bert.encoder.layer.3.output.LayerNorm.weight',\n",
       "              tensor([1.0300, 1.0295, 1.0064, 1.0086, 1.0195, 0.9945, 1.0195, 1.0221, 1.0008,\n",
       "                      1.0257, 0.9865, 1.0181, 1.0222, 1.0210, 1.0103, 1.0112, 1.0233, 0.9936,\n",
       "                      0.9960, 1.0339, 1.0196, 1.0310, 1.0062, 1.0249, 1.0096, 1.0253, 1.0273,\n",
       "                      0.9883, 0.9858, 1.0415, 1.0171, 1.0284, 1.0097, 1.0261, 1.0273, 1.0001,\n",
       "                      1.0165, 0.9664, 1.0263, 1.0251, 1.0231, 1.0254, 1.0245, 1.0154, 1.0286,\n",
       "                      1.0306, 1.0161, 1.0369, 1.0137, 1.0279, 1.0279, 0.9621, 0.9562, 1.0276,\n",
       "                      1.0304, 1.0196, 1.0245, 1.0192, 1.0300, 1.0188, 1.0172, 0.9984, 1.0367,\n",
       "                      1.0112, 1.0266, 1.0213, 1.0204, 0.9371, 1.0168, 1.0105, 1.0156, 0.9935,\n",
       "                      1.0156, 1.0329, 1.0290, 1.0026, 1.0316, 1.0224, 1.0416, 1.0341, 1.0149,\n",
       "                      1.0201, 1.0309, 1.0332, 1.0490, 1.0129, 1.0420, 1.0184, 1.0260, 1.0193,\n",
       "                      1.0120, 0.9996, 1.0338, 1.0190, 1.0041, 0.9619, 1.0160, 0.9744, 1.0227,\n",
       "                      1.0209, 1.0120, 1.0274, 1.0286, 1.0203, 1.0305, 1.0113, 1.0301, 1.0124,\n",
       "                      1.0391, 1.0423, 0.9964, 1.0271, 1.0328, 1.0119, 0.9215, 1.0182, 1.0140,\n",
       "                      1.0181, 1.0120, 1.0349, 1.0013, 0.9778, 1.0138, 1.0166, 1.0337, 1.0348,\n",
       "                      1.0269, 1.0210, 1.0309, 1.0205, 0.9762, 1.0012, 1.0286, 1.0137, 1.0346,\n",
       "                      1.0285, 1.0003, 1.0273, 0.9904, 1.0264, 1.0274, 1.0251, 1.0115, 1.0352,\n",
       "                      0.9951, 1.0155, 0.9978, 1.0242, 1.0198, 1.0365, 1.0095, 0.9599, 1.0269,\n",
       "                      1.0370, 1.0090, 1.0423, 1.0228, 1.0025, 1.0193, 0.9671, 1.0153, 0.9982,\n",
       "                      1.0148, 1.0304, 1.0136, 1.0317, 1.0182, 1.0291, 1.0276, 1.0060, 1.0032,\n",
       "                      1.0184, 1.0229, 1.0171, 1.0316, 1.0139, 1.0253, 0.9647, 1.0237, 1.0160,\n",
       "                      0.9390, 1.0265, 1.0131, 1.0307, 1.0259, 1.0206, 1.0205, 1.0248, 1.0053,\n",
       "                      0.9941, 1.0107, 1.0288])),\n",
       "             ('bert.encoder.layer.3.output.LayerNorm.bias',\n",
       "              tensor([ 2.9746e-03,  7.3791e-03,  1.1108e-02,  1.4529e-03, -3.2950e-03,\n",
       "                      -1.0496e-02, -3.3863e-03, -1.1029e-02,  4.6644e-03, -6.0331e-03,\n",
       "                       1.5177e-02,  1.5403e-02, -8.0486e-03,  7.4304e-03,  9.8133e-03,\n",
       "                      -7.7987e-03,  2.1068e-03,  1.2265e-02, -2.3479e-03, -1.7198e-03,\n",
       "                       9.7381e-03, -1.6018e-03, -9.5928e-03,  4.2882e-03, -4.8762e-03,\n",
       "                      -2.9384e-03,  6.6671e-03,  2.1654e-02, -3.0699e-03,  1.1203e-02,\n",
       "                       1.2119e-02, -1.3057e-03, -4.7858e-03, -1.2220e-02, -7.2741e-03,\n",
       "                       2.2700e-04, -8.1267e-03, -8.2948e-03,  3.7343e-03, -8.4111e-03,\n",
       "                       4.0517e-05, -4.2076e-03, -3.7447e-03,  1.2042e-02,  1.0671e-04,\n",
       "                      -8.8603e-03,  5.6028e-03,  1.1451e-04, -1.1524e-03, -5.9040e-04,\n",
       "                      -3.9606e-03, -7.5602e-03, -6.5143e-03,  2.1868e-03,  2.5230e-03,\n",
       "                      -6.2274e-03, -6.9694e-03, -1.9519e-03, -4.0725e-04,  1.4635e-03,\n",
       "                      -4.9007e-03, -1.2469e-03,  1.1382e-02, -1.0825e-02,  2.9651e-04,\n",
       "                      -2.3631e-03, -8.2809e-03,  1.0664e-02,  1.1434e-03,  3.4934e-03,\n",
       "                       6.6678e-03, -2.3253e-04, -4.8640e-04,  7.6117e-03, -9.3183e-04,\n",
       "                       1.1206e-02, -3.8877e-05,  5.9638e-03, -1.3190e-03, -2.4464e-03,\n",
       "                      -1.0469e-03,  1.0951e-02, -8.1938e-03, -6.4173e-03,  6.4881e-04,\n",
       "                      -1.1564e-02, -6.0510e-03, -6.6571e-03,  7.9177e-03, -2.1038e-03,\n",
       "                      -1.2020e-02,  1.0135e-02,  4.0670e-03,  6.2569e-03, -1.5705e-02,\n",
       "                       2.6876e-02,  9.4951e-04, -3.0818e-03, -7.8688e-05, -5.1751e-04,\n",
       "                      -6.9991e-03,  1.2987e-02, -7.1814e-03,  6.3010e-04,  4.1110e-03,\n",
       "                       7.9189e-03, -4.9900e-03,  9.0434e-03, -6.1101e-03,  5.4959e-03,\n",
       "                      -5.0854e-03,  1.0893e-03,  3.7802e-03, -1.1434e-02, -1.5400e-02,\n",
       "                       1.3203e-02,  6.5431e-03, -6.1083e-03,  1.1322e-02, -1.4410e-02,\n",
       "                      -5.0945e-03, -1.0279e-02,  5.3799e-03, -4.5826e-04, -1.0223e-03,\n",
       "                       8.7181e-05,  1.2091e-02,  2.2675e-03,  1.6108e-02, -6.0815e-03,\n",
       "                       7.6429e-03,  2.9216e-03,  7.3896e-03, -6.1252e-03, -2.8453e-03,\n",
       "                       5.0567e-03, -5.8366e-03,  7.2488e-03,  5.1731e-03,  1.2387e-02,\n",
       "                      -2.0719e-03,  2.7756e-03,  3.9320e-03,  5.0703e-04, -5.1164e-03,\n",
       "                      -7.1026e-03,  6.4285e-03,  1.8047e-03,  3.3027e-03, -8.2864e-03,\n",
       "                      -2.5218e-03,  7.7459e-03, -7.3802e-03, -7.2467e-03,  2.3301e-04,\n",
       "                      -3.1382e-03, -1.4547e-02,  3.7487e-03,  4.9030e-03, -7.3913e-03,\n",
       "                      -7.3692e-03,  2.5840e-03, -3.2443e-03, -5.9890e-03, -1.2175e-03,\n",
       "                       8.9703e-03, -5.0600e-04,  6.1399e-03,  8.6350e-03, -6.8268e-03,\n",
       "                      -6.3163e-03, -1.0590e-02,  3.3124e-03, -5.2123e-03,  1.0556e-02,\n",
       "                       5.2167e-03, -9.1044e-03, -9.9181e-03,  5.3663e-04, -2.8727e-04,\n",
       "                       2.2947e-02, -2.3485e-03, -1.0765e-02, -7.4963e-03,  3.2601e-03,\n",
       "                       5.3947e-03, -1.2269e-02, -2.2665e-03,  1.0524e-02,  1.1736e-02,\n",
       "                      -1.1269e-02,  2.6143e-03])),\n",
       "             ('bert.encoder.layer.4.attention.self.query.weight',\n",
       "              tensor([[ 0.0516, -0.0856, -0.0353,  ..., -0.0191,  0.0354,  0.0214],\n",
       "                      [ 0.0513, -0.0100,  0.0182,  ..., -0.0321,  0.0151,  0.0232],\n",
       "                      [-0.0164,  0.0419,  0.0033,  ...,  0.0242, -0.0467, -0.0713],\n",
       "                      ...,\n",
       "                      [-0.0211,  0.0211, -0.0091,  ...,  0.0236, -0.0404,  0.0056],\n",
       "                      [-0.0100, -0.0056,  0.0275,  ...,  0.0509, -0.0209,  0.0017],\n",
       "                      [-0.0379, -0.0162,  0.0426,  ..., -0.0419,  0.0251, -0.0292]])),\n",
       "             ('bert.encoder.layer.4.attention.self.query.bias',\n",
       "              tensor([-0.0482, -0.0209,  0.0457,  0.0080,  0.0430, -0.0260,  0.0300, -0.0007,\n",
       "                       0.0275, -0.0486,  0.0308, -0.0462,  0.0426,  0.0168,  0.0081, -0.0252,\n",
       "                      -0.0222, -0.0313, -0.0410,  0.0173,  0.0289, -0.0508,  0.0359,  0.0352,\n",
       "                       0.0367,  0.0259, -0.0547,  0.0078,  0.0431,  0.0304, -0.0004,  0.0125,\n",
       "                       0.0028, -0.0031,  0.0142,  0.0090, -0.0327, -0.0292,  0.0057, -0.0078,\n",
       "                      -0.0021,  0.0215,  0.0311,  0.0147,  0.0231,  0.0027, -0.0225, -0.0183,\n",
       "                      -0.0204,  0.0026, -0.0010,  0.0178, -0.0113, -0.0141, -0.0228, -0.0145,\n",
       "                       0.0288, -0.0035, -0.0423, -0.0236,  0.0077, -0.0389,  0.0220,  0.0153,\n",
       "                       0.0459, -0.0324,  0.0230, -0.0338, -0.0479, -0.0203, -0.0211, -0.0355,\n",
       "                      -0.0084,  0.0275,  0.0022,  0.0098,  0.0290, -0.0247, -0.0162, -0.0124,\n",
       "                       0.0228, -0.0420, -0.0086, -0.0296, -0.0209,  0.0231, -0.0309, -0.0354,\n",
       "                      -0.0299,  0.0151,  0.0198, -0.0549,  0.0430,  0.0321, -0.0067,  0.0265,\n",
       "                      -0.0010,  0.0207, -0.0015, -0.0008,  0.0222, -0.0091,  0.0339,  0.0435,\n",
       "                       0.0144,  0.0056, -0.0347, -0.0137, -0.0274,  0.0065,  0.0039,  0.0176,\n",
       "                      -0.0009,  0.0052, -0.0248,  0.0272,  0.0089, -0.0354,  0.0157, -0.0304,\n",
       "                      -0.0152,  0.0057,  0.0179, -0.0035,  0.0117, -0.0463, -0.0365,  0.0239,\n",
       "                      -0.0192, -0.0055,  0.0174,  0.0126,  0.0192, -0.0213,  0.0421, -0.0325,\n",
       "                       0.0078, -0.0012, -0.0300,  0.0089, -0.0365,  0.0318, -0.0111, -0.0306,\n",
       "                      -0.0155,  0.0221, -0.0007, -0.0186,  0.0391,  0.0121,  0.0026, -0.0137,\n",
       "                       0.0043,  0.0171, -0.0110, -0.0038,  0.0145, -0.0087, -0.0293, -0.0088,\n",
       "                       0.0170,  0.0218,  0.0065, -0.0053,  0.0116, -0.0282,  0.0070,  0.0144,\n",
       "                      -0.0154,  0.0042,  0.0178, -0.0092, -0.0156,  0.0145, -0.0023, -0.0163,\n",
       "                       0.0126, -0.0192,  0.0095, -0.0064,  0.0142, -0.0213,  0.0144, -0.0058,\n",
       "                      -0.0004, -0.0354, -0.0055, -0.0110,  0.0038,  0.0188,  0.0127, -0.0102])),\n",
       "             ('bert.encoder.layer.4.attention.self.key.weight',\n",
       "              tensor([[ 0.0258,  0.0249,  0.0010,  ...,  0.0547, -0.0225, -0.0258],\n",
       "                      [ 0.0104,  0.0188, -0.0153,  ..., -0.0122, -0.0673, -0.0033],\n",
       "                      [-0.0275, -0.0438, -0.0383,  ..., -0.0277,  0.0060,  0.0168],\n",
       "                      ...,\n",
       "                      [-0.0208, -0.0373,  0.0068,  ...,  0.0032, -0.0013,  0.0151],\n",
       "                      [-0.0156,  0.0240,  0.0118,  ...,  0.0132,  0.0207,  0.0041],\n",
       "                      [-0.0367,  0.0259,  0.0215,  ..., -0.0246,  0.0201, -0.0305]])),\n",
       "             ('bert.encoder.layer.4.attention.self.key.bias',\n",
       "              tensor([-3.8319e-09,  2.3135e-08,  6.8063e-08,  4.1863e-07,  2.0705e-07,\n",
       "                      -1.2710e-07,  8.1770e-08, -2.4784e-07, -9.6610e-08,  4.8646e-08,\n",
       "                      -1.4163e-07,  3.4240e-07,  2.6940e-08,  1.9371e-07,  2.1843e-07,\n",
       "                      -2.4006e-07,  1.9845e-07,  2.4859e-07, -3.1104e-07,  1.9654e-07,\n",
       "                       3.1564e-07, -1.3672e-07,  2.6908e-07, -2.1939e-07, -1.3327e-07,\n",
       "                       3.0932e-07,  2.2541e-07, -1.3664e-07,  9.2220e-08, -2.8351e-08,\n",
       "                      -3.4307e-08,  2.9468e-07, -5.6605e-08,  2.0288e-07,  2.2589e-07,\n",
       "                       5.9769e-08,  2.5513e-07, -3.6532e-08,  5.2071e-08,  6.1330e-08,\n",
       "                      -4.5426e-08, -3.4027e-08,  1.9398e-07, -6.0256e-08,  4.6264e-08,\n",
       "                       6.2791e-08,  1.5190e-07,  2.6172e-07, -1.8466e-08,  1.2106e-08,\n",
       "                      -1.1329e-07,  7.6225e-09,  2.1932e-07, -4.2773e-08,  4.0610e-08,\n",
       "                       6.5120e-09,  1.1902e-07,  6.0222e-08,  7.3412e-08, -1.2319e-07,\n",
       "                      -1.0137e-07,  4.0407e-08, -1.1833e-07,  5.5755e-08,  1.7864e-07,\n",
       "                      -1.7276e-07,  8.6450e-08, -9.1138e-08, -1.6973e-07, -5.8491e-08,\n",
       "                      -2.2097e-07,  8.1304e-08, -6.3374e-08, -8.9361e-08, -1.7527e-08,\n",
       "                       1.1140e-07, -3.3453e-08, -1.0552e-07, -1.0078e-07, -1.2002e-07,\n",
       "                       2.4879e-07, -2.0988e-07, -4.2151e-08, -2.0493e-07, -1.4348e-07,\n",
       "                       1.0224e-07, -1.3951e-07, -2.2104e-07, -2.2035e-07,  4.6540e-08,\n",
       "                       7.7594e-08, -1.3686e-07,  4.6610e-08,  2.0057e-07, -6.2407e-08,\n",
       "                       1.6584e-07, -1.0166e-07, -1.5833e-07, -1.0582e-08,  9.3531e-08,\n",
       "                      -2.5384e-09,  8.8804e-08, -5.6639e-08,  9.8936e-08, -3.4796e-09,\n",
       "                       1.6851e-07,  1.5252e-07,  1.9696e-07,  1.5406e-07,  4.7870e-08,\n",
       "                      -2.3125e-07,  7.7551e-08,  3.2523e-08,  9.1206e-08,  4.1936e-08,\n",
       "                      -1.1825e-07,  8.2102e-08,  2.4883e-07, -3.7720e-08,  1.3475e-07,\n",
       "                       3.8382e-08,  1.9510e-07, -1.9857e-07, -5.3130e-08,  1.6434e-07,\n",
       "                       2.3550e-07,  1.8073e-07,  4.6566e-08, -2.9683e-07, -1.4862e-07,\n",
       "                       2.5310e-08,  2.5025e-07,  1.2317e-07,  9.2718e-08,  3.3274e-07,\n",
       "                      -3.0929e-07,  7.9568e-08, -1.2182e-07, -1.8828e-07,  9.5735e-08,\n",
       "                      -1.8610e-07,  1.2931e-07,  1.9491e-08, -5.5048e-08, -5.7522e-09,\n",
       "                       1.2242e-07, -5.6975e-08, -3.1341e-08, -9.4964e-08,  1.4936e-07,\n",
       "                       2.6126e-07, -1.2372e-07, -2.0860e-07,  3.6169e-07,  1.2048e-07,\n",
       "                       7.4160e-08,  3.3141e-08,  7.2013e-08, -7.6479e-08, -1.4402e-07,\n",
       "                       9.7715e-08, -3.2770e-08, -4.6257e-08,  3.0744e-08, -8.1828e-10,\n",
       "                       1.8766e-08, -1.3068e-07, -3.5326e-09, -1.1380e-07, -3.3063e-08,\n",
       "                      -1.3027e-08,  9.6960e-08,  2.2013e-08, -2.8704e-08, -2.4641e-08,\n",
       "                      -2.7919e-07,  1.1581e-07, -3.2697e-07,  9.4290e-08, -1.2098e-08,\n",
       "                       1.6775e-07, -1.3871e-07,  2.3011e-07,  5.2738e-09, -8.2020e-08,\n",
       "                       1.9679e-08,  1.2810e-07, -1.9614e-07,  4.7903e-08,  1.5258e-07,\n",
       "                       1.8271e-07, -8.2259e-08])),\n",
       "             ('bert.encoder.layer.4.attention.self.value.weight',\n",
       "              tensor([[ 0.0267,  0.0072,  0.0340,  ...,  0.0101,  0.0072, -0.0074],\n",
       "                      [ 0.0044, -0.0110,  0.0072,  ..., -0.0252, -0.0211,  0.0201],\n",
       "                      [-0.0226,  0.0173, -0.0131,  ...,  0.0422, -0.0495, -0.0050],\n",
       "                      ...,\n",
       "                      [ 0.0138,  0.0039,  0.0107,  ...,  0.0134, -0.0031, -0.0133],\n",
       "                      [ 0.0278, -0.0027,  0.0128,  ...,  0.0046, -0.0187,  0.0291],\n",
       "                      [-0.0391,  0.0213, -0.0201,  ..., -0.0115, -0.0099,  0.0261]])),\n",
       "             ('bert.encoder.layer.4.attention.self.value.bias',\n",
       "              tensor([ 2.5111e-03, -1.2069e-02, -4.4653e-03, -7.7863e-03,  5.4015e-03,\n",
       "                      -1.1996e-02,  5.2072e-03,  2.7205e-03,  3.8880e-03, -2.0801e-03,\n",
       "                       1.2838e-02, -5.3906e-03,  5.6546e-03,  1.0906e-02,  2.9736e-03,\n",
       "                       4.9142e-03,  3.8062e-03,  6.3312e-03, -8.5307e-03, -4.8200e-03,\n",
       "                      -2.8113e-03,  1.3374e-03,  3.2668e-03, -7.1516e-03,  6.2762e-03,\n",
       "                      -2.4234e-03, -3.1491e-03,  8.1209e-03, -5.1596e-03, -3.4173e-03,\n",
       "                       2.7779e-03, -7.8053e-03, -9.5629e-03, -9.0743e-03,  8.4998e-03,\n",
       "                      -2.0343e-03, -8.8367e-04, -2.9255e-03, -1.6827e-03, -4.9122e-03,\n",
       "                       8.9469e-04, -5.5805e-03, -1.8510e-03, -7.7870e-03,  5.0110e-04,\n",
       "                      -9.2432e-03,  3.8383e-03,  2.0737e-03,  2.2126e-03,  4.9521e-03,\n",
       "                      -3.9415e-03,  9.0113e-03, -8.7205e-06, -8.5799e-03, -6.7739e-04,\n",
       "                       2.6513e-03,  8.0082e-03, -1.2187e-03,  5.5814e-03, -7.8356e-03,\n",
       "                      -6.4836e-03,  1.0554e-02,  2.6599e-03,  3.5584e-03,  2.1380e-03,\n",
       "                      -1.0537e-02, -3.3319e-03,  3.1460e-03,  3.9320e-03, -8.7738e-03,\n",
       "                       5.1011e-03,  4.7810e-03, -4.4214e-04,  2.0805e-03, -7.1336e-03,\n",
       "                       3.5691e-03, -8.9899e-03, -4.5444e-03,  5.6281e-03,  8.3690e-03,\n",
       "                       6.7046e-03,  1.1873e-02, -4.4159e-03, -3.1938e-03,  2.1553e-03,\n",
       "                       1.7090e-03,  2.1409e-03,  9.6551e-03, -2.6168e-03, -1.6144e-03,\n",
       "                      -1.2751e-03,  5.0467e-03,  2.2559e-03, -5.1622e-03, -4.6109e-03,\n",
       "                       2.8435e-03, -2.1204e-03, -7.1449e-03, -8.2704e-03, -4.0764e-03,\n",
       "                       8.2325e-03,  5.3607e-03, -6.6233e-03, -4.9976e-04, -6.5172e-03,\n",
       "                       9.1650e-03,  1.7878e-02, -2.5213e-04,  4.2635e-03,  2.5794e-04,\n",
       "                       8.7373e-03, -8.3840e-03,  3.8913e-03,  1.1380e-02,  3.9836e-03,\n",
       "                      -1.7111e-02, -1.2745e-04,  5.6497e-03,  1.9542e-02, -1.8747e-02,\n",
       "                      -1.2201e-02, -1.8948e-02,  1.8524e-03, -5.4408e-03, -1.8047e-02,\n",
       "                      -8.4605e-03, -2.8827e-03, -3.8957e-03, -1.5677e-03,  6.6853e-03,\n",
       "                      -4.4062e-03,  2.8186e-04,  4.7917e-04,  6.1163e-03, -8.2013e-03,\n",
       "                       2.3202e-03,  3.9880e-03, -1.5462e-03, -4.2745e-03, -4.5689e-03,\n",
       "                       3.5596e-03, -1.7012e-04, -8.7395e-03, -3.3788e-03, -2.6078e-04,\n",
       "                      -7.0607e-03,  2.7669e-03, -1.2538e-02, -4.1388e-03, -1.9669e-03,\n",
       "                       5.8108e-03,  2.0291e-03,  4.5703e-03,  4.1038e-03, -7.0698e-03,\n",
       "                       2.6204e-03, -5.9953e-03, -4.3981e-03,  7.8528e-03, -2.6541e-03,\n",
       "                       7.5055e-03, -4.4710e-03, -5.6836e-03,  2.6511e-03, -9.2580e-03,\n",
       "                      -2.8632e-03, -2.5166e-05,  1.3735e-03,  1.9219e-03, -2.6453e-03,\n",
       "                      -7.0955e-04,  1.2119e-02, -3.5768e-03, -6.7374e-03, -1.8497e-03,\n",
       "                       1.6000e-03,  3.6290e-03, -5.0136e-04, -1.3208e-02, -1.2320e-02,\n",
       "                       1.0384e-02, -1.6384e-02, -1.3412e-02, -4.6399e-03, -8.0502e-03,\n",
       "                       7.4695e-04, -7.6880e-03,  9.2014e-03, -9.0373e-03,  1.4921e-03,\n",
       "                       1.0402e-02,  6.4326e-03])),\n",
       "             ('bert.encoder.layer.4.attention.output.dense.weight',\n",
       "              tensor([[ 0.0020,  0.0217,  0.0300,  ...,  0.0090, -0.0050,  0.0091],\n",
       "                      [ 0.0005, -0.0155,  0.0508,  ..., -0.0302,  0.0123, -0.0262],\n",
       "                      [ 0.0131, -0.0114,  0.0433,  ..., -0.0172,  0.0077,  0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0283, -0.0232, -0.0316,  ...,  0.0306,  0.0071, -0.0090],\n",
       "                      [ 0.0354,  0.0237, -0.0373,  ..., -0.0068, -0.0295, -0.0086],\n",
       "                      [ 0.0190, -0.0039,  0.0185,  ...,  0.0301,  0.0247,  0.0284]])),\n",
       "             ('bert.encoder.layer.4.attention.output.dense.bias',\n",
       "              tensor([ 3.6378e-03,  2.6837e-03,  1.7973e-03,  5.9392e-03, -3.1160e-03,\n",
       "                      -1.0664e-02, -8.0167e-03, -4.5140e-03,  5.0472e-03, -4.0974e-03,\n",
       "                       8.2360e-03,  9.6346e-03, -7.7835e-03, -4.9859e-03,  1.2915e-03,\n",
       "                      -3.1205e-03,  6.4817e-04,  5.5849e-03,  1.2630e-03, -7.7262e-04,\n",
       "                       1.7127e-03, -5.4821e-03, -8.0049e-03,  2.3579e-03,  1.1642e-03,\n",
       "                       6.6724e-04,  4.5290e-03,  1.6810e-02, -7.0667e-03,  1.0125e-02,\n",
       "                       1.4879e-03, -1.1055e-03, -4.3769e-03, -1.2231e-02, -9.0146e-03,\n",
       "                      -5.3262e-03, -8.8212e-03, -3.4995e-03,  2.8804e-03,  2.7674e-03,\n",
       "                       1.3470e-03, -3.4833e-03, -2.1464e-03,  4.2674e-03,  6.0540e-03,\n",
       "                      -6.3986e-03, -1.7937e-03,  4.7586e-03,  5.6139e-04, -6.0648e-03,\n",
       "                      -5.0468e-04, -1.7556e-03, -7.8805e-03, -2.2705e-03, -2.4770e-03,\n",
       "                      -3.9534e-03, -3.7368e-03,  4.0542e-03,  4.1726e-03, -3.2489e-04,\n",
       "                      -1.0211e-03, -4.0888e-03,  8.2724e-03,  2.2229e-03, -2.3517e-03,\n",
       "                       7.8832e-04, -7.0384e-03, -2.9526e-03, -2.1995e-03,  1.0316e-03,\n",
       "                       3.4577e-03, -2.8065e-03, -4.7378e-04,  7.3078e-03, -1.7072e-03,\n",
       "                       6.6695e-03,  1.6531e-04,  1.0442e-03,  2.4131e-03, -2.3093e-03,\n",
       "                      -5.0787e-03,  4.7779e-03, -3.9640e-03,  1.6016e-03, -2.1304e-03,\n",
       "                      -4.3253e-03, -2.8539e-03, -4.9838e-03, -1.1810e-03,  3.6043e-04,\n",
       "                      -6.8898e-03,  3.1290e-03,  1.8106e-03,  9.1480e-03, -1.2892e-02,\n",
       "                       1.3842e-02, -5.2034e-03, -1.7175e-03, -2.3784e-03,  1.4612e-03,\n",
       "                      -3.9930e-03,  3.0153e-03, -1.0852e-02,  6.8223e-03, -1.1580e-03,\n",
       "                       1.6352e-03, -8.8297e-04,  1.1057e-03,  1.2134e-03, -1.0433e-03,\n",
       "                       2.8625e-03, -1.1723e-03,  3.0580e-03, -6.1904e-03, -1.8895e-02,\n",
       "                       3.2759e-03,  3.1205e-03, -7.4682e-03,  4.3764e-03, -8.4375e-03,\n",
       "                       4.0213e-03, -3.7660e-03,  3.9786e-03, -6.8215e-03,  4.7744e-03,\n",
       "                       4.1619e-04,  2.6795e-03,  5.9839e-03,  1.2878e-02, -5.6808e-03,\n",
       "                       7.0770e-03,  4.0587e-03,  6.4022e-03, -1.0741e-02,  3.8110e-04,\n",
       "                       5.5087e-03, -3.7249e-03, -1.3351e-03,  5.2380e-03,  3.9559e-03,\n",
       "                       2.8837e-03,  2.3751e-03,  6.1087e-03,  9.5479e-04, -1.7221e-03,\n",
       "                      -3.7360e-03,  5.3290e-04, -6.2289e-03, -8.5067e-04, -3.6764e-03,\n",
       "                      -9.0155e-03,  4.2626e-03,  1.3415e-04, -5.6002e-03, -1.2737e-03,\n",
       "                      -3.5671e-03, -7.5051e-03,  2.4286e-03,  4.2277e-03, -2.3282e-03,\n",
       "                      -1.4520e-03,  8.4786e-04, -2.6348e-04, -6.6002e-03, -1.5171e-04,\n",
       "                       4.9011e-03,  1.8192e-03,  7.3328e-03,  2.4225e-03,  1.1197e-03,\n",
       "                      -2.5742e-04, -2.0181e-03, -3.3433e-06,  1.9661e-03,  5.1957e-03,\n",
       "                       8.3846e-04, -2.8141e-03, -1.7727e-03, -5.8544e-05, -8.1945e-03,\n",
       "                       1.5418e-02, -6.3809e-03, -3.5017e-03, -2.8189e-04,  5.7643e-05,\n",
       "                      -1.1720e-03, -2.8838e-03, -8.3185e-03,  3.2832e-03,  7.5873e-03,\n",
       "                      -3.0553e-03,  1.0303e-03])),\n",
       "             ('bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0235, 1.0158, 0.9953, 1.0029, 1.0110, 0.9817, 1.0140, 1.0061, 0.9879,\n",
       "                      1.0187, 0.9796, 1.0140, 1.0180, 1.0102, 1.0015, 1.0018, 1.0104, 0.9863,\n",
       "                      0.9886, 1.0231, 1.0118, 1.0268, 0.9912, 1.0127, 0.9984, 1.0155, 1.0273,\n",
       "                      0.9882, 0.9752, 1.0294, 1.0086, 1.0137, 1.0039, 1.0173, 1.0181, 0.9954,\n",
       "                      1.0056, 0.9589, 1.0195, 1.0156, 1.0118, 1.0160, 1.0074, 1.0061, 1.0204,\n",
       "                      1.0174, 1.0052, 1.0252, 0.9952, 1.0167, 1.0248, 0.9580, 0.9510, 1.0138,\n",
       "                      1.0154, 1.0080, 1.0122, 1.0147, 1.0223, 1.0107, 1.0044, 0.9893, 1.0269,\n",
       "                      1.0024, 1.0175, 1.0065, 1.0126, 0.9337, 1.0088, 1.0037, 1.0101, 0.9797,\n",
       "                      1.0091, 1.0293, 1.0154, 0.9922, 1.0164, 1.0058, 1.0385, 1.0208, 1.0084,\n",
       "                      1.0052, 1.0179, 1.0271, 1.0331, 1.0031, 1.0316, 1.0084, 1.0102, 1.0106,\n",
       "                      1.0071, 0.9851, 1.0150, 0.9974, 0.9952, 0.9566, 1.0078, 0.9693, 1.0121,\n",
       "                      1.0186, 1.0048, 1.0218, 1.0247, 0.9979, 1.0138, 1.0003, 1.0157, 0.9916,\n",
       "                      1.0243, 1.0293, 0.9872, 1.0218, 1.0209, 0.9950, 0.9281, 1.0159, 1.0030,\n",
       "                      1.0070, 1.0090, 1.0238, 0.9931, 0.9577, 1.0099, 1.0043, 1.0249, 1.0280,\n",
       "                      1.0230, 1.0089, 1.0201, 1.0106, 0.9744, 0.9697, 1.0127, 1.0041, 1.0330,\n",
       "                      1.0033, 0.9908, 1.0130, 0.9769, 1.0166, 1.0112, 1.0110, 0.9984, 1.0233,\n",
       "                      0.9896, 1.0006, 0.9850, 1.0056, 1.0078, 1.0284, 0.9997, 0.9558, 1.0167,\n",
       "                      1.0256, 0.9928, 1.0237, 1.0199, 0.9983, 1.0044, 0.9616, 1.0004, 0.9949,\n",
       "                      1.0088, 1.0140, 0.9926, 1.0147, 1.0056, 1.0135, 1.0114, 0.9956, 0.9999,\n",
       "                      1.0000, 1.0085, 1.0108, 1.0231, 1.0063, 1.0152, 0.9599, 1.0124, 1.0012,\n",
       "                      0.9415, 1.0160, 1.0058, 1.0196, 1.0068, 1.0164, 1.0047, 1.0117, 0.9890,\n",
       "                      0.9866, 1.0004, 1.0232])),\n",
       "             ('bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       "              tensor([ 4.6133e-03,  9.3362e-03,  1.0494e-02,  3.1787e-03, -2.5637e-03,\n",
       "                      -9.4950e-03, -7.7504e-03, -1.3313e-02,  4.4383e-03, -5.1568e-03,\n",
       "                       1.8642e-02,  2.0692e-02, -5.6464e-03,  7.8829e-03,  1.0311e-02,\n",
       "                      -1.2983e-02,  1.0118e-03,  1.4164e-02, -4.7914e-03,  2.0945e-04,\n",
       "                       1.2982e-02, -3.6837e-03, -1.3305e-02,  1.0173e-02, -8.4078e-03,\n",
       "                      -3.8433e-03,  8.5249e-03,  2.9794e-02, -5.4695e-03,  1.6505e-02,\n",
       "                       1.2757e-02, -2.5051e-03, -6.9065e-03, -1.4842e-02, -1.2392e-02,\n",
       "                      -4.5961e-03, -1.2864e-02, -1.2067e-02,  4.5609e-03, -5.5368e-03,\n",
       "                      -1.7999e-03, -7.7919e-03, -4.9427e-03,  1.3081e-02,  2.9191e-03,\n",
       "                      -1.1805e-02,  5.8072e-03, -9.8878e-04,  3.5581e-04, -5.3189e-03,\n",
       "                      -5.5943e-03, -8.5910e-03, -4.9060e-03,  4.6838e-03,  6.0358e-03,\n",
       "                      -6.6393e-03, -6.9913e-03, -9.5986e-04,  1.7586e-03,  4.2425e-03,\n",
       "                      -7.5064e-03, -3.9888e-03,  1.0288e-02, -1.4685e-02,  8.0236e-04,\n",
       "                      -1.8949e-03, -1.0233e-02,  1.3456e-02,  3.0356e-03,  6.8873e-03,\n",
       "                       9.9742e-03, -2.2092e-04,  1.5824e-03,  1.3446e-02, -2.1486e-03,\n",
       "                       1.4351e-02, -1.0930e-03,  2.3500e-03, -7.9622e-04, -1.5366e-03,\n",
       "                       2.0113e-03,  1.2505e-02, -5.3490e-03, -9.3478e-03, -3.0342e-05,\n",
       "                      -1.0131e-02, -4.9898e-03, -7.9646e-03,  1.0099e-02, -1.4939e-03,\n",
       "                      -1.2294e-02,  1.4883e-02,  3.6203e-03,  7.1398e-03, -1.6122e-02,\n",
       "                       3.3743e-02, -1.0047e-04, -1.2511e-03, -3.3187e-03,  2.0722e-03,\n",
       "                      -7.4956e-03,  1.4828e-02, -9.6522e-03,  4.0709e-03,  2.9999e-03,\n",
       "                       1.0414e-02, -8.4374e-03,  7.5867e-03, -9.2333e-03,  6.4617e-03,\n",
       "                      -3.9824e-03, -1.1015e-04,  3.9783e-03, -1.3026e-02, -2.7393e-02,\n",
       "                       1.7281e-02,  8.8441e-03, -9.3252e-03,  1.4154e-02, -1.4111e-02,\n",
       "                      -3.0677e-03, -6.9078e-03,  5.2624e-03, -5.0119e-03,  3.2045e-03,\n",
       "                       1.6492e-03,  1.8061e-02,  3.7507e-03,  2.1712e-02, -4.8211e-03,\n",
       "                       8.0707e-03,  2.7033e-03,  6.6219e-03, -8.1082e-03, -4.2588e-03,\n",
       "                       7.6024e-03, -7.9742e-03,  9.2300e-03,  5.9181e-03,  1.5133e-02,\n",
       "                      -4.5896e-03,  1.2440e-03,  3.2842e-03, -4.2526e-03, -8.3481e-03,\n",
       "                      -9.6930e-03,  4.2573e-03, -8.7411e-06,  3.3900e-03, -1.0632e-02,\n",
       "                      -2.9848e-03,  8.9748e-03, -1.0890e-02, -1.2637e-02,  4.6767e-04,\n",
       "                      -6.0910e-03, -1.6762e-02,  6.4355e-03,  7.9747e-03, -1.1516e-02,\n",
       "                      -9.3376e-03,  4.9917e-03, -5.5415e-03, -1.0514e-02,  1.5482e-03,\n",
       "                       8.3105e-03,  2.2466e-03,  6.9478e-03,  1.1161e-02, -7.0960e-03,\n",
       "                      -7.3751e-03, -8.4563e-03,  4.1216e-03, -5.6408e-03,  9.3079e-03,\n",
       "                       9.1383e-03, -1.3095e-02, -1.1341e-02,  1.3083e-03, -4.5373e-03,\n",
       "                       3.0981e-02, -4.5165e-03, -1.3022e-02, -8.4004e-03,  3.4503e-03,\n",
       "                       4.2809e-03, -1.3913e-02, -5.2080e-03,  9.2101e-03,  1.7230e-02,\n",
       "                      -1.3418e-02, -7.5086e-04])),\n",
       "             ('bert.encoder.layer.4.intermediate.dense.weight',\n",
       "              tensor([[ 6.6158e-03, -4.3227e-02, -2.2611e-02,  ..., -1.9781e-02,\n",
       "                        1.6530e-02, -2.1007e-02],\n",
       "                      [ 2.2926e-06,  1.2516e-02, -1.4085e-02,  ...,  3.8476e-03,\n",
       "                        1.4527e-02,  4.8004e-03],\n",
       "                      [-4.9870e-02,  9.3581e-03, -5.5752e-03,  ..., -8.0646e-04,\n",
       "                        1.2236e-02,  3.0652e-02],\n",
       "                      ...,\n",
       "                      [ 2.9640e-02, -1.8857e-02, -7.9710e-03,  ..., -3.1403e-02,\n",
       "                        4.9274e-02, -2.0044e-03],\n",
       "                      [-1.2566e-02, -2.6421e-03,  4.2273e-02,  ...,  1.7204e-02,\n",
       "                       -1.3719e-03,  1.6898e-02],\n",
       "                      [ 3.3733e-02,  2.4350e-02,  6.7413e-03,  ...,  1.1592e-02,\n",
       "                       -1.8221e-02,  1.2902e-03]])),\n",
       "             ('bert.encoder.layer.4.intermediate.dense.bias',\n",
       "              tensor([-1.4854e-03, -7.3179e-03, -3.8624e-03,  1.7968e-02,  2.3987e-02,\n",
       "                       9.9714e-03,  3.5888e-02,  1.9210e-02,  3.0148e-02, -1.3895e-02,\n",
       "                      -1.4666e-02, -2.0569e-02, -7.5411e-03, -2.3285e-02,  7.9093e-03,\n",
       "                      -3.8591e-03, -3.1959e-04,  2.1876e-02, -2.1515e-02, -2.2328e-02,\n",
       "                       2.9033e-02, -1.3011e-02,  4.1308e-03,  1.6556e-02, -4.5068e-03,\n",
       "                       1.2709e-02, -1.7921e-02, -5.9206e-03,  1.3905e-02, -1.3273e-03,\n",
       "                      -2.7306e-02, -6.4617e-03,  1.2698e-02, -2.2906e-02,  1.8473e-02,\n",
       "                       7.0384e-03, -5.4002e-03,  7.5092e-03, -2.7130e-02,  3.2620e-03,\n",
       "                       2.4345e-02,  2.7145e-02, -8.0069e-04,  1.0143e-02, -9.2409e-03,\n",
       "                      -2.5956e-02, -2.5367e-02, -1.2876e-02, -1.8453e-02, -2.2352e-02,\n",
       "                       4.3102e-03,  1.0917e-02,  8.0063e-05,  1.7208e-02, -3.5360e-02,\n",
       "                      -3.0221e-02,  7.1598e-03,  9.5445e-03, -4.9379e-03, -1.8252e-03,\n",
       "                       1.3975e-02, -2.0973e-02,  3.4039e-02, -5.2556e-03])),\n",
       "             ('bert.encoder.layer.4.output.dense.weight',\n",
       "              tensor([[ 0.0145,  0.0072, -0.0166,  ..., -0.0276,  0.0215,  0.0316],\n",
       "                      [ 0.0178, -0.0167, -0.0156,  ..., -0.0068, -0.0128, -0.0109],\n",
       "                      [ 0.0088, -0.0243, -0.0107,  ..., -0.0238,  0.0292, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0238,  0.0255,  0.0086,  ..., -0.0264,  0.0278,  0.0023],\n",
       "                      [-0.0213, -0.0160,  0.0109,  ...,  0.0314,  0.0219,  0.0103],\n",
       "                      [-0.0284,  0.0091, -0.0182,  ...,  0.0352,  0.0238,  0.0131]])),\n",
       "             ('bert.encoder.layer.4.output.dense.bias',\n",
       "              tensor([ 5.8778e-03,  4.9842e-03,  8.1538e-03, -3.2708e-04, -3.8987e-03,\n",
       "                       2.7655e-04, -6.1985e-03, -1.2274e-02,  2.3118e-03, -2.2273e-03,\n",
       "                       1.1883e-02,  1.6336e-02, -5.0270e-03,  1.1989e-03,  3.0018e-03,\n",
       "                      -8.7210e-03,  4.8739e-03,  1.0749e-02, -4.3413e-03,  2.4792e-03,\n",
       "                       8.3238e-03, -1.0557e-04, -1.1242e-02,  8.5715e-03, -3.0435e-03,\n",
       "                      -2.4667e-03,  3.7525e-03,  2.6333e-02, -3.1502e-03,  9.5665e-03,\n",
       "                       1.2549e-02,  4.3346e-04, -4.9563e-03, -1.4798e-02, -1.3067e-02,\n",
       "                      -3.8608e-03, -9.4838e-03, -1.0907e-02,  4.7604e-03, -3.5318e-03,\n",
       "                       1.2906e-03, -6.9100e-03,  8.0823e-04,  8.7450e-03,  8.6412e-03,\n",
       "                      -5.9683e-03,  6.4318e-03, -6.5855e-03, -1.0019e-03, -4.1238e-03,\n",
       "                      -2.8411e-03, -4.5610e-03, -5.3398e-03,  6.1931e-05,  3.7022e-03,\n",
       "                      -4.6568e-03, -5.0049e-03,  6.0900e-03,  2.0832e-03,  3.2731e-03,\n",
       "                      -1.1777e-03, -1.0065e-04,  7.3038e-03, -8.8985e-03, -3.3664e-03,\n",
       "                      -2.9603e-03, -6.2577e-03,  1.3698e-02,  1.6817e-03, -5.7785e-04,\n",
       "                       8.0122e-03, -2.6936e-03,  4.8180e-03,  1.1228e-02, -1.0002e-03,\n",
       "                       1.0072e-02, -5.0335e-03, -1.1020e-03,  5.4090e-04,  8.3276e-04,\n",
       "                      -3.2166e-03,  7.7679e-03, -4.6070e-03, -3.2532e-03, -2.2275e-03,\n",
       "                      -5.6154e-03, -3.3353e-03, -2.9763e-03,  7.8709e-03, -6.1048e-03,\n",
       "                      -8.6311e-03,  1.3487e-02,  2.1562e-03,  7.6652e-03, -1.8036e-02,\n",
       "                       3.2271e-02,  7.2615e-04, -6.3501e-04,  2.5586e-03,  1.5986e-03,\n",
       "                      -3.4421e-03,  1.0237e-02, -6.0478e-03,  3.3476e-03,  2.0123e-04,\n",
       "                       8.7521e-03, -1.0375e-02,  5.8630e-03, -4.4043e-03,  4.2231e-03,\n",
       "                      -8.7666e-04, -1.5275e-03,  2.6309e-03, -3.8274e-03, -3.1632e-02,\n",
       "                       1.0896e-02,  6.9156e-03, -3.8338e-03,  1.5110e-02, -9.4966e-03,\n",
       "                      -2.7657e-03, -5.9272e-03,  3.8187e-04, -5.1266e-04,  5.6537e-03,\n",
       "                      -2.0968e-03,  1.4736e-02,  1.6476e-03,  1.1563e-02, -3.8725e-03,\n",
       "                       6.8514e-03,  9.7405e-04,  2.3930e-03, -3.8372e-03, -3.4185e-03,\n",
       "                       6.3271e-03, -1.0441e-02,  7.1706e-03, -1.1935e-04,  1.4572e-02,\n",
       "                      -1.2862e-03,  1.9411e-04,  4.4869e-03, -5.1774e-03, -5.2168e-03,\n",
       "                      -8.9763e-03,  2.3889e-03,  2.4460e-03, -1.9811e-03, -4.1849e-03,\n",
       "                      -5.7936e-03,  6.7802e-03, -6.7899e-03, -5.2568e-03, -1.0307e-03,\n",
       "                      -6.0562e-03, -1.2269e-02,  1.2657e-02,  8.2389e-03, -1.1339e-02,\n",
       "                      -4.7606e-03,  1.0474e-03, -1.2779e-03, -8.6472e-03, -2.2952e-03,\n",
       "                       4.5496e-03,  4.2869e-03,  4.2385e-03,  5.6617e-03, -5.2626e-03,\n",
       "                      -4.1856e-03, -2.6916e-03,  5.9389e-03, -7.5047e-03,  1.8078e-03,\n",
       "                       4.0296e-03, -2.1246e-03, -1.2611e-02, -2.0988e-03, -3.6671e-04,\n",
       "                       2.9287e-02, -7.7823e-04, -5.7679e-03, -1.0240e-02,  4.0381e-04,\n",
       "                      -1.4975e-04, -9.7307e-03, -5.4848e-03,  8.3507e-03,  1.2652e-02,\n",
       "                      -4.4695e-03,  1.3786e-03])),\n",
       "             ('bert.encoder.layer.4.output.LayerNorm.weight',\n",
       "              tensor([1.0380, 1.0266, 1.0061, 1.0120, 1.0190, 0.9886, 1.0261, 1.0231, 0.9994,\n",
       "                      1.0259, 0.9796, 1.0265, 1.0308, 1.0228, 1.0173, 1.0100, 1.0202, 1.0056,\n",
       "                      0.9905, 1.0382, 1.0229, 1.0400, 1.0022, 1.0200, 1.0096, 1.0299, 1.0453,\n",
       "                      0.9848, 0.9685, 1.0472, 1.0313, 1.0248, 1.0200, 1.0322, 1.0318, 1.0032,\n",
       "                      1.0174, 0.9487, 1.0327, 1.0260, 1.0242, 1.0337, 1.0257, 1.0243, 1.0345,\n",
       "                      1.0245, 1.0110, 1.0382, 1.0082, 1.0329, 1.0381, 0.9506, 0.9296, 1.0248,\n",
       "                      1.0291, 1.0189, 1.0245, 1.0212, 1.0377, 1.0216, 1.0163, 0.9990, 1.0379,\n",
       "                      1.0067, 1.0276, 1.0206, 1.0274, 0.9103, 1.0230, 1.0088, 1.0180, 0.9919,\n",
       "                      1.0258, 1.0517, 1.0276, 1.0014, 1.0318, 1.0227, 1.0535, 1.0321, 1.0161,\n",
       "                      1.0172, 1.0279, 1.0454, 1.0509, 1.0134, 1.0548, 1.0170, 1.0300, 1.0251,\n",
       "                      1.0211, 0.9874, 1.0280, 1.0105, 1.0013, 0.9370, 1.0187, 0.9672, 1.0220,\n",
       "                      1.0280, 1.0179, 1.0338, 1.0367, 1.0091, 1.0256, 1.0094, 1.0307, 1.0059,\n",
       "                      1.0410, 1.0469, 0.9874, 1.0327, 1.0383, 1.0030, 0.8507, 1.0272, 1.0081,\n",
       "                      1.0144, 1.0204, 1.0449, 0.9928, 0.9548, 1.0197, 1.0155, 1.0346, 1.0430,\n",
       "                      1.0346, 1.0232, 1.0350, 1.0270, 0.9625, 0.9776, 1.0222, 1.0168, 1.0446,\n",
       "                      1.0218, 0.9877, 1.0266, 0.9835, 1.0335, 1.0197, 1.0260, 1.0100, 1.0382,\n",
       "                      0.9942, 1.0032, 0.9861, 1.0248, 1.0211, 1.0454, 1.0179, 0.9447, 1.0267,\n",
       "                      1.0363, 1.0044, 1.0337, 1.0356, 1.0015, 1.0133, 0.9515, 1.0135, 1.0018,\n",
       "                      1.0189, 1.0283, 1.0054, 1.0308, 1.0178, 1.0266, 1.0262, 1.0100, 1.0065,\n",
       "                      1.0212, 1.0280, 1.0298, 1.0350, 1.0156, 1.0312, 0.9586, 1.0236, 1.0143,\n",
       "                      0.9002, 1.0314, 1.0146, 1.0345, 1.0221, 1.0343, 1.0126, 1.0232, 0.9933,\n",
       "                      0.9844, 1.0017, 1.0397])),\n",
       "             ('bert.encoder.layer.4.output.LayerNorm.bias',\n",
       "              tensor([ 4.5536e-04,  8.0099e-03,  1.2062e-02,  9.0986e-03,  1.9523e-03,\n",
       "                      -1.4705e-02, -1.0440e-02, -1.4899e-02,  7.0731e-03, -5.1542e-03,\n",
       "                       3.1410e-02,  1.8845e-02, -1.0840e-02,  1.4134e-02,  1.3801e-02,\n",
       "                      -6.5202e-03,  1.3986e-03,  1.9902e-02, -1.3982e-03, -4.2304e-05,\n",
       "                       6.8979e-03, -6.5548e-03, -5.7511e-03,  4.6976e-03, -2.0765e-03,\n",
       "                      -8.6527e-03,  8.6044e-03,  2.2533e-02,  5.3369e-03,  1.6327e-02,\n",
       "                       2.2287e-02, -1.6334e-03, -1.0199e-02, -1.5038e-02, -1.3233e-02,\n",
       "                      -1.2731e-02, -1.1609e-02, -2.6779e-03,  4.2840e-03, -8.1444e-03,\n",
       "                      -2.1035e-04, -1.1584e-02, -6.2114e-03,  1.7241e-02,  4.5695e-03,\n",
       "                      -9.6746e-03,  2.3145e-03, -4.3053e-04, -3.8796e-03, -2.7635e-03,\n",
       "                      -8.5510e-03,  2.7762e-03,  1.0942e-02,  1.5697e-03,  8.1437e-03,\n",
       "                      -1.2554e-02, -4.6300e-03, -6.2337e-03,  6.2227e-04,  2.2531e-04,\n",
       "                      -6.0169e-03, -8.4368e-03,  1.3771e-02, -9.4711e-03,  5.0295e-05,\n",
       "                       8.3831e-04, -1.0066e-02, -1.6187e-03,  1.6660e-03, -8.2572e-04,\n",
       "                       6.6913e-03, -1.0129e-03, -3.6829e-03,  1.5497e-02, -3.9385e-03,\n",
       "                       1.6119e-02,  9.2634e-04,  6.4315e-03, -2.0004e-03, -3.3265e-04,\n",
       "                      -1.6477e-03,  1.5475e-02, -2.1374e-03, -1.5832e-02,  4.0149e-03,\n",
       "                      -9.2989e-03, -1.0999e-02, -7.7390e-03,  1.7224e-02, -2.3934e-03,\n",
       "                      -1.7875e-02,  9.9147e-03,  3.9528e-03,  6.7472e-03, -1.2488e-02,\n",
       "                       1.6851e-02,  1.1421e-03, -1.1873e-02,  4.0367e-04,  1.4792e-03,\n",
       "                      -9.3170e-03,  1.5628e-02, -6.7758e-03,  1.2135e-03,  6.8755e-04,\n",
       "                       7.8275e-03, -1.2957e-02,  6.3684e-03, -1.1643e-02,  6.4658e-03,\n",
       "                      -1.2589e-03, -4.2964e-04,  7.3696e-03, -1.9632e-02,  2.5559e-02,\n",
       "                       1.9235e-02,  2.4595e-03, -5.3170e-03,  1.5564e-02, -1.6644e-02,\n",
       "                       2.9263e-03, -2.0303e-02,  1.8140e-03, -3.4412e-03,  3.8561e-03,\n",
       "                       2.0918e-03,  1.9055e-02,  3.1774e-03,  2.2321e-02, -5.8323e-03,\n",
       "                      -2.8374e-03,  3.4342e-03,  1.1053e-02, -7.4604e-03, -1.3109e-03,\n",
       "                       1.2954e-02,  5.0923e-03,  9.4718e-03,  7.0264e-03,  2.0137e-02,\n",
       "                      -6.1093e-03,  2.4908e-03,  3.7174e-03, -3.6242e-03, -3.8911e-03,\n",
       "                      -1.0134e-02,  4.7114e-05,  5.2156e-03,  2.3405e-03, -8.6101e-03,\n",
       "                       2.4485e-03, -5.7275e-03, -6.9544e-03, -1.1179e-02, -8.5812e-03,\n",
       "                      -5.9195e-03, -1.9260e-02, -2.2849e-03,  1.2238e-02,  5.9443e-04,\n",
       "                      -9.4146e-03,  7.4059e-03,  1.4085e-04, -6.9448e-03,  2.7550e-03,\n",
       "                       1.1564e-02,  9.6740e-04,  7.8776e-03,  1.5825e-02, -8.5771e-03,\n",
       "                      -3.7508e-03, -1.8586e-02,  1.5795e-03, -6.6262e-03,  1.3135e-02,\n",
       "                       1.6423e-03, -1.1897e-02, -9.3537e-03, -8.0492e-04, -1.0527e-03,\n",
       "                       2.7258e-03, -6.7332e-03, -1.0983e-02, -1.0262e-02,  6.1153e-03,\n",
       "                       9.2110e-03, -1.1673e-02,  6.8945e-05,  3.9481e-03,  6.0037e-03,\n",
       "                      -6.5718e-03, -2.2910e-04])),\n",
       "             ('bert.encoder.layer.5.attention.self.query.weight',\n",
       "              tensor([[ 0.0249, -0.0487, -0.0328,  ..., -0.0283,  0.0579,  0.0275],\n",
       "                      [ 0.0154, -0.0320,  0.0095,  ...,  0.0120, -0.0011,  0.0489],\n",
       "                      [-0.0178, -0.0130, -0.0371,  ..., -0.0099,  0.0173,  0.0499],\n",
       "                      ...,\n",
       "                      [-0.0356,  0.0224, -0.0566,  ...,  0.0179,  0.0024,  0.0171],\n",
       "                      [-0.0150, -0.0227, -0.0525,  ..., -0.0708,  0.0044, -0.0215],\n",
       "                      [-0.0069,  0.0263, -0.0090,  ...,  0.0305, -0.0285, -0.0126]])),\n",
       "             ('bert.encoder.layer.5.attention.self.query.bias',\n",
       "              tensor([-3.3469e-02, -2.2286e-02, -1.5606e-02, -1.8061e-02,  3.3367e-02,\n",
       "                      -1.6786e-02,  8.0374e-03, -2.0422e-02,  3.2736e-02, -4.9067e-03,\n",
       "                       1.8100e-03,  5.2797e-03,  5.4836e-03,  2.7408e-02, -2.3455e-02,\n",
       "                      -1.4033e-02, -8.0355e-03, -9.2293e-03, -5.1406e-04, -7.7880e-03,\n",
       "                      -1.8388e-02,  3.0658e-02, -4.1480e-02,  7.8231e-03,  2.7542e-02,\n",
       "                       2.3130e-02, -4.6169e-02, -1.7389e-02,  3.9791e-02,  5.8022e-03,\n",
       "                      -1.4513e-03, -1.8931e-02,  2.0631e-02,  4.0420e-03, -9.5750e-03,\n",
       "                      -4.0550e-02,  3.2458e-02, -5.5230e-03,  3.5400e-02, -2.0900e-02,\n",
       "                       7.7578e-03, -4.4718e-02,  3.1635e-02,  2.5435e-02, -4.9739e-02,\n",
       "                      -4.3317e-03, -1.1022e-02, -2.8650e-02, -5.5906e-02, -5.6876e-02,\n",
       "                       3.1976e-02, -9.2993e-03,  1.1885e-02,  1.5996e-02,  2.3849e-02,\n",
       "                       3.5762e-02, -4.1887e-02,  5.9313e-03, -2.6727e-02,  2.4278e-02,\n",
       "                      -4.5071e-02, -4.1371e-02, -9.2606e-03, -3.8204e-02,  2.6380e-02,\n",
       "                       2.5180e-02,  1.1014e-02, -1.5633e-02,  1.6812e-02, -7.5001e-03,\n",
       "                      -1.2388e-02, -2.7821e-03,  1.4597e-02,  1.7036e-02, -4.6719e-02,\n",
       "                      -6.8816e-05, -1.6013e-02,  3.7765e-02,  1.1461e-02, -4.5392e-02,\n",
       "                       2.7958e-02,  6.5200e-03, -4.6423e-02,  1.8249e-03, -3.5528e-02,\n",
       "                       5.1705e-03, -2.2993e-02,  1.2708e-02,  5.9935e-02,  4.4134e-03,\n",
       "                       4.1974e-02,  3.6109e-03,  2.3376e-02, -2.2768e-04, -4.0071e-02,\n",
       "                      -7.9357e-03, -2.7076e-02, -4.1150e-02, -1.6044e-02, -4.8144e-02,\n",
       "                       3.1831e-04,  1.6075e-02,  5.1154e-03, -6.4967e-02, -2.6403e-02,\n",
       "                      -2.4792e-02, -1.6802e-02, -2.9846e-02,  2.6487e-02, -1.3610e-02,\n",
       "                       6.5758e-03,  4.5238e-02,  1.7647e-02, -3.5044e-03, -2.9599e-03,\n",
       "                       2.8594e-02,  5.0074e-02, -2.1529e-02, -1.1212e-03,  3.3176e-03,\n",
       "                       1.7497e-02,  4.8686e-02,  2.6062e-02, -2.0962e-02,  3.6025e-02,\n",
       "                       5.8416e-02,  2.0150e-02,  2.5669e-02,  3.1435e-02,  1.9137e-02,\n",
       "                      -2.3841e-03, -2.2776e-02, -2.7429e-02,  3.7943e-02,  1.7159e-02,\n",
       "                      -2.8360e-02, -4.4072e-03, -3.3412e-03, -3.5010e-02, -3.2685e-02,\n",
       "                      -1.0270e-02, -2.5507e-02,  8.7467e-03, -2.5073e-02, -2.6019e-02,\n",
       "                       2.4945e-02, -2.4227e-02, -4.0521e-02,  2.6958e-02,  2.4094e-02,\n",
       "                       1.7532e-02,  2.0983e-02,  1.0474e-02, -1.3317e-02,  2.9241e-03,\n",
       "                      -3.8671e-03, -3.8617e-02, -3.6636e-02, -1.7503e-02,  3.3514e-02,\n",
       "                      -2.0409e-02,  2.4748e-02,  6.1194e-03, -2.1033e-02,  5.9026e-03,\n",
       "                      -8.2350e-03,  1.6618e-02, -8.2303e-03,  2.4233e-03, -1.0251e-02,\n",
       "                       3.4610e-02,  4.7369e-03, -2.7018e-02,  3.1823e-02, -1.5098e-02,\n",
       "                       1.4174e-02, -1.4496e-02,  8.0955e-03, -2.0577e-02, -1.0571e-02,\n",
       "                       1.0256e-02, -8.4979e-03,  1.0801e-02, -2.3056e-02,  2.9382e-02,\n",
       "                      -2.4253e-02,  1.5563e-02, -1.7397e-02,  3.8230e-02, -1.1453e-02,\n",
       "                      -1.6174e-02,  3.1240e-02])),\n",
       "             ('bert.encoder.layer.5.attention.self.key.weight',\n",
       "              tensor([[ 0.0295,  0.0015,  0.0129,  ...,  0.0441, -0.0491, -0.0173],\n",
       "                      [ 0.0142,  0.0472,  0.0079,  ...,  0.0566, -0.0255, -0.0273],\n",
       "                      [ 0.0029,  0.0263, -0.0089,  ...,  0.0018, -0.0504,  0.0001],\n",
       "                      ...,\n",
       "                      [-0.0180,  0.0096,  0.0009,  ...,  0.0120, -0.0112,  0.0011],\n",
       "                      [-0.0215,  0.0497,  0.0044,  ..., -0.0019, -0.0167,  0.0052],\n",
       "                      [-0.0661, -0.0088,  0.0374,  ..., -0.0198, -0.0195,  0.0153]])),\n",
       "             ('bert.encoder.layer.5.attention.self.key.bias',\n",
       "              tensor([ 1.2006e-08,  2.5241e-08, -3.3545e-08,  7.9822e-08, -1.9912e-07,\n",
       "                       4.4925e-08,  7.8977e-09,  6.1361e-08, -3.5873e-08, -7.4048e-08,\n",
       "                       1.2332e-08,  1.5812e-07, -5.6591e-08, -1.5246e-07,  1.1140e-07,\n",
       "                       4.6276e-08, -6.2483e-08,  5.8808e-08, -5.1856e-08,  6.1014e-08,\n",
       "                      -4.5485e-08,  9.1607e-08,  5.5133e-08,  2.1135e-08, -1.4441e-07,\n",
       "                      -2.6967e-08,  2.7906e-08,  7.5913e-08, -3.4241e-08, -1.0583e-07,\n",
       "                       3.3792e-08, -6.8479e-08,  8.7946e-08, -5.5637e-08,  7.4945e-08,\n",
       "                      -1.3219e-07,  3.1163e-07, -8.7147e-08,  1.8036e-07, -1.5045e-07,\n",
       "                       6.7543e-09, -2.4805e-07,  8.0205e-08,  9.4452e-08, -1.5515e-07,\n",
       "                       2.8853e-08,  7.8555e-08, -2.6695e-07, -1.8442e-07, -2.1132e-07,\n",
       "                       7.5734e-09, -6.3870e-08,  1.6945e-07,  8.1975e-08,  1.2684e-07,\n",
       "                       2.5946e-07, -1.6534e-07, -4.5546e-09, -1.6192e-07,  1.6211e-07,\n",
       "                      -1.0048e-07, -2.2443e-07,  2.7358e-08, -3.0156e-07,  9.3673e-08,\n",
       "                      -2.8063e-07, -2.5775e-07, -1.8328e-07, -8.3110e-08,  8.3305e-08,\n",
       "                      -5.0150e-08, -2.4964e-07, -1.4820e-07,  1.1656e-08,  4.2000e-08,\n",
       "                       1.9009e-07, -1.5287e-07,  3.4844e-08,  7.3271e-08,  2.7569e-07,\n",
       "                      -1.3763e-07, -2.0791e-07, -3.4996e-08, -2.2061e-08, -6.5393e-08,\n",
       "                      -1.2811e-07, -5.0495e-08, -2.3926e-07, -8.3380e-08, -8.0721e-08,\n",
       "                      -2.5904e-07, -5.2988e-08,  1.3724e-08,  2.2764e-08,  6.2207e-08,\n",
       "                       1.0874e-07,  1.0240e-07,  4.0840e-08,  1.9044e-08,  3.2037e-08,\n",
       "                       8.9812e-08,  4.5897e-08,  2.2170e-09,  2.0096e-08,  1.4513e-08,\n",
       "                       9.0574e-08, -2.4052e-08,  5.1323e-08,  1.4324e-08, -1.2088e-07,\n",
       "                      -6.8254e-09, -6.3706e-08, -1.0095e-07, -2.8572e-08, -2.7493e-09,\n",
       "                       3.7464e-08, -7.4348e-08,  3.1817e-08, -4.4578e-08, -4.5678e-08,\n",
       "                       6.4301e-08, -1.0122e-07,  1.3191e-08, -6.0478e-09, -1.2291e-08,\n",
       "                      -1.1561e-07, -1.1314e-08,  3.1153e-08, -3.4439e-07, -3.8850e-07,\n",
       "                       3.5044e-07,  4.0649e-07,  5.2445e-07, -3.4138e-07, -4.5156e-08,\n",
       "                       3.4929e-08,  3.0332e-07,  9.2877e-08,  3.2233e-07,  8.7682e-08,\n",
       "                      -1.0626e-07,  2.2512e-07,  3.8664e-07,  4.1359e-07,  1.4209e-07,\n",
       "                      -3.1805e-07,  3.4395e-07,  1.1119e-07, -1.2649e-07, -2.1477e-07,\n",
       "                       3.2412e-08, -7.4194e-08, -4.0819e-07,  4.9064e-07, -9.3954e-08,\n",
       "                      -1.3383e-07,  9.8388e-08,  4.0967e-07,  2.5517e-07, -2.4923e-08,\n",
       "                      -1.3553e-07, -4.1146e-08, -1.1789e-07, -1.7162e-07,  4.9726e-08,\n",
       "                      -1.8985e-07, -8.8065e-08, -5.3889e-08,  1.5322e-07, -3.2165e-08,\n",
       "                       1.2691e-07,  7.9238e-09, -9.1979e-08,  2.4858e-07, -1.3370e-07,\n",
       "                       6.1797e-08, -1.6239e-07,  3.0581e-08, -1.1414e-07, -5.7340e-08,\n",
       "                      -8.6193e-08,  5.5417e-08, -7.0124e-08,  4.6386e-08,  2.4687e-07,\n",
       "                       1.3516e-09,  1.4093e-07, -1.5883e-07,  1.9151e-07,  6.3447e-08,\n",
       "                      -8.6038e-08,  1.6833e-07])),\n",
       "             ('bert.encoder.layer.5.attention.self.value.weight',\n",
       "              tensor([[ 0.0011,  0.0171, -0.0211,  ..., -0.0204, -0.0139,  0.0154],\n",
       "                      [-0.0162, -0.0194,  0.0173,  ...,  0.0367,  0.0206,  0.0060],\n",
       "                      [ 0.0194,  0.0163, -0.0315,  ...,  0.0019, -0.0287,  0.0048],\n",
       "                      ...,\n",
       "                      [-0.0149,  0.0203, -0.0274,  ...,  0.0011,  0.0138,  0.0446],\n",
       "                      [-0.0054,  0.0029, -0.0198,  ...,  0.0206, -0.0287, -0.0098],\n",
       "                      [-0.0352,  0.0075,  0.0179,  ..., -0.0097,  0.0031,  0.0406]])),\n",
       "             ('bert.encoder.layer.5.attention.self.value.bias',\n",
       "              tensor([-0.0020,  0.0091, -0.0028, -0.0049,  0.0015, -0.0059,  0.0167,  0.0094,\n",
       "                      -0.0092,  0.0012, -0.0051,  0.0023, -0.0134, -0.0016,  0.0074, -0.0039,\n",
       "                      -0.0119,  0.0070,  0.0065,  0.0083, -0.0118, -0.0134, -0.0260, -0.0024,\n",
       "                       0.0069, -0.0159,  0.0024,  0.0033,  0.0195, -0.0091, -0.0056, -0.0010,\n",
       "                       0.0031, -0.0107, -0.0051, -0.0026,  0.0034, -0.0132, -0.0063,  0.0041,\n",
       "                      -0.0067, -0.0103,  0.0056, -0.0027, -0.0032,  0.0112, -0.0104,  0.0032,\n",
       "                       0.0015,  0.0075,  0.0015, -0.0008,  0.0035,  0.0090,  0.0089,  0.0072,\n",
       "                       0.0014,  0.0031, -0.0027, -0.0009, -0.0125,  0.0107, -0.0089,  0.0046,\n",
       "                      -0.0036, -0.0077,  0.0011,  0.0171,  0.0029, -0.0064, -0.0044,  0.0037,\n",
       "                      -0.0014,  0.0069, -0.0066,  0.0042, -0.0133, -0.0019,  0.0082,  0.0018,\n",
       "                      -0.0005, -0.0036,  0.0045,  0.0060,  0.0084,  0.0012,  0.0170, -0.0126,\n",
       "                      -0.0013, -0.0085,  0.0004, -0.0002,  0.0005, -0.0167, -0.0070,  0.0115,\n",
       "                       0.0023, -0.0049, -0.0031,  0.0037,  0.0038,  0.0024, -0.0039, -0.0044,\n",
       "                      -0.0008, -0.0013,  0.0004,  0.0004,  0.0066,  0.0026,  0.0064, -0.0048,\n",
       "                       0.0010,  0.0108, -0.0047,  0.0053, -0.0009, -0.0008,  0.0036,  0.0034,\n",
       "                      -0.0047,  0.0048, -0.0074,  0.0009, -0.0040, -0.0070,  0.0097, -0.0029,\n",
       "                      -0.0057,  0.0044, -0.0049,  0.0015,  0.0047, -0.0026,  0.0002,  0.0086,\n",
       "                      -0.0012,  0.0044, -0.0005,  0.0031, -0.0003, -0.0033,  0.0057, -0.0029,\n",
       "                      -0.0007,  0.0044, -0.0035,  0.0014,  0.0016, -0.0032,  0.0024,  0.0043,\n",
       "                       0.0016, -0.0064,  0.0047, -0.0046, -0.0019,  0.0015,  0.0056, -0.0021,\n",
       "                      -0.0039,  0.0007,  0.0160, -0.0029, -0.0018,  0.0064,  0.0035,  0.0099,\n",
       "                      -0.0017, -0.0036,  0.0065, -0.0028, -0.0035,  0.0018,  0.0041,  0.0028,\n",
       "                      -0.0014,  0.0008,  0.0063,  0.0088, -0.0015,  0.0016,  0.0012,  0.0075,\n",
       "                       0.0064,  0.0058,  0.0011,  0.0037,  0.0028, -0.0053,  0.0089,  0.0012])),\n",
       "             ('bert.encoder.layer.5.attention.output.dense.weight',\n",
       "              tensor([[-0.0150,  0.0164,  0.0387,  ..., -0.0204, -0.0086, -0.0323],\n",
       "                      [-0.0323,  0.0266, -0.0034,  ...,  0.0132,  0.0281, -0.0106],\n",
       "                      [ 0.0053,  0.0077, -0.0464,  ..., -0.0216,  0.0003, -0.0014],\n",
       "                      ...,\n",
       "                      [-0.0167,  0.0505, -0.0211,  ..., -0.0052,  0.0486, -0.0388],\n",
       "                      [ 0.0003, -0.0005, -0.0230,  ...,  0.0261,  0.0111, -0.0186],\n",
       "                      [ 0.0070, -0.0367, -0.0059,  ..., -0.0090, -0.0141, -0.0140]])),\n",
       "             ('bert.encoder.layer.5.attention.output.dense.bias',\n",
       "              tensor([ 2.3620e-03, -9.2013e-04,  7.8316e-03,  7.0336e-03,  3.4468e-03,\n",
       "                      -1.1076e-02, -5.6248e-03, -9.3464e-03,  1.1208e-03, -5.0799e-03,\n",
       "                       1.5829e-02,  8.4008e-03, -7.8968e-03,  2.9275e-03,  9.3285e-03,\n",
       "                      -2.7620e-03,  4.7509e-03,  1.4375e-02, -1.7387e-03, -5.0032e-03,\n",
       "                      -9.4200e-04, -5.1988e-03,  4.5425e-03,  1.9308e-03, -2.5991e-03,\n",
       "                      -8.1004e-03,  3.5463e-03,  1.5324e-02,  1.6454e-03,  1.2003e-02,\n",
       "                       1.4920e-02, -1.6807e-03, -6.2587e-03, -4.4869e-03, -1.5168e-02,\n",
       "                      -7.2178e-03, -6.7120e-03, -5.8903e-04, -1.0992e-03, -5.8231e-03,\n",
       "                       2.7375e-03, -7.1063e-03, -6.8010e-03,  9.9190e-03, -8.6203e-04,\n",
       "                      -2.8679e-03,  3.1816e-03,  1.5748e-03, -1.5275e-03, -7.2547e-03,\n",
       "                      -2.9346e-03,  6.4598e-03,  1.1555e-02, -1.4463e-03,  3.2239e-03,\n",
       "                      -8.2558e-03, -7.4158e-03, -2.2782e-03, -3.1370e-03, -7.0749e-03,\n",
       "                      -1.1013e-03, -7.3459e-03,  7.1397e-03,  2.9453e-03, -5.6982e-03,\n",
       "                       3.9639e-03, -5.0677e-03, -4.2680e-03, -4.4581e-03, -6.1207e-04,\n",
       "                       3.0330e-03,  2.4523e-03, -1.1345e-03,  9.1641e-03, -2.3919e-04,\n",
       "                       7.4715e-03,  5.7747e-06,  3.6630e-03, -2.9401e-03, -2.8878e-03,\n",
       "                      -6.7858e-03,  3.7066e-03, -1.0983e-03, -6.7379e-04, -2.0746e-03,\n",
       "                      -1.9947e-03, -8.6795e-03, -3.4143e-03,  7.5320e-03,  6.9089e-03,\n",
       "                      -4.7674e-03,  4.4176e-05, -1.3187e-03,  6.0885e-03, -3.5309e-03,\n",
       "                       5.7967e-03, -3.0752e-03, -8.6630e-03, -2.9652e-03,  5.6349e-03,\n",
       "                      -6.2062e-03,  8.1930e-03, -7.3563e-03,  1.4029e-03, -5.4550e-03,\n",
       "                       2.2393e-03, -1.0299e-02, -2.9689e-03, -3.3644e-04,  2.6160e-03,\n",
       "                       2.7252e-03, -5.1921e-03,  2.2080e-03, -9.1181e-03,  3.3372e-02,\n",
       "                       7.1705e-03, -2.0669e-03, -2.0250e-03,  8.9720e-03, -1.5368e-02,\n",
       "                       4.1776e-03, -1.1914e-02,  3.5285e-03, -2.1411e-03,  9.8515e-03,\n",
       "                       3.3755e-03,  1.0283e-02, -4.9750e-03,  1.2095e-02, -4.2389e-04,\n",
       "                      -3.4900e-04, -9.6948e-04,  7.0151e-03, -7.4600e-03, -2.5372e-04,\n",
       "                       1.2096e-02,  4.3517e-03,  5.0238e-03,  3.1284e-03,  1.2150e-02,\n",
       "                      -5.0814e-03,  3.3819e-03,  3.2718e-03, -1.8585e-03,  5.5931e-04,\n",
       "                      -3.2477e-03, -2.2306e-03,  2.1107e-03, -7.4933e-03, -4.4842e-03,\n",
       "                      -1.2794e-03, -1.2867e-02,  1.4839e-03, -5.5361e-03, -1.0307e-02,\n",
       "                      -6.9929e-03, -1.6685e-02, -2.0000e-03,  5.1857e-03,  5.4580e-03,\n",
       "                      -2.6606e-03,  1.4681e-02,  3.7126e-03, -1.3453e-03, -2.2767e-05,\n",
       "                       3.5518e-03, -8.4240e-04,  5.9214e-03,  7.1336e-03, -1.8039e-03,\n",
       "                       7.2890e-03, -8.2689e-03, -4.3390e-03, -8.3141e-03,  4.2701e-03,\n",
       "                       6.2903e-03, -3.8902e-03, -8.0431e-03, -4.1857e-03, -4.3616e-03,\n",
       "                      -5.7164e-03, -3.4901e-03, -5.5835e-03, -1.3018e-03,  4.4708e-03,\n",
       "                      -1.8924e-03, -3.4705e-03, -4.1363e-03,  2.0563e-03, -1.7320e-03,\n",
       "                       7.3386e-04,  2.4302e-03])),\n",
       "             ('bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       "              tensor([1.0307, 1.0193, 0.9933, 1.0127, 1.0104, 0.9842, 1.0170, 1.0182, 0.9930,\n",
       "                      1.0168, 0.9684, 1.0274, 1.0215, 1.0146, 1.0103, 1.0066, 1.0169, 1.0050,\n",
       "                      0.9894, 1.0338, 1.0165, 1.0367, 0.9910, 1.0126, 0.9943, 1.0224, 1.0358,\n",
       "                      0.9792, 0.9593, 1.0382, 1.0290, 1.0150, 1.0150, 1.0216, 1.0260, 0.9966,\n",
       "                      1.0031, 0.9451, 1.0153, 1.0229, 1.0110, 1.0349, 1.0160, 1.0178, 1.0290,\n",
       "                      1.0108, 1.0075, 1.0323, 1.0077, 1.0233, 1.0311, 0.9456, 0.9265, 1.0166,\n",
       "                      1.0282, 1.0141, 1.0122, 1.0193, 1.0284, 1.0044, 1.0074, 0.9963, 1.0240,\n",
       "                      0.9976, 1.0224, 1.0073, 1.0237, 0.9015, 1.0197, 1.0020, 1.0164, 0.9848,\n",
       "                      1.0221, 1.0482, 1.0085, 0.9926, 1.0234, 1.0164, 1.0485, 1.0140, 1.0079,\n",
       "                      1.0090, 1.0163, 1.0337, 1.0466, 1.0117, 1.0485, 1.0135, 1.0257, 1.0193,\n",
       "                      1.0096, 0.9762, 1.0131, 1.0042, 0.9902, 0.9312, 1.0142, 0.9601, 1.0092,\n",
       "                      1.0188, 1.0106, 1.0273, 1.0174, 0.9986, 1.0203, 1.0007, 1.0238, 0.9924,\n",
       "                      1.0328, 1.0247, 0.9805, 1.0192, 1.0326, 0.9882, 0.8483, 1.0231, 0.9958,\n",
       "                      1.0033, 1.0189, 1.0352, 0.9867, 0.9440, 1.0134, 1.0017, 1.0254, 1.0335,\n",
       "                      1.0296, 1.0158, 1.0312, 1.0219, 0.9581, 0.9668, 1.0078, 1.0035, 1.0379,\n",
       "                      1.0101, 0.9774, 1.0193, 0.9792, 1.0214, 1.0104, 1.0168, 1.0065, 1.0240,\n",
       "                      0.9863, 1.0008, 0.9788, 1.0157, 1.0134, 1.0374, 1.0070, 0.9346, 1.0181,\n",
       "                      1.0237, 0.9921, 1.0275, 1.0325, 0.9957, 1.0065, 0.9425, 1.0070, 0.9995,\n",
       "                      1.0024, 1.0224, 0.9932, 1.0214, 1.0173, 1.0237, 1.0194, 1.0078, 0.9998,\n",
       "                      1.0230, 1.0117, 1.0262, 1.0240, 1.0095, 1.0159, 0.9590, 1.0181, 1.0093,\n",
       "                      0.8959, 1.0279, 1.0104, 1.0297, 1.0152, 1.0287, 1.0096, 1.0155, 0.9860,\n",
       "                      0.9806, 0.9944, 1.0261])),\n",
       "             ('bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       "              tensor([-9.0228e-04,  9.3638e-03,  1.3245e-02,  1.5972e-02,  4.0093e-03,\n",
       "                      -1.3660e-02, -1.2218e-02, -1.7052e-02,  6.8358e-03, -5.3018e-03,\n",
       "                       3.2003e-02,  1.9273e-02, -1.2022e-02,  1.7256e-02,  1.5918e-02,\n",
       "                      -4.8231e-03,  1.3234e-03,  2.5483e-02,  4.0200e-04,  5.9878e-06,\n",
       "                       1.0173e-02, -8.2907e-03, -3.9678e-03,  6.3389e-03, -4.8388e-03,\n",
       "                      -1.1864e-02,  1.0226e-02,  2.3821e-02,  6.1897e-03,  1.6933e-02,\n",
       "                       2.5713e-02,  1.3161e-03, -1.1628e-02, -1.5094e-02, -1.5479e-02,\n",
       "                      -1.4948e-02, -1.5301e-02, -5.1038e-03,  1.5259e-03, -7.1522e-03,\n",
       "                      -1.6284e-03, -1.4919e-02, -1.0318e-02,  1.9679e-02,  4.5474e-03,\n",
       "                      -7.9936e-03,  3.1927e-03, -6.8059e-04, -3.8435e-03, -1.1587e-03,\n",
       "                      -1.1498e-02,  4.7050e-03,  1.0818e-02,  3.6911e-03,  1.0602e-02,\n",
       "                      -1.6380e-02, -4.9227e-03, -7.0821e-03,  1.1681e-03,  1.2363e-03,\n",
       "                      -8.0847e-03, -1.1275e-02,  1.4247e-02, -9.5656e-03,  4.0412e-04,\n",
       "                       7.8784e-04, -1.3085e-02, -4.4901e-04,  2.5097e-03, -1.1318e-03,\n",
       "                       9.7463e-03,  8.0328e-05, -6.0496e-03,  2.1048e-02, -1.5262e-03,\n",
       "                       1.7927e-02,  9.4427e-04,  7.8530e-03, -4.2749e-04, -4.8071e-04,\n",
       "                      -2.7974e-03,  1.6565e-02, -1.1711e-03, -1.9477e-02,  5.7297e-03,\n",
       "                      -9.4112e-03, -1.5475e-02, -1.1852e-02,  1.8831e-02,  1.5273e-04,\n",
       "                      -1.7901e-02,  1.3122e-02,  2.1498e-03,  5.9095e-03, -8.7104e-03,\n",
       "                       1.8113e-02,  2.3131e-03, -1.5851e-02,  2.4990e-03, -1.8839e-03,\n",
       "                      -9.6352e-03,  1.5802e-02, -6.6235e-03, -1.1438e-03, -2.1586e-03,\n",
       "                       6.5870e-03, -1.6921e-02,  4.7070e-03, -1.3463e-02,  8.0564e-03,\n",
       "                      -1.5985e-03, -9.7002e-04,  8.7599e-03, -2.2147e-02,  2.0231e-02,\n",
       "                       2.1737e-02,  3.2116e-04, -5.6263e-03,  1.5121e-02, -1.7845e-02,\n",
       "                       3.2609e-03, -1.9141e-02,  1.7695e-03, -2.1824e-03,  4.6848e-03,\n",
       "                       1.9155e-03,  2.2262e-02,  1.7538e-03,  2.6938e-02, -5.7051e-03,\n",
       "                      -2.4868e-03,  2.6619e-03,  1.0326e-02, -8.6372e-03, -1.6737e-03,\n",
       "                       1.6859e-02,  7.5914e-03,  1.1053e-02,  2.8560e-03,  2.4831e-02,\n",
       "                      -9.1229e-03,  4.1906e-03,  2.7934e-03, -2.4375e-03, -4.6360e-03,\n",
       "                      -1.3287e-02,  6.8240e-04,  8.7911e-03,  3.4819e-04, -9.3258e-03,\n",
       "                       4.6703e-03, -6.3410e-03, -9.3196e-03, -8.3338e-03, -1.1915e-02,\n",
       "                      -6.0797e-03, -2.3925e-02, -4.0534e-03,  1.1650e-02,  8.6707e-04,\n",
       "                      -1.1415e-02,  1.0940e-02,  9.1870e-04, -8.8499e-03,  4.8776e-03,\n",
       "                       1.2519e-02, -9.4445e-04,  9.8107e-03,  1.8447e-02, -9.6272e-03,\n",
       "                      -4.1099e-03, -2.1281e-02, -2.8119e-03, -6.8706e-03,  1.2127e-02,\n",
       "                       4.3606e-04, -1.3600e-02, -1.5218e-02, -1.4313e-03, -1.1615e-03,\n",
       "                       5.4910e-03, -1.0482e-02, -1.4420e-02, -1.2498e-02,  6.5442e-03,\n",
       "                       9.5666e-03, -1.1860e-02,  1.1821e-03,  4.2762e-03,  4.8806e-03,\n",
       "                      -7.7769e-03, -2.2695e-03])),\n",
       "             ('bert.encoder.layer.5.intermediate.dense.weight',\n",
       "              tensor([[ 0.0428, -0.0614,  0.0382,  ..., -0.0239,  0.0258, -0.0246],\n",
       "                      [-0.0100, -0.0206,  0.0076,  ...,  0.0151, -0.0060,  0.0272],\n",
       "                      [-0.0429, -0.0279,  0.0305,  ...,  0.0528,  0.0243, -0.0103],\n",
       "                      ...,\n",
       "                      [-0.0164, -0.0280, -0.0183,  ...,  0.0092,  0.0148,  0.0234],\n",
       "                      [ 0.0031, -0.0074,  0.0456,  ..., -0.0269, -0.0302,  0.0232],\n",
       "                      [-0.0450,  0.0091, -0.0363,  ..., -0.0294,  0.0324, -0.0399]])),\n",
       "             ('bert.encoder.layer.5.intermediate.dense.bias',\n",
       "              tensor([-4.6155e-03,  3.4288e-02,  1.4150e-02, -3.0023e-03, -1.3184e-03,\n",
       "                      -1.4228e-02, -1.2283e-02, -1.6272e-02, -6.0292e-03,  3.2425e-02,\n",
       "                       1.5867e-02, -1.3036e-02,  3.1609e-02, -1.3658e-02,  1.5289e-02,\n",
       "                      -2.7055e-02, -1.1539e-02, -1.3435e-02, -1.8962e-02,  1.1888e-02,\n",
       "                       3.0102e-02,  1.8275e-02, -9.5526e-03, -9.3743e-03, -2.7886e-03,\n",
       "                       1.4261e-02, -2.4983e-02, -2.3259e-02,  1.5399e-02, -7.3491e-03,\n",
       "                      -3.3216e-02,  2.2487e-02,  2.2815e-02, -1.4929e-02, -2.1504e-02,\n",
       "                      -1.7841e-02, -1.6864e-02,  1.6730e-03, -8.5055e-03,  1.1526e-02,\n",
       "                       7.2859e-03, -1.8329e-02,  2.9620e-02,  1.5131e-02, -9.2016e-03,\n",
       "                       2.0629e-02,  2.9782e-02, -6.1845e-03,  1.6909e-02,  1.9501e-02,\n",
       "                       1.8890e-02, -1.6884e-02, -2.1053e-02, -1.1261e-02, -2.6031e-02,\n",
       "                      -8.0745e-03,  2.4845e-02,  1.6497e-02,  1.9134e-02, -2.6175e-03,\n",
       "                       1.9376e-02,  1.6449e-02, -8.5098e-05, -2.6176e-02])),\n",
       "             ('bert.encoder.layer.5.output.dense.weight',\n",
       "              tensor([[ 1.5655e-02,  2.3280e-03, -4.5653e-02,  ..., -2.6772e-03,\n",
       "                       -4.6612e-03, -2.9817e-02],\n",
       "                      [ 5.2960e-02, -4.5857e-03,  2.3095e-02,  ..., -3.3756e-02,\n",
       "                        1.5616e-02, -2.8728e-02],\n",
       "                      [ 2.2073e-02, -8.7330e-03, -2.3445e-02,  ...,  4.3414e-02,\n",
       "                        1.8809e-02, -2.3539e-02],\n",
       "                      ...,\n",
       "                      [-2.2391e-03,  4.6266e-02,  3.0158e-02,  ...,  5.0692e-05,\n",
       "                       -6.8371e-03,  8.5381e-03],\n",
       "                      [-5.4946e-02,  1.2480e-02,  2.3095e-02,  ..., -3.0342e-02,\n",
       "                       -1.2586e-02,  1.1635e-02],\n",
       "                      [ 3.1851e-03, -1.9745e-02,  2.5970e-02,  ..., -3.4060e-02,\n",
       "                       -2.5241e-02, -2.9841e-02]])),\n",
       "             ('bert.encoder.layer.5.output.dense.bias',\n",
       "              tensor([-0.0024,  0.0081,  0.0127,  0.0137, -0.0008, -0.0025, -0.0134, -0.0114,\n",
       "                       0.0002, -0.0014,  0.0206,  0.0137, -0.0104,  0.0099,  0.0121, -0.0052,\n",
       "                      -0.0042,  0.0197, -0.0045,  0.0055,  0.0117, -0.0051, -0.0041,  0.0092,\n",
       "                       0.0011, -0.0070,  0.0085,  0.0261, -0.0048,  0.0087,  0.0152,  0.0019,\n",
       "                      -0.0113, -0.0084, -0.0125, -0.0082, -0.0114, -0.0070,  0.0038, -0.0018,\n",
       "                      -0.0021, -0.0056, -0.0002,  0.0152,  0.0049, -0.0082,  0.0045,  0.0006,\n",
       "                      -0.0013,  0.0008, -0.0105, -0.0037,  0.0048,  0.0029,  0.0068, -0.0060,\n",
       "                       0.0013, -0.0045,  0.0029, -0.0040, -0.0069, -0.0099,  0.0089, -0.0068,\n",
       "                      -0.0027,  0.0031, -0.0062,  0.0061,  0.0006,  0.0064,  0.0018, -0.0068,\n",
       "                       0.0008,  0.0174,  0.0037,  0.0165, -0.0004,  0.0052,  0.0029, -0.0021,\n",
       "                      -0.0024,  0.0126, -0.0007, -0.0129, -0.0013, -0.0009, -0.0113, -0.0076,\n",
       "                       0.0147,  0.0020, -0.0149,  0.0118, -0.0016,  0.0009, -0.0110,  0.0135,\n",
       "                      -0.0056, -0.0092,  0.0025, -0.0029, -0.0022,  0.0125, -0.0088,  0.0036,\n",
       "                       0.0007,  0.0055, -0.0139,  0.0094, -0.0089,  0.0078, -0.0044, -0.0001,\n",
       "                       0.0022, -0.0084,  0.0101,  0.0139, -0.0046, -0.0050,  0.0110, -0.0145,\n",
       "                       0.0050, -0.0165,  0.0017, -0.0068,  0.0036, -0.0057,  0.0118,  0.0078,\n",
       "                       0.0197, -0.0026,  0.0020,  0.0023,  0.0048, -0.0055,  0.0014,  0.0130,\n",
       "                      -0.0029,  0.0069,  0.0013,  0.0224, -0.0048,  0.0007,  0.0044, -0.0068,\n",
       "                      -0.0103, -0.0051,  0.0048,  0.0076,  0.0013, -0.0018, -0.0006,  0.0006,\n",
       "                      -0.0075, -0.0029, -0.0120, -0.0119, -0.0207, -0.0015,  0.0134, -0.0023,\n",
       "                      -0.0089,  0.0024,  0.0045, -0.0008, -0.0046,  0.0087, -0.0019,  0.0058,\n",
       "                       0.0103, -0.0048, -0.0082, -0.0108,  0.0012, -0.0069,  0.0082,  0.0012,\n",
       "                      -0.0072, -0.0206, -0.0040,  0.0062,  0.0042, -0.0061, -0.0093, -0.0111,\n",
       "                       0.0059,  0.0052, -0.0055,  0.0005,  0.0158,  0.0057, -0.0087,  0.0023])),\n",
       "             ('bert.encoder.layer.5.output.LayerNorm.weight',\n",
       "              tensor([1.0148, 1.0134, 0.9881, 1.0019, 1.0056, 0.9847, 1.0090, 1.0046, 0.9924,\n",
       "                      1.0079, 0.9599, 1.0272, 1.0181, 1.0046, 1.0020, 0.9991, 1.0090, 1.0037,\n",
       "                      0.9845, 1.0341, 1.0015, 1.0322, 0.9893, 1.0035, 0.9897, 1.0199, 1.0339,\n",
       "                      0.9725, 0.9525, 1.0228, 1.0234, 1.0067, 1.0069, 1.0216, 1.0189, 0.9883,\n",
       "                      0.9887, 0.9403, 1.0120, 1.0136, 1.0068, 1.0268, 1.0030, 1.0063, 1.0201,\n",
       "                      1.0021, 1.0015, 1.0191, 0.9994, 1.0175, 1.0150, 0.9408, 0.9219, 1.0094,\n",
       "                      1.0176, 1.0092, 1.0114, 1.0135, 1.0218, 0.9951, 1.0013, 0.9825, 1.0086,\n",
       "                      0.9900, 1.0140, 0.9968, 1.0208, 0.8932, 1.0120, 1.0000, 1.0028, 0.9810,\n",
       "                      1.0156, 1.0284, 0.9999, 0.9869, 1.0132, 1.0180, 1.0411, 1.0089, 1.0040,\n",
       "                      0.9931, 1.0037, 1.0190, 1.0326, 1.0077, 1.0411, 0.9955, 1.0172, 1.0161,\n",
       "                      0.9983, 0.9650, 1.0059, 0.9937, 0.9871, 0.9205, 1.0064, 0.9586, 0.9989,\n",
       "                      1.0160, 1.0038, 1.0195, 1.0152, 0.9903, 1.0131, 0.9953, 1.0180, 0.9906,\n",
       "                      1.0177, 1.0152, 0.9769, 1.0142, 1.0283, 0.9793, 0.8389, 1.0085, 0.9909,\n",
       "                      1.0100, 1.0094, 1.0253, 0.9765, 0.9380, 1.0067, 0.9990, 1.0191, 1.0282,\n",
       "                      1.0230, 1.0104, 1.0148, 1.0146, 0.9508, 0.9622, 0.9952, 0.9918, 1.0278,\n",
       "                      1.0075, 0.9752, 1.0058, 0.9777, 1.0122, 0.9986, 1.0146, 0.9990, 1.0174,\n",
       "                      0.9771, 0.9926, 0.9719, 1.0070, 1.0074, 1.0372, 1.0093, 0.9280, 0.9980,\n",
       "                      1.0188, 0.9911, 1.0169, 1.0174, 0.9938, 0.9996, 0.9340, 1.0020, 0.9969,\n",
       "                      1.0008, 1.0133, 0.9803, 1.0209, 1.0109, 1.0131, 1.0063, 1.0028, 0.9914,\n",
       "                      1.0176, 1.0043, 1.0227, 1.0147, 1.0116, 1.0059, 0.9419, 1.0064, 1.0035,\n",
       "                      0.8873, 1.0197, 0.9980, 1.0235, 1.0067, 1.0230, 0.9987, 1.0031, 0.9811,\n",
       "                      0.9709, 0.9868, 1.0102])),\n",
       "             ('bert.encoder.layer.5.output.LayerNorm.bias',\n",
       "              tensor([-3.5064e-03,  5.0593e-03,  1.0105e-02,  1.1171e-02,  5.7431e-03,\n",
       "                      -1.2978e-02, -1.4126e-02, -8.6369e-03,  7.5646e-03, -2.2991e-03,\n",
       "                       2.7482e-02,  1.4865e-02, -1.3521e-02,  1.7406e-02,  1.3019e-02,\n",
       "                      -6.0578e-04,  5.0781e-04,  2.2746e-02,  3.6414e-03,  8.1362e-04,\n",
       "                      -5.0253e-04, -4.2271e-03, -7.3829e-04,  1.1324e-03, -2.0299e-03,\n",
       "                      -1.1486e-02,  6.0449e-03,  1.6548e-02,  3.1432e-03,  1.3834e-02,\n",
       "                       2.4349e-02,  4.7871e-03, -9.2217e-03, -8.1972e-03, -1.3881e-02,\n",
       "                      -1.0566e-02, -9.3756e-03, -1.6019e-03, -3.6244e-03, -5.3568e-03,\n",
       "                       1.6140e-03, -1.4090e-02, -4.9957e-03,  1.5628e-02,  3.2869e-03,\n",
       "                      -3.9877e-03,  8.9488e-04,  3.0841e-03, -2.5541e-03, -7.9578e-04,\n",
       "                      -9.9349e-03,  8.0108e-03,  8.1441e-03, -6.7980e-04,  7.7107e-03,\n",
       "                      -1.4959e-02, -2.1993e-03, -7.5221e-03,  3.4918e-04, -3.1042e-03,\n",
       "                      -5.1831e-03, -7.2843e-03,  9.6634e-03, -5.7721e-03, -1.7573e-03,\n",
       "                      -2.7893e-04, -1.0349e-02, -2.6972e-03, -1.0776e-03, -4.8977e-03,\n",
       "                       5.3968e-03, -2.4185e-03, -9.6378e-03,  1.1767e-02, -4.4777e-04,\n",
       "                       1.2461e-02, -1.7693e-03,  8.8339e-03, -8.3559e-04, -1.8928e-03,\n",
       "                      -5.4076e-03,  1.4903e-02,  4.5380e-03, -1.3025e-02,  1.0760e-03,\n",
       "                      -6.8456e-03, -1.6779e-02, -3.2884e-03,  1.5463e-02,  4.4410e-04,\n",
       "                      -1.3256e-02,  3.4407e-03, -2.2221e-03,  3.5848e-03, -5.5888e-03,\n",
       "                       6.1890e-03, -1.7945e-03, -1.2664e-02,  2.4032e-03, -2.5602e-03,\n",
       "                      -6.7482e-03,  1.0534e-02, -2.4020e-03, -4.0260e-03, -3.5717e-03,\n",
       "                      -4.0169e-04, -1.5532e-02,  5.4057e-03, -1.2332e-02,  4.5198e-03,\n",
       "                      -2.3196e-03, -3.7155e-03,  5.2492e-03, -1.8219e-02,  2.8066e-02,\n",
       "                       1.4648e-02, -6.8002e-03, -2.5978e-03,  8.4137e-03, -1.6622e-02,\n",
       "                       8.5141e-03, -2.0638e-02, -4.3309e-03,  2.0213e-04,  2.6921e-03,\n",
       "                       2.1690e-03,  1.5065e-02, -3.4207e-03,  1.6688e-02, -5.4012e-03,\n",
       "                      -7.1793e-03,  4.1142e-03,  1.1403e-02, -8.5739e-03,  8.8576e-04,\n",
       "                       1.5333e-02,  4.8612e-03,  3.1955e-03, -8.9311e-04,  1.8853e-02,\n",
       "                      -5.0239e-03,  6.1536e-03,  4.7762e-03, -1.3490e-03,  3.0281e-04,\n",
       "                      -9.6360e-03,  1.0896e-03,  9.4090e-03,  1.2997e-03, -4.9884e-03,\n",
       "                       7.3778e-03, -1.2523e-02, -4.7256e-04, -2.0688e-03, -1.5546e-02,\n",
       "                      -7.7338e-03, -1.5389e-02, -9.7882e-03,  1.0007e-02,  4.7575e-03,\n",
       "                      -9.2471e-03,  9.2051e-03,  3.1379e-03, -5.6569e-03,  3.9771e-03,\n",
       "                       1.1279e-02, -2.4232e-03,  6.4256e-03,  1.0378e-02, -1.0884e-02,\n",
       "                       6.3331e-05, -1.9899e-02, -1.1023e-03, -3.7551e-03,  1.2739e-02,\n",
       "                      -4.8869e-03, -7.5361e-03, -8.3758e-03, -4.1507e-03,  5.0670e-05,\n",
       "                      -7.9912e-03, -7.7760e-03, -1.0840e-02, -9.0178e-03,  6.5914e-03,\n",
       "                       7.0370e-03, -6.4722e-03, -2.6799e-03,  3.2961e-03, -3.4687e-03,\n",
       "                      -2.8383e-03, -3.6958e-04])),\n",
       "             ('bert.pooler.dense.weight',\n",
       "              tensor([[ 0.0196,  0.0208, -0.0182,  ...,  0.0041, -0.0018,  0.0113],\n",
       "                      [-0.0268, -0.0331,  0.0017,  ...,  0.0227,  0.0427,  0.0503],\n",
       "                      [ 0.0172,  0.0089, -0.0352,  ..., -0.0110,  0.0135,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0061, -0.0231, -0.0179,  ..., -0.0439,  0.0483, -0.0164],\n",
       "                      [-0.0101, -0.0165, -0.0027,  ..., -0.0047,  0.0070,  0.0298],\n",
       "                      [ 0.0044,  0.0035, -0.0087,  ...,  0.0291, -0.0252, -0.0067]])),\n",
       "             ('bert.pooler.dense.bias',\n",
       "              tensor([ 5.8375e-03,  2.8746e-03, -6.2476e-03, -1.1664e-02, -2.5100e-03,\n",
       "                       1.2292e-03,  2.2210e-03,  7.0760e-03,  2.9990e-03,  8.8105e-03,\n",
       "                       4.1650e-03, -9.8019e-03, -8.4401e-03,  2.3563e-03, -2.7696e-04,\n",
       "                      -1.1673e-03,  8.4968e-03,  6.9579e-03,  7.0998e-03, -4.3943e-03,\n",
       "                       1.0980e-02, -1.7808e-03, -4.5994e-03,  6.7036e-03, -1.4195e-02,\n",
       "                       1.7770e-05, -4.9811e-03, -4.5800e-03,  1.1116e-02, -1.0104e-02,\n",
       "                      -5.0014e-04, -4.9827e-03, -1.9769e-03,  9.9974e-04, -1.3160e-03,\n",
       "                       2.0984e-03,  7.2314e-05,  2.2490e-03,  9.2767e-04, -3.2817e-03,\n",
       "                      -9.1148e-05,  3.8108e-04,  1.1911e-02,  7.4571e-03,  1.0682e-02,\n",
       "                      -1.2052e-02,  5.2199e-03, -1.0537e-02,  3.5229e-03,  1.4544e-03,\n",
       "                       5.5221e-03, -1.9194e-03,  7.5294e-03, -1.5210e-03, -5.8734e-03,\n",
       "                       5.2985e-03,  2.1591e-04, -3.2165e-03,  8.3366e-03, -1.0070e-02,\n",
       "                      -2.3711e-03, -1.9816e-03, -9.9825e-04,  2.1027e-03,  6.5407e-03,\n",
       "                       9.1098e-04, -9.2319e-03,  8.0912e-03,  8.1528e-03,  8.8372e-03,\n",
       "                       1.5232e-03,  1.0593e-02,  1.1115e-02,  8.6742e-03, -8.7393e-03,\n",
       "                      -8.1157e-03, -6.9797e-03, -3.9819e-04, -3.8310e-04,  1.1698e-02,\n",
       "                      -8.8463e-03,  7.8386e-03,  3.5181e-03,  8.3776e-03,  1.0638e-02,\n",
       "                       4.0390e-03,  8.3744e-03,  7.0782e-03, -1.6967e-03, -9.4236e-03,\n",
       "                      -3.5241e-03, -6.6552e-03, -5.7229e-03, -8.5699e-03, -2.1767e-03,\n",
       "                       7.3044e-03, -4.4493e-04, -1.0548e-02, -1.0382e-02,  7.1521e-03,\n",
       "                       1.1375e-02,  1.8898e-03,  6.2280e-03,  1.0896e-02, -2.7497e-03,\n",
       "                       6.9059e-03, -8.1004e-03,  3.7372e-03, -6.6883e-04,  7.6206e-03,\n",
       "                       1.0687e-02, -8.2146e-03,  4.2654e-03, -6.8405e-03,  9.1744e-03,\n",
       "                      -3.2075e-04,  1.0795e-02,  1.7196e-04,  6.1672e-03, -5.0441e-03,\n",
       "                      -5.0114e-03, -7.8530e-03, -7.3984e-03,  4.7339e-03, -9.7846e-03,\n",
       "                       7.1884e-03, -7.5079e-03,  5.1296e-04, -2.8740e-03,  1.0814e-02,\n",
       "                       2.8907e-03, -9.2711e-03, -3.2895e-03,  7.8665e-03,  4.4011e-03,\n",
       "                      -1.3838e-03,  1.2436e-03,  6.0072e-03,  7.7435e-03,  7.1162e-03,\n",
       "                       1.9681e-04, -2.7787e-03, -9.6359e-03, -5.1274e-03,  7.2051e-03,\n",
       "                       6.3816e-03, -6.9447e-03,  3.9199e-03,  8.7098e-03, -3.3579e-03,\n",
       "                       6.2362e-03,  4.8698e-03,  1.1400e-02,  1.0584e-02, -1.9225e-03,\n",
       "                      -9.1574e-03,  1.2531e-03,  1.3271e-03,  3.6703e-03, -9.5306e-04,\n",
       "                       8.3118e-03,  1.9835e-03, -8.3204e-03,  8.0669e-03, -1.7875e-03,\n",
       "                      -6.3975e-03, -1.3106e-03, -8.4176e-03,  6.5042e-03, -5.1665e-03,\n",
       "                      -7.2621e-04, -2.6879e-03,  1.5541e-03, -1.6089e-03,  7.2232e-03,\n",
       "                       3.4469e-04,  5.3864e-03,  1.7834e-03, -1.7098e-03, -3.2839e-03,\n",
       "                      -1.0943e-02,  4.0915e-03,  1.1770e-02,  1.0007e-03, -5.2735e-03,\n",
       "                       8.9759e-03, -6.4132e-03, -4.3968e-04, -1.2724e-02, -8.3822e-03,\n",
       "                       2.7959e-03,  9.6675e-03])),\n",
       "             ('cls.predictions.bias',\n",
       "              tensor([-1.1020, -0.0146, -0.0058,  ..., -0.0652, -0.0662, -0.0667])),\n",
       "             ('cls.predictions.transform.dense.weight',\n",
       "              tensor([[ 0.1659, -0.0080,  0.0047,  ...,  0.0076,  0.0010,  0.0132],\n",
       "                      [-0.0218,  0.1622,  0.0305,  ...,  0.0037,  0.0138,  0.0075],\n",
       "                      [-0.0227,  0.0110,  0.1169,  ...,  0.0106,  0.0115, -0.0269],\n",
       "                      ...,\n",
       "                      [-0.0234,  0.0055, -0.0010,  ...,  0.1126,  0.0110, -0.0065],\n",
       "                      [-0.0007, -0.0121,  0.0055,  ...,  0.0088,  0.1352,  0.0111],\n",
       "                      [ 0.0042,  0.0121,  0.0131,  ...,  0.0124,  0.0134,  0.1618]])),\n",
       "             ('cls.predictions.transform.dense.bias',\n",
       "              tensor([-2.3597e-03,  2.5548e-03,  1.1080e-03, -4.1874e-03, -1.0007e-03,\n",
       "                      -2.5120e-01,  2.8545e-03,  1.1980e-02,  7.4477e-03, -1.0117e-02,\n",
       "                      -6.6092e-03,  1.0262e-02, -5.3299e-03, -5.1444e-03, -8.1676e-03,\n",
       "                       7.9640e-04, -2.8799e-03, -2.0463e-01, -4.4928e-03,  2.2583e-03,\n",
       "                       7.4804e-04,  8.7750e-04, -5.3549e-03,  6.6997e-04,  4.4504e-03,\n",
       "                      -1.8706e-03, -3.1981e-03,  2.7873e-03, -2.0106e-03,  3.0274e-03,\n",
       "                      -5.8671e-03,  4.8301e-03,  4.3967e-03,  5.0626e-03,  3.4346e-04,\n",
       "                       4.1138e-04,  4.2554e-03,  6.5650e-03,  7.1145e-03, -2.9518e-03,\n",
       "                       2.3052e-03, -3.4155e-03,  2.6310e-03, -4.0001e-03, -6.2535e-03,\n",
       "                      -1.8001e-03,  3.5040e-03,  5.7080e-03,  5.9055e-03,  9.2052e-03,\n",
       "                       5.2957e-03,  1.1873e-02,  5.8112e-04,  4.4796e-03, -5.5418e-04,\n",
       "                       6.8351e-03, -7.3152e-03,  8.4599e-03, -1.4906e-03,  9.0089e-03,\n",
       "                      -6.2133e-04, -2.9987e-01, -2.6273e-03,  1.4948e-03, -2.7520e-03,\n",
       "                      -1.0760e-03, -9.1560e-03, -2.1621e-03,  8.1720e-04,  1.5701e-03,\n",
       "                       6.0318e-03,  9.2689e-03, -3.5561e-03, -4.9927e-03,  1.5151e-02,\n",
       "                       4.7063e-03, -5.7794e-03, -2.7101e-03,  3.1200e-03, -6.2623e-03,\n",
       "                       1.1747e-02, -6.7686e-03,  9.7233e-03,  4.6460e-03, -1.3930e-02,\n",
       "                       1.1980e-02,  1.7351e-03,  3.3273e-03, -1.2968e-02,  5.6194e-03,\n",
       "                       6.9499e-03,  3.5890e-03,  7.6850e-03,  4.3844e-03, -3.6810e-03,\n",
       "                      -7.0615e-03,  3.2039e-04, -8.4185e-03,  1.3863e-02, -1.8792e-03,\n",
       "                       7.3185e-03, -1.3371e-03,  3.1015e-03,  7.8295e-03,  9.2269e-03,\n",
       "                       3.5080e-03, -8.9045e-03, -2.7438e-01,  2.9849e-03, -3.4118e-03,\n",
       "                      -9.8282e-04, -1.7030e-03,  2.2888e-03,  1.0530e-02,  4.9347e-03,\n",
       "                       1.9850e-03, -2.7019e-03,  2.0229e-03, -8.2919e-03,  7.3864e-04,\n",
       "                       3.1317e-03, -1.6501e-03,  3.4725e-03, -9.0969e-03,  4.2031e-03,\n",
       "                      -9.3691e-03, -8.7240e-04, -5.6532e-03, -1.0939e-02, -1.5773e-03,\n",
       "                       9.8177e-03, -2.7540e-01, -6.1178e-03,  1.0379e-03,  3.2650e-03,\n",
       "                      -4.8662e-03,  7.2252e-03,  4.0480e-03, -5.2957e-04, -3.6481e-03,\n",
       "                       8.0589e-04,  1.8066e-03,  4.7732e-03, -1.4309e-03,  5.6458e-04,\n",
       "                       1.0759e-02, -5.5612e-03, -4.6232e-03,  2.7491e-03, -3.0676e-03,\n",
       "                      -1.2690e-02,  5.0495e-03,  4.6715e-03,  4.3054e-03,  2.1617e-04,\n",
       "                       2.8296e-03, -2.0601e-03, -3.7593e-03,  5.6215e-03,  8.7982e-03,\n",
       "                      -8.1243e-03,  7.9428e-03,  6.5062e-03, -1.7608e-03, -2.6728e-03,\n",
       "                      -1.8346e-03, -3.4372e-03, -3.1712e-03, -8.7783e-03, -4.2215e-03,\n",
       "                       6.3449e-03,  7.0591e-04,  1.0102e-02,  9.0939e-03,  1.2059e-02,\n",
       "                       9.8759e-03, -2.4047e-01, -7.5672e-03, -3.1050e-04, -1.5847e-03,\n",
       "                      -5.8528e-03,  8.8967e-03,  8.9736e-03, -5.5419e-03,  8.4191e-03,\n",
       "                      -6.2700e-03,  7.8061e-03,  4.9567e-03,  5.4176e-03, -7.0094e-03,\n",
       "                       1.9300e-03,  1.1080e-02])),\n",
       "             ('cls.predictions.transform.LayerNorm.weight',\n",
       "              tensor([2.2364, 2.2396, 2.3895, 2.2455, 2.2448, 1.7680, 2.1327, 2.0943, 2.1679,\n",
       "                      2.1416, 2.2598, 2.1600, 2.2408, 2.2136, 2.1156, 2.1609, 2.2368, 2.0375,\n",
       "                      2.2447, 2.2336, 2.4170, 2.2003, 2.2273, 2.1985, 2.1813, 2.2497, 2.2293,\n",
       "                      2.1647, 2.2122, 2.2412, 2.3017, 2.2055, 2.3201, 2.4163, 2.2197, 2.1954,\n",
       "                      2.1936, 2.0884, 2.2663, 2.1195, 2.1808, 2.2373, 2.2268, 2.1627, 2.3522,\n",
       "                      2.1973, 2.3677, 2.2258, 2.2371, 2.2537, 2.1589, 2.2778, 2.1474, 2.2985,\n",
       "                      2.4082, 2.2393, 2.1525, 2.2966, 2.2557, 2.2316, 2.2721, 1.6671, 2.1674,\n",
       "                      2.1035, 2.2319, 2.2645, 2.1452, 2.2252, 2.1502, 2.2035, 2.3087, 2.3332,\n",
       "                      2.2440, 2.2563, 2.1502, 2.1108, 2.1774, 2.2227, 2.1958, 2.2489, 2.2124,\n",
       "                      2.3126, 2.2097, 2.1993, 2.2974, 2.3019, 2.3206, 2.3179, 2.2457, 2.3763,\n",
       "                      2.1408, 2.1164, 2.2348, 2.2401, 2.3303, 2.2050, 2.2158, 2.2512, 2.1474,\n",
       "                      2.2332, 2.1733, 2.1803, 2.2039, 2.1725, 2.2425, 2.2711, 2.2517, 1.7274,\n",
       "                      2.2190, 2.2263, 2.3278, 2.1624, 2.1661, 2.2503, 2.1631, 2.2166, 2.2765,\n",
       "                      2.2231, 2.3315, 2.3006, 2.1846, 2.2287, 2.2330, 2.1363, 2.2963, 2.3556,\n",
       "                      2.2356, 2.2376, 2.2594, 2.2560, 2.1261, 1.7233, 2.2129, 2.4201, 2.2384,\n",
       "                      2.1205, 2.1454, 2.1524, 2.2674, 2.1912, 2.1332, 2.3933, 2.2213, 2.1958,\n",
       "                      2.2435, 2.3501, 2.1062, 2.1299, 2.1685, 2.2586, 2.2298, 2.2181, 2.2053,\n",
       "                      2.1999, 2.2939, 2.2561, 2.2357, 2.2276, 2.1592, 2.0914, 2.3138, 2.2633,\n",
       "                      2.1957, 2.2681, 2.2344, 2.0727, 2.2443, 2.2106, 2.1971, 2.3398, 2.1540,\n",
       "                      2.2781, 2.1645, 2.3768, 2.2250, 2.2092, 1.8959, 2.3756, 2.2195, 2.2348,\n",
       "                      2.3232, 2.1739, 2.3014, 2.1635, 2.1752, 2.1536, 2.1840, 2.1802, 2.1200,\n",
       "                      2.2986, 2.1610, 2.2371])),\n",
       "             ('cls.predictions.transform.LayerNorm.bias',\n",
       "              tensor([ 0.2777,  0.3413,  0.4607,  0.3043, -0.3174, -0.7037, -0.3682, -0.3437,\n",
       "                      -0.4141,  0.0560,  0.4981,  0.4859,  0.6137, -0.6453, -0.5515, -0.4018,\n",
       "                      -0.2512, -0.8815,  0.3157, -0.3554,  0.6655, -0.4051, -0.3244,  0.3451,\n",
       "                      -0.4930,  0.3510,  0.3891,  0.3974, -0.0635,  0.4074, -0.4640, -0.1869,\n",
       "                       0.5278, -0.0533,  0.1254, -0.4548, -0.4813,  0.3169, -0.2440,  0.3271,\n",
       "                      -0.4383, -0.3205, -0.5309, -0.4977,  0.3604, -0.2821,  0.5128, -0.3576,\n",
       "                      -0.3542, -0.2340,  0.2940, -0.3613,  0.3576,  0.4510, -0.0386, -0.3851,\n",
       "                      -0.3554, -0.2191,  0.4234,  0.4482,  0.5326, -0.6224, -0.3103, -0.2432,\n",
       "                       0.2878,  0.5074, -0.3563, -0.3498,  0.2701, -0.3384,  0.3962,  0.5350,\n",
       "                       0.4137,  0.2923, -0.3928, -0.4092, -0.3652, -0.3307,  0.2294, -0.3940,\n",
       "                       0.4648,  0.6121,  0.3596, -0.3528,  0.3177, -0.0997,  0.5156, -0.3311,\n",
       "                      -0.3522, -0.2116,  0.3378, -0.2857, -0.3324,  0.4769, -0.0265,  0.4419,\n",
       "                       0.4557, -0.3582, -0.4003,  0.2343,  0.3224, -0.4262, -0.4089, -0.3728,\n",
       "                      -0.4220,  0.4000,  0.2992, -0.6602, -0.3928,  0.2434,  0.6131, -0.3013,\n",
       "                      -0.3856, -0.3487,  0.4008, -0.3153,  0.2812, -0.0034,  0.5025,  0.4834,\n",
       "                      -0.3470,  0.3111,  0.3706, -0.4505,  0.2318,  0.1822, -0.0502,  0.3647,\n",
       "                       0.4295,  0.3915, -0.2601, -0.6456, -0.3616,  0.0572, -0.3276, -0.3348,\n",
       "                      -0.5326,  0.3531,  0.4756, -0.3951,  0.3187, -0.1048, -0.3983, -0.4244,\n",
       "                       0.4553,  0.4399, -0.3553, -0.4262, -0.2746, -0.2886, -0.3855, -0.4144,\n",
       "                      -0.3621, -0.4965,  0.4916, -0.2616, -0.4182,  0.4543, -0.4505, -0.3760,\n",
       "                      -0.2602,  0.5526, -0.3551, -0.4042,  0.4035, -0.4078,  0.4287, -0.3763,\n",
       "                       0.2891,  0.5225, -0.3185,  0.4410, -0.5797,  0.4321, -0.3314,  0.4135,\n",
       "                      -0.7614,  0.5840, -0.3686, -0.3687,  0.0742, -0.3392, -0.2083,  0.3650,\n",
       "                      -0.4166, -0.3880, -0.3832, -0.4243, -0.4219,  0.5533, -0.3635, -0.3504])),\n",
       "             ('cls.predictions.decoder.weight',\n",
       "              tensor([[-0.0011,  0.0004, -0.0045,  ...,  0.0159, -0.0101,  0.0027],\n",
       "                      [-0.0006,  0.0035,  0.0629,  ..., -0.0453,  0.0473, -0.0018],\n",
       "                      [ 0.0705,  0.0396,  0.0785,  ...,  0.0846,  0.0274, -0.0472],\n",
       "                      ...,\n",
       "                      [-0.0316, -0.0742, -0.0694,  ..., -0.0674,  0.0521,  0.0679],\n",
       "                      [-0.0442, -0.0457, -0.0513,  ..., -0.0837,  0.0522,  0.0241],\n",
       "                      [-0.0279, -0.0400, -0.0354,  ..., -0.0921,  0.0450,  0.0153]])),\n",
       "             ('cls.predictions.decoder.bias',\n",
       "              tensor([-1.1020, -0.0146, -0.0058,  ..., -0.0652, -0.0662, -0.0667])),\n",
       "             ('cls.seq_relationship.weight',\n",
       "              tensor([[ 0.0400,  0.0592, -0.0292, -0.0228,  0.0451,  0.0307,  0.0370,  0.0471,\n",
       "                        0.0530,  0.0191,  0.0493, -0.0544, -0.0181, -0.0080,  0.0280, -0.0176,\n",
       "                        0.0294,  0.0364,  0.0380, -0.0621,  0.0664, -0.0447, -0.0239,  0.0289,\n",
       "                       -0.0383, -0.0105, -0.0416, -0.0153,  0.0457, -0.0523,  0.0554, -0.0134,\n",
       "                        0.0142,  0.0292,  0.0032,  0.0211, -0.0583,  0.0283,  0.0453, -0.0649,\n",
       "                       -0.0709, -0.0647,  0.0204,  0.0356,  0.0208, -0.0366,  0.0266, -0.0267,\n",
       "                        0.0382,  0.0399,  0.0206, -0.0287,  0.0410,  0.0570, -0.0278,  0.0040,\n",
       "                        0.0624, -0.0267,  0.0083, -0.0277, -0.0276, -0.0013, -0.0539,  0.0099,\n",
       "                        0.0294, -0.0364, -0.0411,  0.0188,  0.0384,  0.0404,  0.0406,  0.0330,\n",
       "                        0.0319,  0.0652, -0.0373, -0.0181, -0.0156, -0.0285,  0.0163,  0.0409,\n",
       "                       -0.0491,  0.0409,  0.0431,  0.0390,  0.0452,  0.0333,  0.0161,  0.0412,\n",
       "                       -0.0189, -0.0453, -0.0026, -0.0518, -0.0485, -0.0599, -0.0283,  0.0411,\n",
       "                       -0.0426, -0.0317, -0.0285,  0.0373,  0.0172,  0.0369,  0.0658,  0.0121,\n",
       "                       -0.0054,  0.0104, -0.0184,  0.0039, -0.0320,  0.0274,  0.0269, -0.0428,\n",
       "                        0.0328, -0.0261,  0.0382, -0.0292,  0.0387, -0.0209,  0.0249, -0.0166,\n",
       "                       -0.0280, -0.0632, -0.0204,  0.0180, -0.0134,  0.0255, -0.0119, -0.0458,\n",
       "                       -0.0115,  0.0319,  0.0329, -0.0207, -0.0418,  0.0395,  0.0204, -0.0478,\n",
       "                       -0.0532,  0.0290,  0.0185, -0.0154, -0.0376, -0.0515, -0.0337,  0.0376,\n",
       "                        0.0465,  0.0413, -0.0335,  0.0280,  0.0261, -0.0655,  0.0286,  0.0522,\n",
       "                        0.0337,  0.0396, -0.0471, -0.0238,  0.0484, -0.0276,  0.0163,  0.0090,\n",
       "                        0.0213,  0.0192, -0.0297,  0.0463, -0.0367, -0.0278, -0.0122, -0.0131,\n",
       "                        0.0567, -0.0357, -0.0398, -0.0100,  0.0287, -0.0023,  0.0515, -0.0480,\n",
       "                        0.0361,  0.0610, -0.0098, -0.0176, -0.0035,  0.0318,  0.0428, -0.0315,\n",
       "                       -0.0785,  0.0364, -0.0423, -0.0377, -0.0473, -0.0350,  0.0347,  0.0320],\n",
       "                      [-0.0275, -0.0255,  0.0143,  0.0253, -0.0203, -0.0343, -0.0410, -0.0320,\n",
       "                       -0.0447, -0.0615,  0.0027,  0.0294,  0.0257, -0.0464, -0.0421,  0.0438,\n",
       "                       -0.0381, -0.0566, -0.0309,  0.0432, -0.0400,  0.0655,  0.0430, -0.0313,\n",
       "                        0.0286, -0.0524,  0.0200,  0.0549, -0.0570,  0.0282, -0.0594,  0.0324,\n",
       "                       -0.0440, -0.0550,  0.0416, -0.0323,  0.0305, -0.0364, -0.0223,  0.0473,\n",
       "                        0.0507,  0.0519, -0.0619, -0.0187, -0.0458,  0.0242, -0.0438,  0.0407,\n",
       "                       -0.0637, -0.0415, -0.0330,  0.0539, -0.0399, -0.0589,  0.0203, -0.0202,\n",
       "                       -0.0673,  0.0086, -0.0389,  0.0207,  0.0519, -0.0074,  0.0153, -0.0437,\n",
       "                       -0.0226,  0.0179,  0.0144, -0.0319, -0.0353, -0.0481, -0.0292, -0.0219,\n",
       "                       -0.0269, -0.0576,  0.0432,  0.0261,  0.0090,  0.0391, -0.0221, -0.0363,\n",
       "                        0.0385, -0.0677, -0.0443, -0.0414, -0.0436, -0.0062, -0.0261, -0.0180,\n",
       "                        0.0559,  0.0521,  0.0360,  0.0570,  0.0150,  0.0503,  0.0214,  0.0101,\n",
       "                        0.0340,  0.0321,  0.0233, -0.0247, -0.0484, -0.0365, -0.0130, -0.0388,\n",
       "                        0.0522, -0.0366,  0.0349, -0.0117,  0.0473, -0.0033, -0.0315,  0.0457,\n",
       "                       -0.0278,  0.0352, -0.0367,  0.0428, -0.0637,  0.0437, -0.0469,  0.0564,\n",
       "                        0.0287,  0.0095,  0.0559, -0.0347,  0.0182, -0.0317,  0.0294,  0.0436,\n",
       "                        0.0454, -0.0418, -0.0211,  0.0267,  0.0074, -0.0377, -0.0091,  0.0404,\n",
       "                        0.0335, -0.0303, -0.0521, -0.0493,  0.0289,  0.0251,  0.0205, -0.0161,\n",
       "                       -0.0419, -0.0111,  0.0509, -0.0138, -0.0170,  0.0175, -0.0356, -0.0616,\n",
       "                       -0.0496, -0.0204,  0.0398,  0.0446, -0.0275,  0.0531, -0.0373,  0.0667,\n",
       "                       -0.0126, -0.0408,  0.0240, -0.0322,  0.0448,  0.0394,  0.0558,  0.0279,\n",
       "                       -0.0130,  0.0428,  0.0476, -0.0007, -0.0525,  0.0605, -0.0094,  0.0558,\n",
       "                       -0.0087, -0.0252,  0.0115,  0.0591,  0.0411, -0.0545, -0.0542,  0.0269,\n",
       "                        0.0784, -0.0318,  0.0160,  0.0601,  0.0709,  0.0036, -0.0497, -0.0339]])),\n",
       "             ('cls.seq_relationship.bias', tensor([ 0.0022, -0.0022]))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(root_path + \"MedBERT_Pretraining/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Packages\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertForSequenceClassification\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import transformers\n",
    "import time\n",
    "import tqdm\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from termcolor import colored\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle as pkl\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "%matplotlib inline\n",
    "use_cuda = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = pkl.load(\n",
    "    open(\n",
    "        root_path + \"data/finetuning_data_updated/finetuning_gentacimin_2.bencs.train\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    encoding=\"bytes\",\n",
    ")\n",
    "valid_f = pkl.load(\n",
    "    open(\n",
    "        root_path + \"data/finetuning_data_updated/finetuning_gentacimin_2.bencs.valid\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    encoding=\"bytes\",\n",
    ")\n",
    "test_f = pkl.load(\n",
    "    open(\n",
    "        root_path + \"data/finetuning_data_updated/finetuning_gentacimin_2.bencs.test\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    encoding=\"bytes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 12038\n",
      "length of validation dataset 1719\n",
      "length of testing dataset 3439\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of training dataset:\", len(train_f))\n",
    "print(\"length of validation dataset\", len(valid_f))\n",
    "print(\"length of testing dataset\", len(test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11046564,\n",
       " 1,\n",
       " [8931, 261, 1031, 192, 4026, 205, 11799, 397, 2391, 3107],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The version in this file is updated to dump pt_id for better visualization and results analysis\n",
    "\n",
    "# Below are key functions for  Data prepartion ,formating input data into features, and model defintion\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    Based on original BERT code: We use this class instead of `None` because treating `None` as padding\n",
    "    batches could cause silent errors.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_ids, input_mask, segment_ids, label_id, pt_id, is_real_example=True\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example\n",
    "        self.pt_id = pt_id\n",
    "\n",
    "\n",
    "def convert_EHRexamples_to_features(examples, max_seq_length):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        feature = convert_singleEHR_example(ex_index, example, max_seq_length)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "# This is the EHR version\n",
    "def convert_singleEHR_example(ex_index, example, max_seq_length):\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_id=0,\n",
    "            is_real_example=False,\n",
    "        )\n",
    "\n",
    "    input_ids = example[2]\n",
    "    segment_ids = example[3]\n",
    "    label_id = example[1]\n",
    "    pt_id = example[0]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # LR 5/13 Left Truncate longer sequence\n",
    "    while len(input_ids) > max_seq_length:\n",
    "        input_ids = input_ids[-max_seq_length:]\n",
    "        input_mask = input_mask[-max_seq_length:]\n",
    "        segment_ids = segment_ids[-max_seq_length:]\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    feature = [input_ids, input_mask, segment_ids, label_id, pt_id, True]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTdataEHR(Dataset):\n",
    "    def __init__(self, Features):\n",
    "\n",
    "        self.data = Features\n",
    "\n",
    "    def __getitem__(self, idx, seeDescription=False):\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# customized parts for EHRdataloader\n",
    "def my_collate(batch):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    all_label_ids = []\n",
    "    all_pt_ids = []\n",
    "\n",
    "    for feature in batch:\n",
    "        all_input_ids.append(feature[0])\n",
    "        all_input_mask.append(feature[1])\n",
    "        all_segment_ids.append(feature[2])\n",
    "        all_label_ids.append(feature[3])\n",
    "        all_pt_ids.append(feature[4])\n",
    "    return [[all_input_ids, all_input_mask, all_segment_ids, all_label_ids], all_pt_ids]\n",
    "\n",
    "\n",
    "class BERTdataEHRloader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=0,\n",
    "        collate_fn=my_collate,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "    ):\n",
    "        DataLoader.__init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            sampler=None,\n",
    "            batch_sampler=None,\n",
    "            num_workers=0,\n",
    "            collate_fn=my_collate,\n",
    "            pin_memory=False,\n",
    "            drop_last=False,\n",
    "            timeout=0,\n",
    "            worker_init_fn=None,\n",
    "        )\n",
    "        self.collate_fn = collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_BERT_LR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embed_dim,\n",
    "        hidden_size,\n",
    "        n_layers=1,\n",
    "        dropout_r=0.1,\n",
    "        cell_type=\"LSTM\",\n",
    "        bi=False,\n",
    "        time=False,\n",
    "        preTrainEmb=\"\",\n",
    "    ):\n",
    "        super(EHR_BERT_LR, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.preTrainEmb = preTrainEmb\n",
    "        self.time = time\n",
    "\n",
    "        if bi:\n",
    "            self.bi = 2\n",
    "        else:\n",
    "            self.bi = 1\n",
    "\n",
    "        self.PreBERTmodel = BertForSequenceClassification.from_pretrained(\n",
    "            root_path + \"MedBERT_Pretraining/\"\n",
    "        )\n",
    "        if use_cuda:\n",
    "            self.PreBERTmodel.cuda()\n",
    "        input_size = self.PreBERTmodel.bert.config.vocab_size\n",
    "        self.in_size = self.PreBERTmodel.bert.config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_r)\n",
    "        self.out = nn.Linear(self.in_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        if use_cuda:\n",
    "            self.flt_typ = torch.cuda.FloatTensor\n",
    "            self.lnt_typ = torch.cuda.LongTensor\n",
    "        else:\n",
    "            self.lnt_typ = torch.LongTensor\n",
    "            self.flt_typ = torch.FloatTensor\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        token_t = torch.from_numpy(np.asarray(sequence[0][0], dtype=int)).type(\n",
    "            self.lnt_typ\n",
    "        )\n",
    "        seg_t = torch.from_numpy(np.asarray(sequence[0][2], dtype=int)).type(\n",
    "            self.lnt_typ\n",
    "        )\n",
    "        Label_t = torch.from_numpy(np.asarray(sequence[0][3], dtype=int)).type(\n",
    "            self.lnt_typ\n",
    "        )\n",
    "        Bert_out = self.PreBERTmodel.bert(\n",
    "            input_ids=token_t,\n",
    "            attention_mask=torch.from_numpy(np.asarray(sequence[0][1], dtype=int)).type(\n",
    "                self.lnt_typ\n",
    "            ),\n",
    "            token_type_ids=seg_t,\n",
    "        )\n",
    "        output = self.sigmoid(self.out(Bert_out[1]))\n",
    "        return output.squeeze(), Label_t.type(self.flt_typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def trainsample(sample, model, optimizer, criterion=nn.BCELoss()):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    # print(sample[0])\n",
    "    output, label_tensor = model(sample)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "# train with loaders\n",
    "\n",
    "\n",
    "def trainbatches(mbs_list, model, optimizer, shuffle=True):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    plot_every = 5\n",
    "    n_iter = 0\n",
    "    if shuffle:\n",
    "        random.shuffle(mbs_list)\n",
    "    for i, batch in enumerate(mbs_list):\n",
    "        output, loss = trainsample(batch, model, optimizer, criterion=nn.BCELoss())\n",
    "        current_loss += loss\n",
    "        n_iter += 1\n",
    "\n",
    "        if n_iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "    return current_loss, all_losses\n",
    "\n",
    "\n",
    "def calculate_auc(model, mbs_list, shuffle=True):\n",
    "    model.eval()\n",
    "    y_real = []\n",
    "    y_hat = []\n",
    "    if shuffle:\n",
    "        random.shuffle(mbs_list)\n",
    "    for i, batch in enumerate(mbs_list):\n",
    "        output, label_tensor = model(batch)\n",
    "        y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "        y_real.extend(label_tensor.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(y_real, y_hat)\n",
    "    return auc, y_real, y_hat\n",
    "\n",
    "\n",
    "# define the final epochs running, use the different names\n",
    "\n",
    "\n",
    "def epochs_run(\n",
    "    epochs,\n",
    "    train,\n",
    "    valid,\n",
    "    test,\n",
    "    model,\n",
    "    optimizer,\n",
    "    shuffle=True,\n",
    "    patience=20,\n",
    "    output_dir=\"../models/\",\n",
    "    model_prefix=\"dhf.train\",\n",
    "    model_customed=\"\",\n",
    "):\n",
    "    bestValidAuc = 0.0\n",
    "    bestTestAuc = 0.0\n",
    "    bestValidEpoch = 0\n",
    "    # header = 'BestValidAUC|TestAUC|atEpoch'\n",
    "    # logFile = output_dir + model_prefix + model_customed +'EHRmodel.log'\n",
    "    # print2file(header, logFile)\n",
    "    # writer = SummaryWriter(output_dir+'/tsb_runs/') ## LR added 9/27 for tensorboard integration\n",
    "    for ep in range(epochs):\n",
    "        print(ep)\n",
    "        start = time.time()\n",
    "        current_loss, train_loss = trainbatches(\n",
    "            mbs_list=train, model=model, optimizer=optimizer\n",
    "        )\n",
    "        train_time = timeSince(start)\n",
    "        # epoch_loss.append(train_loss)\n",
    "        avg_loss = np.mean(train_loss)\n",
    "        # writer.add_scalar('Loss/train', avg_loss, ep) ## LR added 9/27\n",
    "        valid_start = time.time()\n",
    "        train_auc, _, _ = calculate_auc(model=model, mbs_list=train, shuffle=shuffle)\n",
    "        valid_auc, _, _ = calculate_auc(model=model, mbs_list=valid, shuffle=shuffle)\n",
    "        valid_time = timeSince(valid_start)\n",
    "        # writer.add_scalar('train_auc', train_auc, ep) ## LR added 9/27\n",
    "        # writer.add_scalar('valid_auc', valid_auc, ep) ## LR added 9/27\n",
    "        print(\n",
    "            colored(\n",
    "                \"\\n Epoch (%s): Train_auc (%s), Valid_auc (%s) ,Training Average_loss (%s), Train_time (%s), Eval_time (%s)\"\n",
    "                % (ep, train_auc, valid_auc, avg_loss, train_time, valid_time),\n",
    "                \"green\",\n",
    "            )\n",
    "        )\n",
    "        if valid_auc > bestValidAuc:\n",
    "            bestValidAuc = valid_auc\n",
    "            bestValidEpoch = ep\n",
    "            best_model = model\n",
    "            bestTrainAuc = train_auc\n",
    "            if test:\n",
    "                testeval_start = time.time()\n",
    "                bestTestAuc, _, _ = calculate_auc(\n",
    "                    model=best_model, mbs_list=test, shuffle=shuffle\n",
    "                )\n",
    "\n",
    "                # writer.add_scalar('test_auc', valid_auc, ep) ## LR added 9/27\n",
    "                print(\n",
    "                    colored(\n",
    "                        \"\\n Test_AUC (%s) , Test_eval_time (%s) \"\n",
    "                        % (bestTestAuc, timeSince(testeval_start)),\n",
    "                        \"yellow\",\n",
    "                    )\n",
    "                )\n",
    "                # print(best_model,model) ## to verify that the hyperparameters already impacting the model definition\n",
    "                # print(optimizer)\n",
    "        if ep - bestValidEpoch > patience:\n",
    "            break\n",
    "\n",
    "    # writer.close()\n",
    "    # if not os.path.exists(output_dir):\n",
    "    #    os.makedirs(output_dir)\n",
    "    # save model & parameters\n",
    "    # torch.save(best_model, output_dir + model_prefix + model_customed + 'EHRmodel.pth')\n",
    "    # torch.save(best_model.state_dict(), output_dir + model_prefix + model_customed + 'EHRmodel.st')\n",
    "\n",
    "    if test:\n",
    "        print(\n",
    "            colored(\n",
    "                \"BestValidAuc %f has a TestAuc of %f at epoch %d \"\n",
    "                % (bestValidAuc, bestTestAuc, bestValidEpoch),\n",
    "                \"green\",\n",
    "            )\n",
    "        )\n",
    "        return bestTrainAuc, bestValidAuc, bestTestAuc, bestValidEpoch\n",
    "    else:\n",
    "        print(\n",
    "            colored(\n",
    "                \"BestValidAuc %f at epoch %d \" % (bestValidAuc, bestValidEpoch), \"green\"\n",
    "            )\n",
    "        )\n",
    "        print(\"No Test Accuracy\")\n",
    "\n",
    "    print(\n",
    "        colored(\n",
    "            \"Details see ../models/%sEHRmodel.log\" % (model_prefix + model_customed),\n",
    "            \"green\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating the list of training minibatches\n",
      " creating the list of test minibatches\n",
      " creating the list of valid minibatches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7458129575797803), Valid_auc (0.5654629861982434) ,Training Average_loss (0.2815218127643069), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6106615366261036) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.7682603571471412), Valid_auc (0.6141706398996236) ,Training Average_loss (0.258651839569211), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5445776664280602) , Test_eval_time (0m 0s) \n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.5383837212556928), Valid_auc (0.5084240903387703) ,Training Average_loss (0.2674365228662888), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.5818002702994486), Valid_auc (0.5262308657465495) ,Training Average_loss (0.26642555234332876), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.5722634180085008), Valid_auc (0.5213149309912171) ,Training Average_loss (0.2650428790599108), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.5789798694885502), Valid_auc (0.528745294855709) ,Training Average_loss (0.26629300757000846), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.5751140575705838), Valid_auc (0.5300803011292347) ,Training Average_loss (0.26557936035096646), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.5779605986572994), Valid_auc (0.5256587202007528) ,Training Average_loss (0.26613627796371775), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.6016790254025502), Valid_auc (0.5415332496863238) ,Training Average_loss (0.26558555426696934), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.6052266057026786), Valid_auc (0.5370213299874529) ,Training Average_loss (0.26523683841029805), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.6271591967787983), Valid_auc (0.5548130489335005) ,Training Average_loss (0.2664239080622792), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.6258807723526847), Valid_auc (0.5561907151819323) ,Training Average_loss (0.2665864228581389), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.6472984049933426), Valid_auc (0.5589008782936009) ,Training Average_loss (0.2668137355397145), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.6478942833266026), Valid_auc (0.5598343789209537) ,Training Average_loss (0.2657312277704477), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.6566880620409209), Valid_auc (0.5626223337515684) ,Training Average_loss (0.2658196945364277), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.639991423190573), Valid_auc (0.5545972396486827) ,Training Average_loss (0.26639976470420756), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.6663951510038265), Valid_auc (0.5559673776662483) ,Training Average_loss (0.26540223049620787), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.6607059173987277), Valid_auc (0.5460878293601004) ,Training Average_loss (0.2658022491261363), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.6680353908346096), Valid_auc (0.5545922208281056) ,Training Average_loss (0.26686844117939473), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.6700458929278308), Valid_auc (0.5426474278544543) ,Training Average_loss (0.2652978456268708), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.6620428200709336), Valid_auc (0.5386348808030114) ,Training Average_loss (0.26677864156663417), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.5633946172024007), Valid_auc (0.5197189460476788) ,Training Average_loss (0.2660396805033088), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (22): Train_auc (0.6638957807695538), Valid_auc (0.54364617314931) ,Training Average_loss (0.2655405956010024), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.614171 has a TestAuc of 0.544578 at epoch 1 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7682960438716796), Valid_auc (0.561676286072773) ,Training Average_loss (0.2701652484635512), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5927380100214745) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.8918523809142999), Valid_auc (0.5999774153074027) ,Training Average_loss (0.25251959972083565), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6097434979718445) , Test_eval_time (0m 0s) \n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.9238882135845466), Valid_auc (0.6044767879548306) ,Training Average_loss (0.21868330184370277), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6134460749224528) , Test_eval_time (0m 0s) \n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.9594104992942626), Valid_auc (0.6214755332496863) ,Training Average_loss (0.16755947129180035), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5969559770937723) , Test_eval_time (0m 0s) \n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.9756510078250892), Valid_auc (0.6167478042659975) ,Training Average_loss (0.1383099958921472), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.9875098963185693), Valid_auc (0.6111493099121705) ,Training Average_loss (0.09864609825114408), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.9873174679019404), Valid_auc (0.6142609786700126) ,Training Average_loss (0.08408010120037944), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.988768528107544), Valid_auc (0.6097867001254704) ,Training Average_loss (0.08812112347222863), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.9878655140287813), Valid_auc (0.617651191969887) ,Training Average_loss (0.08667139584819476), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.985555873215164), Valid_auc (0.596762860727729) ,Training Average_loss (0.09989448095827053), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.987268536104569), Valid_auc (0.5986298619824342) ,Training Average_loss (0.08914059045103688), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.990161210029869), Valid_auc (0.6034880803011292) ,Training Average_loss (0.08328003022276485), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.9917175310784387), Valid_auc (0.6083086574654957) ,Training Average_loss (0.0720472102674345), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.9906772180748761), Valid_auc (0.5701681304893351) ,Training Average_loss (0.06785041740707431), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.9915677867833165), Valid_auc (0.5490890840652446) ,Training Average_loss (0.07620148116257042), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.990441755666892), Valid_auc (0.5819121706398995) ,Training Average_loss (0.068307422947449), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.989731969707269), Valid_auc (0.5662208281053952) ,Training Average_loss (0.08810493785422295), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.9873429084380609), Valid_auc (0.5692572145545797) ,Training Average_loss (0.07975963356439024), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.9929611684453383), Valid_auc (0.5916762860727729) ,Training Average_loss (0.07171195307746529), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.9921039873167181), Valid_auc (0.5813651191969887) ,Training Average_loss (0.06573300583016438), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.9913509674401124), Valid_auc (0.6034127979924718) ,Training Average_loss (0.07753604551156361), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.9928719516339922), Valid_auc (0.5815809284818068) ,Training Average_loss (0.07117212310743828), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n",
      "\n",
      " Epoch (22): Train_auc (0.9939011187838123), Valid_auc (0.577987452948557) ,Training Average_loss (0.06956811946971964), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "23\n",
      "\n",
      " Epoch (23): Train_auc (0.9958713358630589), Valid_auc (0.5988005018820577) ,Training Average_loss (0.06094922212262948), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (24): Train_auc (0.9965176454358977), Valid_auc (0.5884140526976162) ,Training Average_loss (0.04854492836942276), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.621476 has a TestAuc of 0.596956 at epoch 3 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7940904483532127), Valid_auc (0.5862735257214554) ,Training Average_loss (0.2800937828918298), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6188821283703173) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.9277385312663688), Valid_auc (0.6401555834378921) ,Training Average_loss (0.2420380999644597), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6364805535671678) , Test_eval_time (0m 0s) \n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.9721363652721187), Valid_auc (0.6179598494353826) ,Training Average_loss (0.17886195086563625), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.9855333815820515), Valid_auc (0.6223889585947302) ,Training Average_loss (0.13121016955313583), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.9885718512713272), Valid_auc (0.6124190715181932) ,Training Average_loss (0.10514077788684517), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.9910019472756134), Valid_auc (0.6147176913425345) ,Training Average_loss (0.08840786841853211), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.9799821466414493), Valid_auc (0.611543287327478) ,Training Average_loss (0.0906879868164348), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.9836591288040848), Valid_auc (0.6230188205771643) ,Training Average_loss (0.10682346497972806), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.9920486578992614), Valid_auc (0.6234730238393978) ,Training Average_loss (0.08790425688493997), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.9946725817995705), Valid_auc (0.5966750313676287) ,Training Average_loss (0.06875759347264344), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.9939054171848072), Valid_auc (0.6328030112923462) ,Training Average_loss (0.057529947842704134), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.9938472388271564), Valid_auc (0.6122183186951066) ,Training Average_loss (0.059383701983218386), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.9944184263453995), Valid_auc (0.6044843161856963) ,Training Average_loss (0.06915034252839784), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.9934127504568301), Valid_auc (0.6122509410288582) ,Training Average_loss (0.06349935327113296), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.9921420231673816), Valid_auc (0.6058996235884567) ,Training Average_loss (0.057346008619060744), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.9951309113009961), Valid_auc (0.5973977415307402) ,Training Average_loss (0.05221136305869247), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.9918304890580705), Valid_auc (0.5999021329987453) ,Training Average_loss (0.05693222629682471), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.9888259067626842), Valid_auc (0.5774880803011292) ,Training Average_loss (0.0814386403731381), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.9571715821714323), Valid_auc (0.5827603513174404) ,Training Average_loss (0.10748044109592836), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.9693314586973648), Valid_auc (0.6125721455457966) ,Training Average_loss (0.13140245211931567), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.9759192080546039), Valid_auc (0.5955959849435382) ,Training Average_loss (0.11096336174135407), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.9832493812301825), Valid_auc (0.6149585947302384) ,Training Average_loss (0.1024335708313932), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (22): Train_auc (0.9856758285917637), Valid_auc (0.6075809284818068) ,Training Average_loss (0.09148495428574582), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.640156 has a TestAuc of 0.636481 at epoch 1 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7889258695765176), Valid_auc (0.5661355081555834) ,Training Average_loss (0.27596257558713355), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5541141732283464) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.7855373301131979), Valid_auc (0.5584943538268506) ,Training Average_loss (0.2773261588066816), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.9215654276609101), Valid_auc (0.6249686323713927) ,Training Average_loss (0.23846212650338808), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6280493915533285) , Test_eval_time (0m 0s) \n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.9461313891232461), Valid_auc (0.61299121706399) ,Training Average_loss (0.1866426659747958), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.965925425741624), Valid_auc (0.6009636135508156) ,Training Average_loss (0.1354962116262565), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.9765468245812556), Valid_auc (0.6221028858218319) ,Training Average_loss (0.10345340196896967), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.9763102126009124), Valid_auc (0.5968331242158093) ,Training Average_loss (0.08090818637671571), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.9836915167557668), Valid_auc (0.5976260978670012) ,Training Average_loss (0.06971684305074936), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.979042346247196), Valid_auc (0.6106273525721455) ,Training Average_loss (0.06129225817276165), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.9810984313835254), Valid_auc (0.6018845671267253) ,Training Average_loss (0.06700163560065751), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.9772072289108451), Valid_auc (0.6258168130489336) ,Training Average_loss (0.08340356650296599), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5792155810069196) , Test_eval_time (0m 0s) \n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.9760962921793094), Valid_auc (0.5619799247176913) ,Training Average_loss (0.07497060302800189), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.9792311260211201), Valid_auc (0.5637365119196989) ,Training Average_loss (0.08673157812251399), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.9769906594746756), Valid_auc (0.5988983688833125) ,Training Average_loss (0.07659211605399226), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.9729100774511882), Valid_auc (0.6087503136762862) ,Training Average_loss (0.07590930391646301), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.9709922908677973), Valid_auc (0.6061053952321205) ,Training Average_loss (0.10967580961684387), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.9737945484279847), Valid_auc (0.6032973651191971) ,Training Average_loss (0.10693717806134373), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.9810727409403701), Valid_auc (0.6010740276035133) ,Training Average_loss (0.08479728989768774), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.9828909645611831), Valid_auc (0.5992446675031367) ,Training Average_loss (0.07062274793861434), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.9852693797907178), Valid_auc (0.5816838143036387) ,Training Average_loss (0.0647157477447763), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.9840628786091574), Valid_auc (0.607465495608532) ,Training Average_loss (0.06413697361325224), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.9847955560531481), Valid_auc (0.5344491844416561) ,Training Average_loss (0.06504446350736544), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n",
      "\n",
      " Epoch (22): Train_auc (0.9837540934772265), Valid_auc (0.5949761606022584) ,Training Average_loss (0.05939287103634949), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "23\n",
      "\n",
      " Epoch (23): Train_auc (0.9845118116060825), Valid_auc (0.6021631116687579) ,Training Average_loss (0.057113591267261656), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "24\n",
      "\n",
      " Epoch (24): Train_auc (0.9870363224880345), Valid_auc (0.6186198243412798) ,Training Average_loss (0.049543055841544024), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "25\n",
      "\n",
      " Epoch (25): Train_auc (0.9880511949554766), Valid_auc (0.4812421580928482) ,Training Average_loss (0.04022282087535132), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "26\n",
      "\n",
      " Epoch (26): Train_auc (0.986341131099231), Valid_auc (0.5784040150564618) ,Training Average_loss (0.04815338399494067), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "27\n",
      "\n",
      " Epoch (27): Train_auc (0.9861606482388552), Valid_auc (0.5933826850690088) ,Training Average_loss (0.048996222900071495), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "28\n",
      "\n",
      " Epoch (28): Train_auc (0.9855586721739513), Valid_auc (0.5806976160602259) ,Training Average_loss (0.05695929350719477), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "29\n",
      "\n",
      " Epoch (29): Train_auc (0.9865284114309477), Valid_auc (0.61433626097867) ,Training Average_loss (0.05442560790494705), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "30\n",
      "\n",
      " Epoch (30): Train_auc (0.9874810570467787), Valid_auc (0.5890915934755332) ,Training Average_loss (0.04228855808808779), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (31): Train_auc (0.9902698696085057), Valid_auc (0.5663864491844417) ,Training Average_loss (0.04228288782954526), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.625817 has a TestAuc of 0.579216 at epoch 10 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7701427568944352), Valid_auc (0.5814705144291091) ,Training Average_loss (0.27265145586182676), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5894243617275113) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.7833667875550296), Valid_auc (0.5581028858218319) ,Training Average_loss (0.26631685507794217), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.6043461832198423), Valid_auc (0.5129385194479297) ,Training Average_loss (0.26623645424842834), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.6496524792777087), Valid_auc (0.5289159347553325) ,Training Average_loss (0.2655621608098348), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.5309324431311553), Valid_auc (0.4947051442910916) ,Training Average_loss (0.26524125741173826), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.6350377159696593), Valid_auc (0.5258594730238394) ,Training Average_loss (0.26667132253448167), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.49789693234116905), Valid_auc (0.4813927227101631) ,Training Average_loss (0.2660154111683369), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.5979795016253954), Valid_auc (0.5034328732747804) ,Training Average_loss (0.2662100742260615), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.4594673281539267), Valid_auc (0.47603764115432873) ,Training Average_loss (0.26463839802891015), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.619210103841371), Valid_auc (0.5213249686323713) ,Training Average_loss (0.2653021892532706), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.6049986405057319), Valid_auc (0.5236010037641154) ,Training Average_loss (0.26588011185328164), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.5283912884407), Valid_auc (0.49325721455457966) ,Training Average_loss (0.26569452087084455), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.5622344987664589), Valid_auc (0.4968506900878294) ,Training Average_loss (0.26581731171657647), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.5286434446385944), Valid_auc (0.48176662484316185) ,Training Average_loss (0.2662130316098531), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.44474940322200146), Valid_auc (0.4753977415307403) ,Training Average_loss (0.2670847556864222), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.615953615255125), Valid_auc (0.5131668757841907) ,Training Average_loss (0.265382577975591), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.6664663745086827), Valid_auc (0.5515332496863237) ,Training Average_loss (0.2668698495874802), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.6027066931101631), Valid_auc (0.48189962358845667) ,Training Average_loss (0.2652946416288614), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.7098423886314291), Valid_auc (0.5267101631116688) ,Training Average_loss (0.26674048403898876), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.7103687928090751), Valid_auc (0.5350439146800502) ,Training Average_loss (0.2667123174294829), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.7704535412826429), Valid_auc (0.5524592220828106) ,Training Average_loss (0.26719374575962623), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (21): Train_auc (0.741502810954325), Valid_auc (0.5511191969887076) ,Training Average_loss (0.2655864448597034), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.581471 has a TestAuc of 0.589424 at epoch 0 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.792555519346803), Valid_auc (0.6101204516938519) ,Training Average_loss (0.27584683435658613), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6325077547124791) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.8789855673689387), Valid_auc (0.5834705144291092) ,Training Average_loss (0.25451570084939396), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.9378381242177908), Valid_auc (0.6084968632371394) ,Training Average_loss (0.20720099788159133), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.9505465966660401), Valid_auc (0.6282032622333752) ,Training Average_loss (0.17340186388852694), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6090855404438081) , Test_eval_time (0m 0s) \n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.6654645971698528), Valid_auc (0.5067503136762861) ,Training Average_loss (0.21371918798734746), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.6696066063424406), Valid_auc (0.5760978670012546) ,Training Average_loss (0.2648951078454653), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.6782759813349434), Valid_auc (0.5776436637390213) ,Training Average_loss (0.2663505874574184), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.6838590544317513), Valid_auc (0.5764943538268507) ,Training Average_loss (0.2648416470736265), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.6972455746462316), Valid_auc (0.5358368883312421) ,Training Average_loss (0.26460293071965374), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.7172957659750572), Valid_auc (0.5422986198243412) ,Training Average_loss (0.26562139994154377), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.7092438612836025), Valid_auc (0.5367151819322459) ,Training Average_loss (0.2668557973578572), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.7492970115117177), Valid_auc (0.5810514429109159) ,Training Average_loss (0.26368560940027236), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.6874345243569392), Valid_auc (0.5576712672521957) ,Training Average_loss (0.26426173970103267), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.40304436749529177), Valid_auc (0.47380677540777916) ,Training Average_loss (0.26511764327685033), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.40010051260930934), Valid_auc (0.4712321204516939) ,Training Average_loss (0.2651480420182149), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.40063951210149823), Valid_auc (0.4698845671267252) ,Training Average_loss (0.26541579316059744), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.46653964725122254), Valid_auc (0.48111919698870764) ,Training Average_loss (0.26495172604918477), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.432184877225672), Valid_auc (0.4583864491844417) ,Training Average_loss (0.26589115541428326), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.4183682670046742), Valid_auc (0.46404767879548303) ,Training Average_loss (0.26501619375000396), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.45241475171236295), Valid_auc (0.46427352572145547) ,Training Average_loss (0.2646725474546353), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.45975607074168406), Valid_auc (0.4766323713927227) ,Training Average_loss (0.2644401668881377), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.4940931473491861), Valid_auc (0.4963563362609787) ,Training Average_loss (0.26534511310358844), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n",
      "\n",
      " Epoch (22): Train_auc (0.5429383269423774), Valid_auc (0.49682559598494347) ,Training Average_loss (0.26607589858273667), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "23\n",
      "\n",
      " Epoch (23): Train_auc (0.6023340317401927), Valid_auc (0.5514705144291091) ,Training Average_loss (0.26644617704053725), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (24): Train_auc (0.5855849623939894), Valid_auc (0.5303136762860727) ,Training Average_loss (0.26542334804932277), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.628203 has a TestAuc of 0.609086 at epoch 3 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.8299145217978912), Valid_auc (0.5915884567126725) ,Training Average_loss (0.2719197185710072), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.602294201861131) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.8005771852870733), Valid_auc (0.5993902132998745) ,Training Average_loss (0.26584233368436494), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6331883798616083) , Test_eval_time (0m 0s) \n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.556743041588529), Valid_auc (0.5050840652446675) ,Training Average_loss (0.26194289239744345), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.48803635047762234), Valid_auc (0.4635508155583438) ,Training Average_loss (0.26554917947699624), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.5269750152943105), Valid_auc (0.5077038895859474) ,Training Average_loss (0.2647446271032095), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.6508370386216328), Valid_auc (0.501237139272271) ,Training Average_loss (0.26485409401357174), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.6514058270323438), Valid_auc (0.502823086574655) ,Training Average_loss (0.26597589341302713), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.6494111690451152), Valid_auc (0.5128983688833124) ,Training Average_loss (0.2657350829492013), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.6507575182032284), Valid_auc (0.5139046424090339) ,Training Average_loss (0.26523391784479217), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.6494659986485027), Valid_auc (0.5140250941028858) ,Training Average_loss (0.2674217900882165), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.6390268320184891), Valid_auc (0.5096913425345044) ,Training Average_loss (0.26638351064175364), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.6487882007893064), Valid_auc (0.5213350062735257) ,Training Average_loss (0.2658772132669886), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.6267896842374637), Valid_auc (0.5157791718946048) ,Training Average_loss (0.2661966597661376), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.5023449776683074), Valid_auc (0.49142032622333753) ,Training Average_loss (0.26624993632237115), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.5622151559619821), Valid_auc (0.5051442910915934) ,Training Average_loss (0.26530808756748836), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.5828705221657544), Valid_auc (0.49861982434127977) ,Training Average_loss (0.26539184469729665), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.5656423310528482), Valid_auc (0.49132245922208284) ,Training Average_loss (0.26664917518695197), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.5709241162287629), Valid_auc (0.4771944792973651) ,Training Average_loss (0.2656567153210441), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.5022125769213853), Valid_auc (0.46904391468005013) ,Training Average_loss (0.26590029677997035), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.492291167685621), Valid_auc (0.4475006273525722) ,Training Average_loss (0.26436835483958326), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.5718323783552519), Valid_auc (0.48410037641154324) ,Training Average_loss (0.2647090472280979), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.4713227679303299), Valid_auc (0.4480828105395232) ,Training Average_loss (0.2648798141628504), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (22): Train_auc (0.5732996825181033), Valid_auc (0.48895608531994983) ,Training Average_loss (0.2663395336518685), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.599390 has a TestAuc of 0.633188 at epoch 1 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7929439248599521), Valid_auc (0.6044818067754077) ,Training Average_loss (0.27189924828708173), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6511506800286329) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.8314479513620933), Valid_auc (0.5872170639899624) ,Training Average_loss (0.24913591692845027), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.562414281887138), Valid_auc (0.49971141781681305) ,Training Average_loss (0.25355972386896614), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.6897402666208171), Valid_auc (0.5565771643663739) ,Training Average_loss (0.2629288138821721), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.6415510929934064), Valid_auc (0.5258243412797993) ,Training Average_loss (0.26388186011463405), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.6601625695241371), Valid_auc (0.5356260978670012) ,Training Average_loss (0.2643020222584407), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.5960011875582283), Valid_auc (0.523892095357591) ,Training Average_loss (0.2648784281065067), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.5772120271259091), Valid_auc (0.5127578419071518) ,Training Average_loss (0.26594163527091347), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.5714777102917715), Valid_auc (0.5088983688833124) ,Training Average_loss (0.2641337471082807), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.5501069102293947), Valid_auc (0.5131568381430364) ,Training Average_loss (0.2658558574815591), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.5642947323595623), Valid_auc (0.5228582183186951) ,Training Average_loss (0.26600960884243247), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.5489800794104593), Valid_auc (0.5320652446675032) ,Training Average_loss (0.2658742861201366), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.5671494204156053), Valid_auc (0.5218670012547051) ,Training Average_loss (0.2660072798530261), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.5760135729508622), Valid_auc (0.5436612296110414) ,Training Average_loss (0.26608277317136525), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.5765263821858269), Valid_auc (0.5219071518193225) ,Training Average_loss (0.267023678869009), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.5875945648218863), Valid_auc (0.5349058971141782) ,Training Average_loss (0.26539200724413), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.5531099431011663), Valid_auc (0.5430138017565873) ,Training Average_loss (0.26636263107260066), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.5654005210061857), Valid_auc (0.5523437892095358) ,Training Average_loss (0.26657564379274845), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.5793614775303586), Valid_auc (0.549189460476788) ,Training Average_loss (0.2660179887587826), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.57001105588721), Valid_auc (0.5654805520702635) ,Training Average_loss (0.26720682928959527), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.5699540271019181), Valid_auc (0.5650439146800502) ,Training Average_loss (0.2659401853258411), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (21): Train_auc (0.5720885330656995), Valid_auc (0.5618670012547051) ,Training Average_loss (0.26692321021109816), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.604482 has a TestAuc of 0.651151 at epoch 0 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.8037187666188177), Valid_auc (0.5659071518193225) ,Training Average_loss (0.2760312722374995), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6372709377236936) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.9426803229198738), Valid_auc (0.5957867001254704) ,Training Average_loss (0.24134745157013338), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6233017179670723) , Test_eval_time (0m 0s) \n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.9730122394469256), Valid_auc (0.5986599749058971) ,Training Average_loss (0.16971625089645384), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5752702219040802) , Test_eval_time (0m 0s) \n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.9861221125741223), Valid_auc (0.6034981179422836) ,Training Average_loss (0.1170559811250617), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5922667621092819) , Test_eval_time (0m 0s) \n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.9894640193847888), Valid_auc (0.5954755332496863) ,Training Average_loss (0.08402120443449046), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.9908815920477583), Valid_auc (0.5963889585947303) ,Training Average_loss (0.0719246159462879), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.9928382141843234), Valid_auc (0.6234429109159348) ,Training Average_loss (0.05930806566029787), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5813863039847291) , Test_eval_time (0m 0s) \n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.9889270691302835), Valid_auc (0.593440401505646) ,Training Average_loss (0.07394261806815242), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.9845878333260027), Valid_auc (0.5826223337515684) ,Training Average_loss (0.07860849535791203), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.9898499758089991), Valid_auc (0.5951442910915934) ,Training Average_loss (0.07337301968752095), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.9894631697008712), Valid_auc (0.5939021329987453) ,Training Average_loss (0.06462091316158573), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.9796343760121233), Valid_auc (0.5971593475533249) ,Training Average_loss (0.07499053936917334), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.9783243133554318), Valid_auc (0.595357590966123) ,Training Average_loss (0.10965890797475973), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.9827747078086952), Valid_auc (0.5719623588456713) ,Training Average_loss (0.09254138031974435), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.9777417800578186), Valid_auc (0.5423613550815559) ,Training Average_loss (0.10852948318546018), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.985940230234353), Valid_auc (0.5568281053952321) ,Training Average_loss (0.0950721306571116), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.9873776455158682), Valid_auc (0.5715884567126726) ,Training Average_loss (0.08228055561970299), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.9875071473411893), Valid_auc (0.557189460476788) ,Training Average_loss (0.062054480969284974), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.9899326450560392), Valid_auc (0.5840326223337515) ,Training Average_loss (0.0654341975459829), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.9917914535792685), Valid_auc (0.599129234629862) ,Training Average_loss (0.0544364480030102), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.9932173231557859), Valid_auc (0.5994654956085319) ,Training Average_loss (0.04774413508518288), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.9924564561982939), Valid_auc (0.5928657465495609) ,Training Average_loss (0.048405678702207904), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "22\n",
      "\n",
      " Epoch (22): Train_auc (0.9930004538311749), Valid_auc (0.6124542032622334) ,Training Average_loss (0.03894771137177789), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "23\n",
      "\n",
      " Epoch (23): Train_auc (0.9931606942217496), Valid_auc (0.5687954830614804) ,Training Average_loss (0.0353921033112177), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "24\n",
      "\n",
      " Epoch (24): Train_auc (0.9939400542998005), Valid_auc (0.5654604767879549) ,Training Average_loss (0.03327327176424053), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "25\n",
      "\n",
      " Epoch (25): Train_auc (0.9945848644304319), Valid_auc (0.5608406524466751) ,Training Average_loss (0.0334125989174936), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "26\n",
      "\n",
      " Epoch (26): Train_auc (0.9938976700667351), Valid_auc (0.5858569636135509) ,Training Average_loss (0.04822976521681995), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/mai.kassem/Documents/Med-BERT/MedBERT Pretraining/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch (27): Train_auc (0.9934609825145047), Valid_auc (0.5697666248431619) ,Training Average_loss (0.042441770768103494), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.623443 has a TestAuc of 0.581386 at epoch 6 \n",
      "0\n",
      "\n",
      " Epoch (0): Train_auc (0.7791038233777035), Valid_auc (0.5661028858218319) ,Training Average_loss (0.27306947664668163), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6272888332140301) , Test_eval_time (0m 0s) \n",
      "1\n",
      "\n",
      " Epoch (1): Train_auc (0.9350285193907866), Valid_auc (0.5908205771643664) ,Training Average_loss (0.2413986290494601), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6196546170365068) , Test_eval_time (0m 0s) \n",
      "2\n",
      "\n",
      " Epoch (2): Train_auc (0.9663202788562655), Valid_auc (0.5839548306148056) ,Training Average_loss (0.18724807326992354), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "3\n",
      "\n",
      " Epoch (3): Train_auc (0.9779011207830689), Valid_auc (0.6019949811794227) ,Training Average_loss (0.14486427512019873), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.6065419947506562) , Test_eval_time (0m 0s) \n",
      "4\n",
      "\n",
      " Epoch (4): Train_auc (0.983807123749965), Valid_auc (0.5970062735257214) ,Training Average_loss (0.10171882751553009), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "5\n",
      "\n",
      " Epoch (5): Train_auc (0.988366927502969), Valid_auc (0.5809460476787954) ,Training Average_loss (0.09527696445584298), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "6\n",
      "\n",
      " Epoch (6): Train_auc (0.9895464387247944), Valid_auc (0.5689435382685069) ,Training Average_loss (0.08178716138160476), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "7\n",
      "\n",
      " Epoch (7): Train_auc (0.9908768937955081), Valid_auc (0.5491794228356337) ,Training Average_loss (0.0744093486146691), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "8\n",
      "\n",
      " Epoch (8): Train_auc (0.9914970131111226), Valid_auc (0.5720376411543288) ,Training Average_loss (0.07565403319507216), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "9\n",
      "\n",
      " Epoch (9): Train_auc (0.9863363828655741), Valid_auc (0.562474278544542) ,Training Average_loss (0.06665777939294154), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "10\n",
      "\n",
      " Epoch (10): Train_auc (0.9867453307369658), Valid_auc (0.5835809284818068) ,Training Average_loss (0.07798248843755572), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "11\n",
      "\n",
      " Epoch (11): Train_auc (0.9918367867153419), Valid_auc (0.5731944792973651) ,Training Average_loss (0.09035007040171572), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "12\n",
      "\n",
      " Epoch (12): Train_auc (0.9914108951470052), Valid_auc (0.5627227101631117) ,Training Average_loss (0.07597484019352123), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "13\n",
      "\n",
      " Epoch (13): Train_auc (0.989204365975857), Valid_auc (0.570564617314931) ,Training Average_loss (0.0785516609204933), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "14\n",
      "\n",
      " Epoch (14): Train_auc (0.9909378211305395), Valid_auc (0.5588908406524467) ,Training Average_loss (0.08107687563945849), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "15\n",
      "\n",
      " Epoch (15): Train_auc (0.9912157177529959), Valid_auc (0.6103212045169385) ,Training Average_loss (0.07935463538548598), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.588709735146743) , Test_eval_time (0m 0s) \n",
      "16\n",
      "\n",
      " Epoch (16): Train_auc (0.9788821558380284), Valid_auc (0.5601555834378921) ,Training Average_loss (0.09693602531527479), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "17\n",
      "\n",
      " Epoch (17): Train_auc (0.9877238167401727), Valid_auc (0.5779974905897115) ,Training Average_loss (0.08933920504835746), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "18\n",
      "\n",
      " Epoch (18): Train_auc (0.9924350641561339), Valid_auc (0.6023462986198242) ,Training Average_loss (0.08172836812833945), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "19\n",
      "\n",
      " Epoch (19): Train_auc (0.9946598365408069), Valid_auc (0.5744391468005019) ,Training Average_loss (0.07092857311169305), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "20\n",
      "\n",
      " Epoch (20): Train_auc (0.9944371693729932), Valid_auc (0.571176913425345) ,Training Average_loss (0.06184612792373325), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "21\n",
      "\n",
      " Epoch (21): Train_auc (0.9946867765191347), Valid_auc (0.5891618569636136) ,Training Average_loss (0.06149652428769817), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "22\n",
      "\n",
      " Epoch (22): Train_auc (0.9953394837120592), Valid_auc (0.5696537013801756) ,Training Average_loss (0.058511348796309905), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "23\n",
      "\n",
      " Epoch (23): Train_auc (0.9920192688319944), Valid_auc (0.6132622333751567) ,Training Average_loss (0.06619775684084743), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5790527320448581) , Test_eval_time (0m 0s) \n",
      "24\n",
      "\n",
      " Epoch (24): Train_auc (0.9934022543613777), Valid_auc (0.6099322459222083) ,Training Average_loss (0.06999400098187229), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "25\n",
      "\n",
      " Epoch (25): Train_auc (0.9958710859560244), Valid_auc (0.6112747804265998) ,Training Average_loss (0.05527349461335689), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "26\n",
      "\n",
      " Epoch (26): Train_auc (0.9938630829331488), Valid_auc (0.6175081555834379) ,Training Average_loss (0.0648766009719111), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "\n",
      " Test_AUC (0.5643301121450728) , Test_eval_time (0m 0s) \n",
      "27\n",
      "\n",
      " Epoch (27): Train_auc (0.9953799186702548), Valid_auc (0.6056386449184442) ,Training Average_loss (0.05192832502458866), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "28\n",
      "\n",
      " Epoch (28): Train_auc (0.9958667375736225), Valid_auc (0.6068431618569636) ,Training Average_loss (0.04117724523724367), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "29\n",
      "\n",
      " Epoch (29): Train_auc (0.9967815972458246), Valid_auc (0.6021254705144291) ,Training Average_loss (0.03637710032150305), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "30\n",
      "\n",
      " Epoch (30): Train_auc (0.9973974181604442), Valid_auc (0.6057590966122961) ,Training Average_loss (0.030646142462501302), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "31\n",
      "\n",
      " Epoch (31): Train_auc (0.9976412274633836), Valid_auc (0.5982308657465496) ,Training Average_loss (0.024440945506891392), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "32\n",
      "\n",
      " Epoch (32): Train_auc (0.9975102261958552), Valid_auc (0.588840652446675) ,Training Average_loss (0.021232341583042092), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "33\n",
      "\n",
      " Epoch (33): Train_auc (0.9976309312935587), Valid_auc (0.592047678795483) ,Training Average_loss (0.020930008356905696), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "34\n",
      "\n",
      " Epoch (34): Train_auc (0.9979627078726714), Valid_auc (0.5934504391468005) ,Training Average_loss (0.020579202850543273), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "35\n",
      "\n",
      " Epoch (35): Train_auc (0.9976228842870452), Valid_auc (0.5953174404015057) ,Training Average_loss (0.01551295956693745), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "36\n",
      "\n",
      " Epoch (36): Train_auc (0.9986092673525447), Valid_auc (0.5909259723964868) ,Training Average_loss (0.01666610142031762), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "37\n",
      "\n",
      " Epoch (37): Train_auc (0.9983630589420736), Valid_auc (0.5787126725219573) ,Training Average_loss (0.013205443619760142), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "38\n",
      "\n",
      " Epoch (38): Train_auc (0.9991408196151031), Valid_auc (0.5464040150564619) ,Training Average_loss (0.013369235745146096), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "39\n",
      "\n",
      " Epoch (39): Train_auc (0.9984518759021644), Valid_auc (0.5677390213299874) ,Training Average_loss (0.015323687350852802), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "40\n",
      "\n",
      " Epoch (40): Train_auc (0.9981445902124411), Valid_auc (0.5748908406524467) ,Training Average_loss (0.016702247353532585), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "41\n",
      "\n",
      " Epoch (41): Train_auc (0.9982378555177474), Valid_auc (0.5296386449184441) ,Training Average_loss (0.013201172505311358), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "42\n",
      "\n",
      " Epoch (42): Train_auc (0.9983502637019029), Valid_auc (0.5886599749058972) ,Training Average_loss (0.013101813889322028), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "43\n",
      "\n",
      " Epoch (43): Train_auc (0.9981800770113518), Valid_auc (0.5790012547051442) ,Training Average_loss (0.01241255270385106), Train_time (0m 4s), Eval_time (0m 1s)\n",
      "44\n",
      "\n",
      " Epoch (44): Train_auc (0.9994314614963233), Valid_auc (0.5327352572145545) ,Training Average_loss (0.012922691905744918), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "45\n",
      "\n",
      " Epoch (45): Train_auc (0.9986544005629906), Valid_auc (0.5442961104140527) ,Training Average_loss (0.013516697307932193), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "46\n",
      "\n",
      " Epoch (46): Train_auc (0.9982211617278371), Valid_auc (0.5688632371392722) ,Training Average_loss (0.011253224612300985), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "47\n",
      "\n",
      " Epoch (47): Train_auc (0.9987982470520966), Valid_auc (0.5360225846925973) ,Training Average_loss (0.022081733698723835), Train_time (0m 5s), Eval_time (0m 1s)\n",
      "BestValidAuc 0.617508 has a TestAuc of 0.564330 at epoch 26 \n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 75\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 91\n",
    "\n",
    "results = []\n",
    "\n",
    "# Data Preparation\n",
    "train_features = convert_EHRexamples_to_features(train_f, MAX_SEQ_LENGTH)\n",
    "test_features = convert_EHRexamples_to_features(test_f, MAX_SEQ_LENGTH)\n",
    "valid_features = convert_EHRexamples_to_features(valid_f, MAX_SEQ_LENGTH)\n",
    "train = BERTdataEHR(train_features)\n",
    "test = BERTdataEHR(test_features)\n",
    "valid = BERTdataEHR(valid_features)\n",
    "print(\" creating the list of training minibatches\")\n",
    "train_mbs = list(BERTdataEHRloader(train, batch_size=BATCH_SIZE))\n",
    "print(\" creating the list of test minibatches\")\n",
    "test_mbs = list(BERTdataEHRloader(test, batch_size=BATCH_SIZE))\n",
    "print(\" creating the list of valid minibatches\")\n",
    "valid_mbs = list(BERTdataEHRloader(valid, batch_size=BATCH_SIZE))\n",
    "\n",
    "for run in range(10):  # to average the results on 10 runs\n",
    "    for model_type in [\"Bert only\"]:\n",
    "        ehr_model = EHR_BERT_LR(input_size=90000, embed_dim=192, hidden_size=192)\n",
    "        if use_cuda:\n",
    "            ehr_model.cuda()\n",
    "        optimizer = optim.Adam(\n",
    "            ehr_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
    "        )\n",
    "        out_dir_name = \"Gentacimin_finetuning\"  # + str(i)\n",
    "        trauc, vauc, testauc, bep = epochs_run(\n",
    "            NUM_EPOCHS,\n",
    "            train=train_mbs,\n",
    "            valid=valid_mbs,\n",
    "            test=test_mbs,\n",
    "            model=ehr_model,\n",
    "            optimizer=optimizer,\n",
    "            shuffle=True,\n",
    "            # batch_size = args.batch_size,\n",
    "            patience=20,\n",
    "            output_dir=out_dir_name,\n",
    "            model_prefix=\"first_run\",\n",
    "        )\n",
    "        results.append(\n",
    "            [\n",
    "                model_type,\n",
    "                run,\n",
    "                len(train_features),\n",
    "                len(test_features),\n",
    "                len(valid_features),\n",
    "                trauc,\n",
    "                vauc,\n",
    "                testauc,\n",
    "                bep,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_EHRexamples_to_features(train_f, MAX_SEQ_LENGTH)\n",
    "test_features = convert_EHRexamples_to_features(test_f, MAX_SEQ_LENGTH)\n",
    "valid_features = convert_EHRexamples_to_features(valid_f, MAX_SEQ_LENGTH)\n",
    "train = BERTdataEHR(train_features)\n",
    "test = BERTdataEHR(test_features)\n",
    "valid = BERTdataEHR(valid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_mbs[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Run</th>\n",
       "      <th>Train_size</th>\n",
       "      <th>Test_size</th>\n",
       "      <th>Valid_size</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Valid_AUC</th>\n",
       "      <th>Test_AUC</th>\n",
       "      <th>Best_Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>0</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.768260</td>\n",
       "      <td>0.614171</td>\n",
       "      <td>0.544578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>1</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.959410</td>\n",
       "      <td>0.621476</td>\n",
       "      <td>0.596956</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>2</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.927739</td>\n",
       "      <td>0.640156</td>\n",
       "      <td>0.636481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>3</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.977207</td>\n",
       "      <td>0.625817</td>\n",
       "      <td>0.579216</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>4</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.770143</td>\n",
       "      <td>0.581471</td>\n",
       "      <td>0.589424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>5</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.950547</td>\n",
       "      <td>0.628203</td>\n",
       "      <td>0.609086</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>6</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.800577</td>\n",
       "      <td>0.599390</td>\n",
       "      <td>0.633188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>7</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.792944</td>\n",
       "      <td>0.604482</td>\n",
       "      <td>0.651151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>8</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.992838</td>\n",
       "      <td>0.623443</td>\n",
       "      <td>0.581386</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bert only</td>\n",
       "      <td>9</td>\n",
       "      <td>12038</td>\n",
       "      <td>3439</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.993863</td>\n",
       "      <td>0.617508</td>\n",
       "      <td>0.564330</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Run  Train_size  Test_size  Valid_size  Train_AUC  Valid_AUC  \\\n",
       "0  Bert only    0       12038       3439        1719   0.768260   0.614171   \n",
       "1  Bert only    1       12038       3439        1719   0.959410   0.621476   \n",
       "2  Bert only    2       12038       3439        1719   0.927739   0.640156   \n",
       "3  Bert only    3       12038       3439        1719   0.977207   0.625817   \n",
       "4  Bert only    4       12038       3439        1719   0.770143   0.581471   \n",
       "5  Bert only    5       12038       3439        1719   0.950547   0.628203   \n",
       "6  Bert only    6       12038       3439        1719   0.800577   0.599390   \n",
       "7  Bert only    7       12038       3439        1719   0.792944   0.604482   \n",
       "8  Bert only    8       12038       3439        1719   0.992838   0.623443   \n",
       "9  Bert only    9       12038       3439        1719   0.993863   0.617508   \n",
       "\n",
       "   Test_AUC  Best_Epoch  \n",
       "0  0.544578           1  \n",
       "1  0.596956           3  \n",
       "2  0.636481           1  \n",
       "3  0.579216          10  \n",
       "4  0.589424           0  \n",
       "5  0.609086           3  \n",
       "6  0.633188           1  \n",
       "7  0.651151           0  \n",
       "8  0.581386           6  \n",
       "9  0.564330          26  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.columns = [\n",
    "    \"Model\",\n",
    "    \"Run\",\n",
    "    \"Train_size\",\n",
    "    \"Test_size\",\n",
    "    \"Valid_size\",\n",
    "    \"Train_AUC\",\n",
    "    \"Valid_AUC\",\n",
    "    \"Test_AUC\",\n",
    "    \"Best_Epoch\",\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    root_path + \"MedBERT_Results/results_gentacimin_antibiotic_2e3_finetuning.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "847e22e810249873d4908bdc935e4d43d04de5ac553b0385e48d62b133981721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
